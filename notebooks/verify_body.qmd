---
title: "Document Extraction Verification: `r params$country`"
subtitle: "Data completeness and parsing quality checks"
date: today
params:
  country: "US"
  body_target: "us_body"
  labels_target: "us_labels"
  min_year: 1946
  max_year: 2022
  fiscal_vocab:
    - "tax"
    - "fiscal"
    - "budget"
    - "deficit"
    - "revenue"
    - "spending"
    - "expenditure"
    - "appropriation"
execute:
  cache: false
  warning: false
  message: false
---

## Setup

```{r setup}
library(tidyverse)
library(targets)
library(here)
library(quanteda)
library(gt)
library(digest)
pacman::p_load(quarto)

here::i_am("notebooks/verify_body.qmd")
tar_config_set(store = here("_targets"))

# Load gt theme
source(here("R/gt_theme.R"))

# Set global ggplot theme
theme_set(theme_minimal())

# Load data - tar_read_raw accepts character strings
body_data <- tar_read(us_body)

# Load labels if available
has_labels <- FALSE
if (!is.null(params$labels_target) && params$labels_target != "") {
  tryCatch({
    labels_data <- tar_read_raw(params$labels_target)
    has_labels <- TRUE
  }, error = function(e) {
    message("Labels target not found - skipping known act validation")
    has_labels <<- FALSE
  })
}

# Fiscal vocabulary
fiscal_terms <- params$fiscal_vocab

# Test results storage
test_results <- list()
```

## Overview Statistics

```{r overview}
# Total documents, pages, sources
overview_stats <- body_data %>%
  summarize(
    total_documents = n(),
    successful_extractions = sum(n_pages > 0),
    total_pages = sum(n_pages),
    years_covered = n_distinct(year),
    year_range = sprintf("%d-%d", min(year), max(year)),
    sources_used = n_distinct(source),
    document_types = n_distinct(body),
    ocr_documents = sum(ocr_used, na.rm = TRUE)
  )

overview_stats %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = "Metric", values_to = "Value") %>%
  mutate(Metric = str_replace_all(Metric, "_", " ") %>% str_to_title()) %>%
  gt() %>%
  tab_header(title = "Extraction Overview") %>%
  gt_theme_report()
```

The corpus comprises **360 documents** totaling over **104,000 pages** across 4 document types and 4 source domains. Of these, 350 (97.2%) were successfully extracted. The 69 documents requiring OCR are concentrated in pre-1990 ERP volumes from Fraser, which were digitized as scanned images rather than born-digital PDFs. The addition of 47 CBO reports contributes 7,438 pages of non-partisan budget analysis spanning 1976-2022.

### Page Distribution by Source and Body

```{r overview-viz}
# Pages by year, body, and source
body_data %>%
  filter(n_pages > 0) %>%
  ggplot(aes(x = year, y = n_pages, fill = body)) +
  geom_col() +
  facet_wrap(~source, ncol = 1) +
  labs(
    title = "Pages Extracted by Year, Source, and Body",
    x = "Year",
    y = "Number of Pages",
    fill = "Document Type"
  ) +
  theme(legend.position = "bottom")

# Page count distribution
body_data %>%
  filter(n_pages > 0) %>%
  ggplot(aes(x = n_pages, fill = body)) +
  geom_histogram(bins = 50, alpha = 0.7) +
  facet_wrap(~body, ncol = 1, scales = "free_y") +
  labs(
    title = "Distribution of Page Counts by Document Type",
    x = "Number of Pages",
    y = "Count"
  ) +
  theme(legend.position = "none")
```

## RR1 Source Coverage

This section documents our implementation of RR1 (Identify and Collect Sources) from the Romer & Romer (2010) methodology. R&R used 9 distinct source types; we assess availability for our LLM-assisted approach.

```{r rr1-sources}
# Define RR1 source requirements
rr_sources <- tribble(
  ~source_name, ~rr_purpose, ~status, ~notes,
  "Economic Report of the President", "Executive fiscal narrative", "Available", "1946-2022, ERP URLs",
  "Treasury Annual Reports", "Revenue estimates, implementation details", "Available", "1946-1980, 2011-2022",
  "Budget of the United States", "Budget proposals, revenue projections", "Available", "1946-2022",
  "CBO Budget and Economic Outlook", "Non-partisan revenue estimates (post-1974)", "Available", "1976-2022, manually downloaded (CAPTCHA)",
  "Social Security Bulletin", "Payroll tax changes", "Deferred", "CAPTCHA-protected; revisit in Phase 1",
  "House Ways & Means Committee Reports", "Legislative intent, bill details", "Deferred", "Per-bill source; requires Congress.gov API",
  "Senate Finance Committee Reports", "Legislative intent, bill details", "Deferred", "Per-bill source; requires Congress.gov API",
  "Conference Reports", "Final bill versions", "Deferred", "Per-bill source; requires Congress.gov API",
  "Congressional Record", "Floor debates, stated motivations", "Deferred", "Per-bill source; requires Congress.gov API"
)

# Render source table with status coloring
rr_sources %>%
  gt() %>%
  tab_header(
    title = "RR1 Source Coverage Assessment",
    subtitle = "Phase 0: Codebook development sources"
  ) %>%
  cols_label(
    source_name = "Source",
    rr_purpose = "R&R Purpose",
    status = "Status",
    notes = "Coverage Notes"
  ) %>%
  tab_style(
    style = cell_text(color = "#2E7D32", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "Available")
  ) %>%
  tab_style(
    style = cell_text(color = "#F57C00", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "Deferred")
  ) %>%
  gt_theme_report()
```

### Actual Coverage by Document Type

```{r rr1-coverage}
# Cross-reference against actual body_data
actual_doc_types <- body_data %>%
  filter(n_pages > 0) %>%
  group_by(body) %>%
  summarize(
    year_min = min(year),
    year_max = max(year),
    n_documents = n(),
    total_pages = sum(n_pages),
    .groups = "drop"
  ) %>%
  mutate(coverage = sprintf("%d-%d", year_min, year_max))

actual_doc_types %>%
  select(body, coverage, n_documents, total_pages) %>%
  gt() %>%
  tab_header(title = "Extracted Document Coverage") %>%
  cols_label(
    body = "Document Type",
    coverage = "Years",
    n_documents = "Documents",
    total_pages = "Total Pages"
  ) %>%
  fmt_number(columns = c(n_documents, total_pages), decimals = 0) %>%
  gt_theme_report()
```

::: {.callout-note}
## Source Strategy

**Phase 0 (Current):** 4 of 9 R&R sources available. The three core narrative sources (ERP, Treasury, Budget) provide continuous coverage from 1946 to 2022. CBO Budget and Economic Outlook reports (1976-2022) were added via manual download after discovering that cbo.gov requires CAPTCHA verification that blocks automated extraction.

**CAPTCHA-Protected Sources:** SSB (ssa.gov) also requires human verification. URL functions exist in `R/pull_us.R` and can be re-enabled once PDFs are manually downloaded. CBO was resolved this way.

**Deferred Sources:** Per-bill congressional documents (4 sources) require Congress.gov API integration. These provide fine-grained legislative detail but are not essential for initial codebook validation using the 44 labeled US acts.

**Implication:** Our corpus now totals over 104,000 pages across 4 document types. The 47 CBO reports add 7,400 pages of non-partisan budget analysis, complementing the executive-branch perspective of the ERP and Budget documents. This provides sufficient coverage for Phase 0 codebook development.
:::

## Test (i): PDF URL Resolution & Page Count Validation

```{r test-i}
# Identify failed extractions
failed_extractions <- body_data %>%
  filter(n_pages == 0) %>%
  select(year, body, source, pdf_url, n_pages)

# Summary by source
url_resolution_summary <- body_data %>%
  group_by(source, body) %>%
  summarize(
    total_docs = n(),
    successful = sum(n_pages > 0),
    failed = sum(n_pages == 0),
    success_rate = mean(n_pages > 0),
    ocr_docs = sum(ocr_used, na.rm = TRUE),
    ocr_rate = mean(ocr_used, na.rm = TRUE),
    .groups = "drop"
  )

# Overall success rate
overall_success_rate <- mean(body_data$n_pages > 0)

# Determine status
test_i_status <- case_when(
  overall_success_rate >= 0.95 ~ "PASS",
  overall_success_rate >= 0.85 ~ "WARN",
  TRUE ~ "FAIL"
)

# Store result
test_results$test_i <- list(
  metric = "URL resolution success rate",
  value = sprintf("%.1f%%", overall_success_rate * 100),
  target = "≥95%",
  status = test_i_status
)
```

### Results

**Overall Success Rate:** `r sprintf("%.1f%%", overall_success_rate * 100)` **Status:** `r test_i_status`

```{r test-i-display}
#| results: asis
if (nrow(failed_extractions) > 0) {
  cat("\n### Failed Extractions\n\n")
  failed_extractions %>%
    gt() %>%
    tab_header(title = sprintf("Failed PDF extractions (%d documents)", nrow(failed_extractions))) %>%
    gt_theme_report()
} else {
  cat("\n✅ All PDFs successfully extracted!\n\n")
}
```

```{r test-i-summary}
# Success rate by source/body
url_resolution_summary %>%
  mutate(
    success_rate = success_rate * 100,
    ocr_rate = ocr_rate * 100
  ) %>%
  gt() %>%
  tab_header(title = "Success Rate by Source and Body") %>%
  fmt_number(columns = c(success_rate, ocr_rate), decimals = 1) %>%
  gt_theme_report()

# OCR usage visualization
body_data %>%
  filter(n_pages > 0) %>%
  count(source, ocr_used) %>%
  ggplot(aes(x = source, y = n, fill = ocr_used)) +
  geom_col(position = "stack") +
  labs(
    title = "OCR Usage by Source",
    x = "Source",
    y = "Number of Documents",
    fill = "OCR Used"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The 10 failed extractions all come from Fraser-hosted Budget PDFs and one Treasury report. The Budget failures cluster in two groups: early 1990s sectioned PDFs (1991-1994) where Fraser uses non-standard filenames, and mid-2000s editions (2005-2009) where the single-file URL pattern does not match the actual hosting. These are supplementary Budget documents; the primary Budget PDF for each of those years was successfully extracted via a different URL. The single Treasury failure (2016) is a known broken link on home.treasury.gov. None of these gaps affect years critical to the 44 labeled acts.

CBO reports achieved **100% extraction success** (47/47), confirming that the manual download workflow fully resolved the CAPTCHA issue. OCR was needed for 5 early CBO reports (1979, 1980, 1982, 1984, 2012), consistent with scanned originals from that era.

## Test (ii): Boundary Document Verification

```{r test-ii}
# Get boundary documents (earliest and latest per source/body)
boundary_docs <- bind_rows(
  body_data %>%
    filter(n_pages > 0) %>%
    group_by(source, body) %>%
    slice_min(year, n = 1, with_ties = FALSE) %>%
    mutate(boundary_type = "Earliest"),
  body_data %>%
    filter(n_pages > 0) %>%
    group_by(source, body) %>%
    slice_max(year, n = 1, with_ties = FALSE) %>%
    mutate(boundary_type = "Latest")
) %>%
  ungroup()


# Extract sample pages
extract_sample_pages <- function(text_list) {
  if (is.null(text_list) || length(text_list) == 0) {
    return(list(first = NA, middle = NA, last = NA))
  }

  pages <- text_list[[1]]
  n <- length(pages)

  if (n == 0) {
    return(list(first = NA, middle = NA, last = NA))
  }

  list(
    first = pages[1],
    middle = if (n > 1) pages[ceiling(n/2)] else NA,
    last = if (n > 1) pages[n] else NA
  )
}

boundary_docs <- boundary_docs %>%
  mutate(sample_pages = map(text, extract_sample_pages))

# Check if all boundary docs have sufficient pages
boundary_valid <- all(boundary_docs$n_pages >= 10, na.rm = TRUE)

test_ii_status <- case_when(
  boundary_valid ~ "PASS",
  any(boundary_docs$n_pages < 10 & boundary_docs$n_pages > 0) ~ "WARN",
  TRUE ~ "FAIL"
)

test_results$test_ii <- list(
  metric = "Boundary documents valid",
  value = sprintf("%d/%d valid", sum(boundary_docs$n_pages >= 10), nrow(boundary_docs)),
  target = "All ≥10 pages",
  status = test_ii_status
)
```

### Results

**Status:** `r test_ii_status`

```{r test-ii-display}
# Summary table
boundary_docs %>%
  select(body, source, year, boundary_type, n_pages, ocr_used) %>%
  arrange(body, source, boundary_type) %>%
  gt() %>%
  tab_header(title = "Boundary Documents") %>%
  gt_theme_report()
```

### Sample Pages from Boundary Documents

```{r test-ii-samples}
#| results: asis
# Display sample text from first few boundary documents
display_sample_text <- function(text, label, max_chars = 1000) {
  if (is.na(text) || is.null(text) || nchar(text) == 0) {
    cat(sprintf("\n**%s:** (No text available)\n\n", label))
    return()
  }

  truncated <- str_trunc(text, max_chars)
  cat(sprintf("\n**%s:**\n\n", label))
  cat("```text\n")
  cat(truncated)
  cat("\n```\n\n")

  if (nchar(text) > max_chars) {
    cat(sprintf("*(Truncated: showing %d of %d characters)*\n\n", max_chars, nchar(text)))
  }
}

# Show samples from first 3 boundary documents
for (i in seq_len(min(3, nrow(boundary_docs)))) {
  row <- boundary_docs[i, ]
  cat(sprintf("\n### %s - %s %d (%s)\n\n",
              row$body, row$boundary_type, row$year, row$source))

  samples <- row$sample_pages[[1]]
  display_sample_text(samples$middle, "Middle Page", 800)
}
```

## Test (iii): Known Act Validation

::: {.callout-note}
## Important Finding: Year Lag in Economic Reports

The Economic Report of the President (ERP) discusses legislation retrospectively. Acts passed in year N are typically discussed in the year N+1 or N+2 ERP. For example:

- **Economic Recovery Tax Act of 1981** → Found in 1982-1990 ERPs (not 1981)
- **Omnibus Budget Reconciliation Act of 1990** → Found in 1991-1994 ERPs (not 1990)
- **Tax Reform Act of 1986** → Found in 1987-1990 ERPs (not 1986)

This is expected behavior: ERPs review the previous year's economic events and policy changes. Therefore, we use an **expanded year window** (year to year+2) when validating act detection.
:::

```{r test-iii}
if (has_labels) {
  # Prepare labels data with expected year
  labels_prepared <- labels_data %>%
    mutate(expected_year = year(date))

  # Get unique acts with their years (filter NA years)
  acts_by_year <- labels_prepared %>%
    distinct(act_name, expected_year) %>%
    filter(!is.na(expected_year))

  # Prepare document text
  docs_with_text <- body_data %>%
    filter(n_pages > 0) %>%
    mutate(full_text = map_chr(.data$text, function(text_list) {
      if (is.null(text_list) || length(text_list) == 0) return("")
      pages <- if (is.list(text_list[[1]])) text_list[[1]] else text_list
      if (length(pages) == 0) return("")
      paste(pages, collapse = " ")
    })) %>%
    select(year, body, full_text)

  # --- STRICT MATCHING (exact year only) ---
  known_acts_strict <- acts_by_year %>%
    left_join(docs_with_text, by = c("expected_year" = "year"),
              relationship = "many-to-many") %>%
    filter(!is.na(full_text))

  act_validation_strict <- known_acts_strict %>%
    mutate(act_name_found = str_detect(full_text, fixed(act_name, ignore_case = TRUE))) %>%
    group_by(act_name, expected_year) %>%
    summarize(n_docs = n(), found_in_any = any(act_name_found), .groups = "drop")

  strict_recall <- mean(act_validation_strict$found_in_any, na.rm = TRUE)

  # --- EXPANDED MATCHING (year to year+2) ---
  # This accounts for the retrospective nature of Economic Reports
  known_acts_expanded <- acts_by_year %>%
    cross_join(docs_with_text) %>%
    filter(year >= expected_year & year <= expected_year + 2) %>%
    filter(!is.na(full_text))

  act_validation_expanded <- known_acts_expanded %>%
    mutate(act_name_found = str_detect(full_text, fixed(act_name, ignore_case = TRUE))) %>%
    group_by(act_name, expected_year) %>%
    summarize(
      n_docs = n(),
      found_in_any = any(act_name_found),
      years_checked = paste(sort(unique(year)), collapse = ", "),
      .groups = "drop"
    )

  expanded_recall <- mean(act_validation_expanded$found_in_any, na.rm = TRUE)

  # Use expanded recall as the primary metric
  act_name_recall <- expanded_recall
  act_name_validation <- act_validation_expanded

  # Determine status based on expanded recall
  test_iii_status <- case_when(
    expanded_recall >= 0.85 ~ "PASS",
    expanded_recall >= 0.75 ~ "WARN",
    TRUE ~ "FAIL"
  )

  test_results$test_iii_strict <- list(
    metric = "Act recall (exact year)",
    value = sprintf("%.1f%%", strict_recall * 100),
    target = "Reference only",
    status = "INFO"
  )

  test_results$test_iii_acts <- list(
    metric = "Act recall (year to year+2)",
    value = sprintf("%.1f%%", expanded_recall * 100),
    target = ">=85%",
    status = test_iii_status
  )
} else {
  test_iii_status <- "SKIP"
  act_name_recall <- NA
  strict_recall <- NA
  expanded_recall <- NA

  test_results$test_iii_acts <- list(
    metric = "Known act validation",
    value = "N/A",
    target = "N/A",
    status = "SKIP"
  )
}
```

### Results

```{r test-iii-display}
#| results: asis
if (has_labels) {
  cat("\n### Recall Comparison\n\n")

  # Create recall comparison table
  recall_comparison <- tribble(
    ~matching_method, ~acts_found, ~total_acts, ~recall, ~notes,
    "Exact year only",
    sum(act_validation_strict$found_in_any),
    nrow(act_validation_strict),
    strict_recall * 100,
    "Too strict - misses retrospective mentions",
    "Year to Year+2",
    sum(act_validation_expanded$found_in_any),
    nrow(act_validation_expanded),
    expanded_recall * 100,
    "Primary metric - accounts for ERP lag"
  )

  recall_comparison %>%
    mutate(acts_found = sprintf("%d/%d", acts_found, total_acts)) %>%
    select(-total_acts) %>%
    gt() %>%
    tab_header(title = "Act Detection Recall Comparison") %>%
    cols_label(
      matching_method = "Matching Method",
      acts_found = "Acts Found",
      recall = "Recall (%)",
      notes = "Notes"
    ) %>%
    fmt_number(columns = recall, decimals = 1) %>%
    tab_style(
      style = cell_text(weight = "bold"),
      locations = cells_body(rows = matching_method == "Year to Year+2")
    ) %>%
    gt_theme_report()

  cat(sprintf("\n**Status:** %s (target: >=85%%)\n\n", test_iii_status))

  # Show acts NOT found (for investigation)
  missing_acts <- act_name_validation %>%
    filter(!found_in_any)

  if (nrow(missing_acts) > 0) {
    cat("\n### Acts Not Found in Expanded Window\n\n")
    cat("These acts were not found even when searching year to year+2:\n\n")
    missing_acts %>%
      select(act_name, expected_year, years_checked, n_docs) %>%
      arrange(expected_year) %>%
      gt() %>%
      tab_header(title = sprintf("Missing acts (%d total)", nrow(missing_acts))) %>%
      gt_theme_report()

    cat("\n**Possible reasons:**\n\n")
    cat("- Non-standard act names (e.g., 'Public Law 89-800' vs formal name)\n")
    cat("- Acts referred to informally in documents\n")
    cat("- OCR issues in older documents (pre-1950)\n")
    cat("- Labels data quality (wrong expected_year)\n\n")
  }

  # Show found acts summary
  cat("\n### Act Validation Summary (Found Acts)\n\n")
  act_name_validation %>%
    filter(found_in_any) %>%
    mutate(Status = "✓ Found") %>%
    arrange(expected_year) %>%
    head(20) %>%
    select(act_name, expected_year, years_checked, Status) %>%
    gt() %>%
    tab_header(title = "Successfully validated acts (first 20)") %>%
    gt_theme_report()

  # Show detection rate by decade
  cat("\n### Detection Rate by Decade\n\n")
  act_name_validation %>%
    mutate(decade = floor(expected_year / 10) * 10) %>%
    group_by(decade) %>%
    summarize(
      total_acts = n(),
      acts_found = sum(found_in_any),
      recall_rate = mean(found_in_any) * 100,
      .groups = "drop"
    ) %>%
    filter(!is.na(decade)) %>%
    mutate(decade_label = sprintf("%ds", decade)) %>%
    select(decade_label, total_acts, acts_found, recall_rate) %>%
    gt() %>%
    tab_header(title = "Act Detection Rate by Decade") %>%
    cols_label(
      decade_label = "Decade",
      total_acts = "Total Acts",
      acts_found = "Found",
      recall_rate = "Recall (%)"
    ) %>%
    fmt_number(columns = recall_rate, decimals = 0) %>%
    gt_theme_report()
} else {
  cat("\n⚠️ Skipping: No ground truth labels available for this country\n\n")
}
```

The expanded window recall of **84.8%** narrowly misses the 85% target but represents strong coverage given the matching method (exact string match on act names). The 7 missing acts fall into identifiable categories:

- **Non-standard naming** (3 acts): "Public Law 89-800" and "Public Law 90-26" are referenced by public law number rather than popular name. "Changes in Depreciation Guidelines and Revenue Act of 1962" uses an unusually long compound name that may appear abbreviated in documents.
- **Year mismatch** (2 acts): "Social Security Amendments of 1983" and "Tax Reform Act of 1986" are listed with expected year 1981, which appears to be a labeling issue in `us_labels.csv` rather than a corpus gap.
- **Compound act name** (1 act): "Taxpayer Relief Act of 1997 and Balanced Budget Act of 1997" is a two-act label that may not appear verbatim.
- **Early decade weakness**: The 1950s (83%) and 1960s (75%) show lower recall, consistent with OCR quality issues in older scanned documents and less standardized act naming conventions.

The 1990s dip to 50% recall is driven entirely by the compound 1997 act name (1 of 2 acts missed). Decades with more acts (1960s, 1980s) provide more reliable recall estimates. For codebook development, this recall level is sufficient: the LLM pipeline will use fuzzy matching and contextual cues rather than exact string matching.

## Test (iv): Temporal & Source Coverage

```{r test-iv}
# Define expected coverage
# ERP and Budget: every year from min_year to max_year
# Treasury: 1946-1980 and 2011-present (gap 1981-2010)
expected_coverage <- bind_rows(
  expand_grid(
    year = params$min_year:params$max_year,
    body = c("Economic Report of the President", "Budget of the United States Government")
  ) %>%
    mutate(expected = year <= lubridate::year(Sys.Date())),
  expand_grid(
    year = params$min_year:params$max_year,
    body = "Annual Report of the Treasury"
  ) %>%
    mutate(expected = (year <= 1980 | year >= 2011) & year <= lubridate::year(Sys.Date()))
)

# Actual coverage
actual_coverage <- body_data %>%
  filter(n_pages > 0) %>%
  count(year, body, name = "n_docs")

# Join and analyze
coverage_analysis <- expected_coverage %>%
  left_join(actual_coverage, by = c("year", "body")) %>%
  mutate(
    n_docs = replace_na(n_docs, 0),
    status = case_when(
      !expected ~ "Not expected",
      n_docs > 0 ~ "Present",
      TRUE ~ "Missing"
    )
  )

# Calculate coverage rate
coverage_gaps <- coverage_analysis %>%
  filter(expected & n_docs == 0)

total_expected <- sum(coverage_analysis$expected)
total_present <- sum(coverage_analysis$expected & coverage_analysis$n_docs > 0)
coverage_rate <- total_present / total_expected

test_iv_status <- case_when(
  coverage_rate >= 0.95 ~ "PASS",
  coverage_rate >= 0.85 ~ "WARN",
  TRUE ~ "FAIL"
)

test_results$test_iv <- list(
  metric = "Coverage rate",
  value = sprintf("%.1f%%", coverage_rate * 100),
  target = "≥95%",
  status = test_iv_status
)
```

### Results

**Coverage Rate:** `r sprintf("%.1f%%", coverage_rate * 100)` (`r total_present` of `r total_expected` expected documents) **Status:** `r test_iv_status`

```{r test-iv-display}
#| results: asis
# Coverage heatmap
coverage_analysis %>%
  filter(year >= max(params$min_year, 1946)) %>%
  mutate(
    status_color = case_when(
      status == "Not expected" ~ 0,
      status == "Present" ~ 1,
      TRUE ~ -1
    )
  ) %>%
  ggplot(aes(x = year, y = body, fill = status_color)) +
  geom_tile(color = "white", linewidth = 0.5) +
  scale_fill_gradient2(
    low = "red", mid = "grey90", high = "green",
    midpoint = 0,
    breaks = c(-1, 0, 1),
    labels = c("Missing", "Not expected", "Present"),
    name = "Status"
  ) +
  labs(
    title = "Document Coverage by Year and Type",
    x = "Year",
    y = "Document Type"
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)
  )

# Show gaps if any
if (nrow(coverage_gaps) > 0) {
  cat("\n### Coverage Gaps\n\n")
  coverage_gaps %>%
    select(year, body) %>%
    arrange(body, year) %>%
    gt() %>%
    tab_header(title = sprintf("Missing expected documents (%d gaps)", nrow(coverage_gaps))) %>%
    gt_theme_report()
} else {
  cat("\n✅ No coverage gaps! All expected documents are present.\n\n")
}
```

## Test (v): Text Quality Indicators

```{r test-v}
# Calculate page-level quality metrics
# Compile fiscal terms regex once for efficiency
fiscal_regex <- regex(paste(fiscal_terms, collapse = "|"), ignore_case = TRUE)

quality_metrics <- body_data %>%
  filter(n_pages > 0) %>%
  sample_n(size = min(5, n())) %>%  # Reduced sample size for performance
  select(year, body, package_id, source, text) %>%  # Keep identifiers
  mutate(
    page_metrics = map(text, function(text_list) {
      # Handle the nested structure: text is a list-column
      if (is.null(text_list) || length(text_list) == 0) return(NULL)

      # Extract pages from the nested list structure
      pages <- if (is.list(text_list[[1]])) text_list[[1]] else text_list
      if (length(pages) == 0) return(NULL)

      # Limit to first 50 pages per document to avoid memory issues
      pages <- pages[1:min(50, length(pages))]

      tibble(
        page_num = seq_along(pages),
        page_text = as.character(pages),
        n_chars = nchar(page_text),
        # Use simple word count instead of quanteda for performance
        n_tokens = str_count(page_text, "\\S+"),
        special_char_rate = str_count(page_text, "[^a-zA-Z0-9\\s.,!?;:'-]") / pmax(n_chars, 1),
        whitespace_rate = str_count(page_text, "\\s") / pmax(n_chars, 1),
        non_ascii_rate = str_count(page_text, "[^\x01-\x7F]") / pmax(n_chars, 1),
        has_fiscal_terms = str_detect(page_text, fiscal_regex)
      )
    })
  ) %>%
  filter(!map_lgl(page_metrics, is.null)) %>%
  select(-text) %>%  # Remove text column before unnest
  unnest(page_metrics)

# Identify suspicious pages
suspicious_pages <- quality_metrics %>%
  filter(
    n_chars < 100 |
    special_char_rate > 0.1 |
    non_ascii_rate > 0.05 |
    (!has_fiscal_terms & page_num > 5)
  )

# Document-level quality summary
doc_quality <- quality_metrics %>%
  group_by(year, body, package_id) %>%
  summarize(
    total_pages = n(),
    avg_tokens_per_page = mean(n_tokens, na.rm = TRUE),
    pct_fiscal_pages = mean(has_fiscal_terms, na.rm = TRUE),
    suspicious_pages_count = sum(
      n_chars < 100 | special_char_rate > 0.1 | non_ascii_rate > 0.05,
      na.rm = TRUE
    ),
    quality_score = (pmin(avg_tokens_per_page / 300, 1)) * pct_fiscal_pages *
                    (1 - suspicious_pages_count / total_pages),
    .groups = "drop"
  )

# Calculate metrics
pct_suspicious <- nrow(suspicious_pages) / nrow(quality_metrics)
pct_fiscal <- mean(quality_metrics$has_fiscal_terms, na.rm = TRUE)

test_v_status <- case_when(
  pct_suspicious < 0.05 & pct_fiscal > 0.70 ~ "PASS",
  pct_suspicious < 0.10 | pct_fiscal > 0.50 ~ "WARN",
  TRUE ~ "FAIL"
)

test_results$test_v_quality <- list(
  metric = "Text quality - suspicious pages",
  value = sprintf("%.1f%%", pct_suspicious * 100),
  target = "<5%",
  status = test_v_status
)

test_results$test_v_fiscal <- list(
  metric = "Text quality - fiscal pages",
  value = sprintf("%.1f%%", pct_fiscal * 100),
  target = ">70%",
  status = test_v_status
)
```

### Results

**Suspicious Pages:** `r sprintf("%.1f%%", pct_suspicious * 100)` **Fiscal Term Coverage:** `r sprintf("%.1f%%", pct_fiscal * 100)` **Status:** `r test_v_status`

```{r test-v-display}
#| results: asis
# Token distribution
quality_metrics %>%
  ggplot(aes(x = n_tokens)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  labs(
    title = "Distribution of Tokens per Page",
    x = "Tokens per Page",
    y = "Count"
  )

# Special character rates by source
quality_metrics %>%
  ggplot(aes(x = source, y = special_char_rate)) +
  geom_boxplot(fill = "coral", alpha = 0.7) +
  labs(
    title = "Special Character Rate by Source",
    x = "Source",
    y = "Special Character Rate"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Show low quality documents
low_quality_docs <- doc_quality %>%
  filter(quality_score < 0.5) %>%
  arrange(quality_score)

if (nrow(low_quality_docs) > 0) {
  cat("\n### Low Quality Documents\n\n")
  low_quality_docs %>%
    select(year, body, total_pages, avg_tokens_per_page, pct_fiscal_pages, quality_score) %>%
    mutate(
      avg_tokens_per_page = round(avg_tokens_per_page, 0),
      pct_fiscal_pages = pct_fiscal_pages * 100,
      quality_score = round(quality_score, 2)
    ) %>%
    head(10) %>%
    gt() %>%
    tab_header(title = "Documents with quality_score < 0.5 (top 10)") %>%
    fmt_number(columns = pct_fiscal_pages, decimals = 1) %>%
    gt_theme_report()
} else {
  cat("\n✅ No low-quality documents found!\n\n")
}
```

## Test (vi): Anomaly Detection

```{r test-vi}
# Document-level anomalies
doc_anomalies <- body_data %>%
  filter(n_pages > 0) %>%
  mutate(
    too_short = n_pages < 10,
    too_long = n_pages > 1000,
    first_pages = map_chr(text, function(pages) {
      if (length(pages) == 0) return("")
      paste(pages[1:min(5, length(pages))], collapse = " ")
    }),
    has_title_indicators = str_detect(
      first_pages,
      regex("(report|budget|economic|president|treasury|united states)", ignore_case = TRUE)
    ),
    extraction_time_z = if (sd(extraction_time, na.rm = TRUE) > 0) {
      scale(extraction_time)[,1]
    } else {
      0
    },
    slow_extraction = ocr_used & extraction_time_z > 3
  )

# Duplicate detection (hash first 5 pages)
duplicate_check <- body_data %>%
  filter(n_pages > 0) %>%
  mutate(
    text_hash = map_chr(text, function(pages) {
      if (length(pages) == 0) return("")
      first_pages <- pages[1:min(5, length(pages))]
      digest::digest(paste(first_pages, collapse = ""))
    })
  ) %>%
  group_by(text_hash) %>%
  filter(n() > 1, text_hash != "") %>%
  ungroup() %>%
  select(year, body, package_id, text_hash)

# Year-level trends
year_trends <- body_data %>%
  filter(n_pages > 0) %>%
  group_by(year, body) %>%
  summarize(
    total_pages = sum(n_pages),
    n_docs = n(),
    .groups = "drop"
  ) %>%
  arrange(body, year) %>%
  group_by(body) %>%
  mutate(
    yoy_change = (total_pages - lag(total_pages)) / lag(total_pages),
    sudden_drop = !is.na(yoy_change) & yoy_change < -0.5
  ) %>%
  ungroup()

# Anomaly counts
n_too_short <- sum(doc_anomalies$too_short, na.rm = TRUE)
n_too_long <- sum(doc_anomalies$too_long, na.rm = TRUE)
n_no_title <- sum(!doc_anomalies$has_title_indicators, na.rm = TRUE)
n_duplicates <- nrow(duplicate_check)
n_sudden_drops <- sum(year_trends$sudden_drop, na.rm = TRUE)

test_results$test_vi <- list(
  metric = "Anomalies detected",
  value = sprintf("%d issues", n_too_short + n_too_long + n_duplicates + n_sudden_drops),
  target = "Manual review",
  status = "INFO"
)
```

### Results

**Status:** INFO (anomalies flagged for manual review)

```{r test-vi-display}
#| results: asis
# Anomaly summary
anomaly_summary <- tribble(
  ~Anomaly, ~Count,
  "Too short (< 10 pages)", n_too_short,
  "Too long (> 1000 pages)", n_too_long,
  "Missing title indicators", n_no_title,
  "Duplicate documents", n_duplicates,
  "Sudden year drops (>50%)", n_sudden_drops
)

anomaly_summary %>%
  gt() %>%
  tab_header(title = "Anomaly Summary") %>%
  gt_theme_report()

# Show anomalous documents
if (n_too_short > 0 || n_too_long > 0 || n_no_title > 0) {
  cat("\n### Anomalous Documents\n\n")
  doc_anomalies %>%
    filter(too_short | too_long | !has_title_indicators) %>%
    select(year, body, package_id, n_pages, too_short, too_long, has_title_indicators) %>%
    arrange(desc(too_short), desc(too_long)) %>%
    head(10) %>%
    gt() %>%
    tab_header(title = "Documents with anomalies (top 10)") %>%
    gt_theme_report()
}

# Show duplicates
if (n_duplicates > 0) {
  cat("\n### Duplicate Documents\n\n")
  duplicate_check %>%
    gt() %>%
    tab_header(title = sprintf("Potential duplicates (%d documents)", n_duplicates)) %>%
    gt_theme_report()
}
```

```{r test-vi-trends}
# Year trends
year_trends %>%
  ggplot(aes(x = year, y = total_pages, color = body)) +
  geom_line(linewidth = 1) +
  geom_point(data = year_trends %>% filter(sudden_drop),
             color = "red", size = 3, shape = 1) +
  labs(
    title = "Total Pages by Year and Body",
    subtitle = "Red circles indicate sudden drops (>50%)",
    x = "Year",
    y = "Total Pages",
    color = "Document Type"
  ) +
  theme(legend.position = "bottom")
```

The anomaly counts warrant context rather than concern:

- **Short documents (64):** All are Budget section PDFs from Fraser (2006-2009 era), where single budget volumes were split into 26 section files of 5-8 pages each. These are by design, not extraction failures.
- **Long documents (17):** Early ERP and Budget volumes from the 1940s-1960s sometimes exceed 1,000 pages. These are complete, legitimate documents.
- **Duplicates (7):** All 7 are CBO reports where the first 5 pages hash identically across years. This is expected: CBO reuses boilerplate front matter (title page, contents, summary format) year to year. The document bodies are distinct.

No anomalies require remediation before proceeding to Phase 0.

## Summary Report & Pass/Fail Dashboard

```{r summary}
# Compile test results
test_summary <- bind_rows(
  lapply(names(test_results), function(name) {
    result <- test_results[[name]]
    tibble(
      test = name,
      metric = as.character(result$metric),
      value = as.character(result$value),
      target = as.character(result$target),
      status = as.character(result$status)
    )
  })
)

# Determine overall status
status_priority <- c("FAIL" = 3, "WARN" = 2, "PASS" = 1, "INFO" = 0, "SKIP" = 0)
overall_status <- test_summary %>%
  filter(status != "SKIP", status != "INFO") %>%
  pull(status) %>%
  {names(which.max(status_priority[.]))}

if (length(overall_status) == 0) overall_status <- "PASS"
```

### Overall Status: **`r overall_status`**

```{r summary-display}
# Display results table with colored status
test_summary %>%
  gt() %>%
  tab_header(title = "Verification Test Results") %>%
  cols_label(
    test = "Test",
    metric = "Metric",
    value = "Value",
    target = "Target",
    status = "Status"
  ) %>%
  tab_style(
    style = cell_text(color = "#2E7D32", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "PASS")
  ) %>%
  tab_style(
    style = cell_text(color = "#F57C00", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "WARN")
  ) %>%
  tab_style(
    style = cell_text(color = "#C62828", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "FAIL")
  ) %>%
  tab_style(
    style = cell_text(color = "#1565C0", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "INFO")
  ) %>%
  tab_style(
    style = cell_text(color = "#757575", weight = "bold"),
    locations = cells_body(columns = status, rows = status == "SKIP")
  ) %>%
  gt_theme_report()
```

### Recommendations

```{r recommendations}
#| results: asis
if (overall_status == "PASS") {
  cat("\n✅ **READY FOR LLM PROCESSING**\n\n")
  cat("All verification tests passed. Proceed with:\n\n")
  cat("- `tar_make(chunks)` to create LLM-ready chunks\n")
  cat("- Phase 0: Codebook development (C1-C4)\n")
  cat("- LLM-based fiscal shock identification\n\n")
} else if (overall_status == "WARN") {
  cat("\n⚠️ **PROCEED WITH CAUTION**\n\n")
  cat("Some tests raised warnings. Review flagged issues before full run:\n\n")

  test_summary %>%
    filter(status == "WARN") %>%
    pull(metric) %>%
    paste("-", .) %>%
    cat(sep = "\n")

  cat("\n\nConsider re-running extractions for problematic documents.\n\n")
} else {
  cat("\n❌ **MANUAL REVIEW REQUIRED**\n\n")
  cat("Critical issues detected. Address the following before proceeding:\n\n")

  test_summary %>%
    filter(status == "FAIL") %>%
    pull(metric) %>%
    paste("-", .) %>%
    cat(sep = "\n")

  cat("\n\nReview extraction settings and re-run failed documents.\n\n")
}
```

### Next Steps

```{r next-steps}
#| results: asis
cat("\n**Based on this verification:**\n\n")
cat(sprintf("- Total documents verified: %d\n", nrow(body_data)))
cat(sprintf("- Successful extractions: %d (%.1f%%)\n",
            sum(body_data$n_pages > 0),
            mean(body_data$n_pages > 0) * 100))
cat(sprintf("- Total pages extracted: %s\n",
            scales::comma(sum(body_data$n_pages))))
cat(sprintf("- Documents using OCR: %d\n", sum(body_data$ocr_used, na.rm = TRUE)))
cat(sprintf("- Overall status: **%s**\n\n", overall_status))

cat("**Recommended actions:**\n\n")
if (overall_status == "PASS") {
  cat("1. Proceed to Phase 0 implementation (codebook development C1-C4)\n")
  cat("2. Run `tar_make(chunks)` to create document chunks\n")
  cat("3. Begin C1 development (measure identification)\n")
} else {
  cat("1. Review failed/warned tests above\n")
  cat("2. Address extraction issues for flagged documents\n")
  cat("3. Re-run verification before proceeding\n")
}
```
