---
title: "Training Data Overview"
subtitle: "Data Composition and Transformation Pipeline for Phase 0 (C1-C4 Framework)"
date: today
date-format: long
execute:
  cache: false
  warning: false
  message: false
---

## Overview

This document provides a comprehensive overview of the training data pipeline for Phase 0, documenting how raw source data (`us_labels`, `us_shocks`, `us_body`) is transformed into evaluation-ready datasets for the C1-C4 codebook framework.

**Purpose:**

- Transparently describe all data transformations
- Document observation-level changes (passage to act, document to chunk, etc.)
- Explain key assumptions and design decisions
- Visualize data flow and composition at each stage

**Key Transformation:**

The most significant transformation is from **passage-level** (us_labels with 340 passages describing 44 unique acts) to **act-level** (aligned_data with 44 rows, one per act). Multiple passages describing the same act are grouped and concatenated into a single observation.

```{r setup}
#| label: setup
#| cache: false

library(targets)
library(tidyverse)
library(gt)
library(here)

here::i_am("notebooks/data_overview.qmd")
tar_config_set(store = here("_targets"))
source(here("R/gt_theme.R"))
set_theme(theme_minimal())

# Source data
us_labels <- tar_read(us_labels)
us_shocks <- tar_read(us_shocks)
us_body <- tar_read(us_body)

# Derived data
aligned_data <- tar_read(aligned_data)

# C1 pipeline
chunks <- tar_read(chunks)
c1_chunk_data <- tar_read(c1_chunk_data)
```


## Data Composition and Transformation Pipeline

This section documents how the raw source data (`us_labels`, `us_shocks`, `us_body`) is transformed into evaluation-ready datasets. We describe the transformation logic, observation-level changes, key assumptions, and provide visual summaries at each stage.

### Data Flow Overview

The data flows through multiple transformation stages, changing observation levels and structure:

```{r flow-summary}
#| echo: false

flow_summary <- tibble(
  `Data Component` = c(
    "us_labels", "us_shocks", "us_body",
    "aligned_data",
    "chunks",
    "c1_chunk_data"
  ),
  `Observation Level` = c(
    "Passage (multiple per act)", "Quarter (multiple per act)", "Document (one per year-body)",
    "Act (one row per act)",
    "Chunk (sliding window over pages)",
    "Tiered chunks (list: tier1, tier2, negatives)"
  ),
  `N Observations` = c(
    nrow(us_labels), nrow(us_shocks), nrow(us_body),
    nrow(aligned_data),
    nrow(chunks),
    sum(nrow(c1_chunk_data$tier1), nrow(c1_chunk_data$tier2), nrow(c1_chunk_data$negatives))
  ),
  `N Unique Acts` = c(
    n_distinct(us_labels$act_name),
    n_distinct(us_shocks$act_name),
    NA_integer_,
    nrow(aligned_data),
    NA_integer_,
    n_distinct(c(c1_chunk_data$tier1$act_name, c1_chunk_data$tier2$act_name))
  ),
  Purpose = c(
    "Human-labeled passages describing acts",
    "Romer & Romer shock magnitudes/timing",
    "Extracted text from government PDFs",
    "Joined labels + shocks at act level",
    "Sliding-window document chunks for LLM input",
    "Tier-assigned chunks for C1 LOOCV evaluation"
  )
)

flow_summary %>%
  mutate(
    `N Observations` = scales::comma(`N Observations`),
    `N Unique Acts` = ifelse(is.na(`N Unique Acts`), "\u2014", as.character(`N Unique Acts`))
  ) %>%
  gt() %>%
  tab_header(
    title = "Data Transformation Pipeline Overview",
    subtitle = "Observation levels and row counts at each stage"
  ) %>%
  cols_label(
    `Data Component` = "Dataset",
    `Observation Level` = "Unit of Observation",
    `N Observations` = "Row Count",
    `N Unique Acts` = "Unique Acts",
    Purpose = "Purpose"
  ) %>%
  tab_style(
    style = cell_fill(color = "#f0f0f0"),
    locations = cells_body(rows = `Data Component` %in% c("us_labels", "us_shocks", "us_body"))
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f5e9"),
    locations = cells_body(rows = `Data Component` == "aligned_data")
  ) %>%
  tab_style(
    style = cell_fill(color = "#e3f2fd"),
    locations = cells_body(rows = `Data Component` %in% c("chunks", "c1_chunk_data"))
  ) %>%
  gt_theme_report()
```

**Key Transformation:**

The most significant transformation is from **passage-level** (us_labels) to **act-level** (aligned_data). Multiple passages describing the same act are grouped and concatenated into a single observation.


### Component 1: Source Data (us_labels)

**Observation Level:** Passage (multiple rows per act)

**Structure:** Human-labeled passages from Romer & Romer (2010) dataset describing motivations for US fiscal policy acts.

```{r us-labels-structure}
# Show structure
us_labels_structure <- us_labels %>%
  summarize(
    n_passages = n(),
    n_unique_acts = n_distinct(act_name),
    passages_per_act_min = min(table(act_name)),
    passages_per_act_median = median(table(act_name)),
    passages_per_act_max = max(table(act_name)),
    n_categories = n_distinct(category),
    date_range = paste(min(date, na.rm = TRUE), "to", max(date, na.rm = TRUE))
  )

tibble(
  Metric = c(
    "Total passages",
    "Unique acts",
    "Passages per act (min/median/max)",
    "Motivation categories",
    "Date range"
  ),
  Value = c(
    scales::comma(us_labels_structure$n_passages),
    as.character(us_labels_structure$n_unique_acts),
    sprintf("%d / %d / %d",
            us_labels_structure$passages_per_act_min,
            us_labels_structure$passages_per_act_median,
            us_labels_structure$passages_per_act_max),
    as.character(us_labels_structure$n_categories),
    us_labels_structure$date_range
  )
) %>%
  gt() %>%
  tab_header(title = "us_labels: Passage-Level Structure") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()

# Distribution of passages per act
us_labels %>%
  count(act_name) %>%
  ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1, fill = "steelblue", alpha = 0.7) +
  labs(
    title = "Distribution of Passages per Act (us_labels)",
    x = "Number of Passages",
    y = "Number of Acts"
  )

# Category distribution
us_labels %>%
  count(category, sort = TRUE) %>%
  mutate(pct = n / sum(n) * 100) %>%
  gt() %>%
  tab_header(title = "Passage Distribution by Category (us_labels)") %>%
  cols_label(category = "Motivation Category", n = "Passages", pct = "Percent") %>%
  fmt_number(columns = pct, decimals = 1, pattern = "{x}%") %>%
  gt_theme_report()
```

**Example Passages (Act with Multiple Passages):**

```{r us-labels-example}
#| results: asis

# Show example act with multiple passages
example_act <- us_labels %>%
  group_by(act_name) %>%
  filter(n() >= 3) %>%
  ungroup() %>%
  filter(act_name == first(act_name)) %>%
  arrange(act_name)

cat(sprintf("**Example: %s (%d passages)**\n\n", example_act$act_name[1], nrow(example_act)))

for (i in 1:min(3, nrow(example_act))) {
  cat(sprintf("**Passage %d:**\n\n", i))
  cat(sprintf("> %s\n\n", str_trunc(example_act$motivation[i], 400)))
}
```

**Key Variables:**

- `act_name`: Name of the fiscal act
- `motivation`: Text passage describing the motivation
- `category`: Motivation category (Spending-driven, Countercyclical, Deficit-driven, Long-run)
- `exogeneity`: Exogenous or Endogenous classification
- `date`: Date of the act
- `source`: Source document for the passage


### Component 2: Source Data (us_shocks)

**Observation Level:** Quarter (multiple rows per act, one per quarter of fiscal impact)

**Structure:** Romer & Romer (2010) dataset with magnitudes and timing of fiscal shocks.

```{r us-shocks-structure}
us_shocks_structure <- us_shocks %>%
  summarize(
    n_quarters = n(),
    n_unique_acts = n_distinct(act_name),
    quarters_per_act_min = min(table(act_name)),
    quarters_per_act_median = median(table(act_name)),
    quarters_per_act_max = max(table(act_name)),
    date_range = paste(min(date_signed, na.rm = TRUE), "to", max(date_signed, na.rm = TRUE))
  )

tibble(
  Metric = c(
    "Total quarters/shocks",
    "Unique acts",
    "Quarters per act (min/median/max)",
    "Date range"
  ),
  Value = c(
    scales::comma(us_shocks_structure$n_quarters),
    as.character(us_shocks_structure$n_unique_acts),
    sprintf("%d / %d / %d",
            us_shocks_structure$quarters_per_act_min,
            us_shocks_structure$quarters_per_act_median,
            us_shocks_structure$quarters_per_act_max),
    us_shocks_structure$date_range
  )
) %>%
  gt() %>%
  tab_header(title = "us_shocks: Quarter-Level Structure") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()

# Distribution of quarters per act
us_shocks %>%
  count(act_name) %>%
  ggplot(aes(x = n)) +
  geom_histogram(binwidth = 1, fill = "coral", alpha = 0.7) +
  labs(
    title = "Distribution of Quarters per Act (us_shocks)",
    subtitle = "Most acts have fiscal impacts in multiple quarters",
    x = "Number of Quarters",
    y = "Number of Acts"
  )
```

**Example: Act with Multiple Quarters**

```{r us-shocks-example}
# Show an act with multiple quarters
example_shock_act <- us_shocks %>%
  group_by(act_name) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  slice(1:min(5, n())) %>%
  select(
    act_name,
    date_signed,
    change_in_liabilities_quarter,
    change_in_liabilities_billion,
    change_in_liabilities_category
  )

example_shock_act %>%
  mutate(
    change_in_liabilities_billion = sprintf("$%.2fB", change_in_liabilities_billion)
  ) %>%
  gt() %>%
  tab_header(
    title = sprintf("Example: %s", example_shock_act$act_name[1]),
    subtitle = "Multiple quarters of fiscal impact"
  ) %>%
  cols_label(
    act_name = "Act",
    date_signed = "Date Signed",
    change_in_liabilities_quarter = "Impact Quarter",
    change_in_liabilities_billion = "Magnitude",
    change_in_liabilities_category = "Category"
  ) %>%
  gt_theme_report()
```

**Key Variables:**

- `act_name`: Name of the fiscal act
- `date_signed`: Date the act was signed
- `change_in_liabilities_quarter`: Quarter of fiscal impact
- `change_in_liabilities_billion`: Magnitude in billions of dollars
- `present_value_billion`: Present value of impact
- `change_in_liabilities_category`: Motivation category
- `change_in_liabilities_exo`: Exogenous flag


### Component 3: Source Data (us_body)

**Observation Level:** PDF volume (one row per extracted PDF file)

**Structure:** Extracted text from US government PDF documents (Economic Reports, Budget documents, Treasury Reports). Each row is one PDF file. Most documents correspond to a single PDF, but multi-volume Budget documents (2006-2009) produce multiple rows sharing the same `package_id`. The `package_id` is the unique document identifier carried forward into the chunking pipeline.

```{r us-body-structure}
us_body_structure <- us_body %>%
  summarize(
    n_rows = n(),
    n_packages = n_distinct(package_id),
    n_with_text = sum(n_pages > 0),
    pct_extracted = mean(n_pages > 0) * 100,
    year_range = paste(min(year), "to", max(year)),
    n_bodies = n_distinct(body)
  )

tibble(
  Metric = c(
    "Rows (PDF volumes)",
    "Unique documents (package_id)",
    "Rows with extracted text (n_pages > 0)",
    "Extraction success rate",
    "Year range",
    "Document bodies"
  ),
  Value = c(
    scales::comma(us_body_structure$n_rows),
    scales::comma(us_body_structure$n_packages),
    sprintf("%s (%.1f%%)",
            scales::comma(us_body_structure$n_with_text),
            us_body_structure$pct_extracted),
    sprintf("%.1f%%", us_body_structure$pct_extracted),
    us_body_structure$year_range,
    as.character(us_body_structure$n_bodies)
  )
) %>%
  gt() %>%
  tab_header(title = "us_body: PDF Volume Structure") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()

# Document distribution by body and year
us_body %>%
  count(body, year) %>%
  ggplot(aes(x = year, fill = body)) +
  geom_bar(stat = "count", position = "stack") +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Document Distribution by Body and Year (us_body)",
    x = "Year",
    y = "Number of Documents",
    fill = "Document Body"
  ) +
  theme(legend.position = "bottom")

# Pages per document distribution
us_body %>%
  filter(n_pages > 0) %>%
  ggplot(aes(x = n_pages)) +
  geom_histogram(bins = 30, fill = "darkgreen", alpha = 0.7) +
  labs(
    title = "Distribution of Pages per Document (us_body)",
    subtitle = "Only documents with successful extraction",
    x = "Number of Pages",
    y = "Number of Documents"
  )
```

**Purpose:** Source corpus for chunking. Chunks serve as LLM inputs for C1 evaluation and production inference.


### Component 4: aligned_data (Labels + Shocks)

**Transformation:** Passage-level to Act-level

**Process:** Join `us_labels` with `us_shocks` to combine human motivations with shock magnitudes/timing.

**Key Transformation Logic** (from `align_labels_shocks()` in `R/prepare_training_data.R`):

1. **Act name cleaning:** Normalize whitespace in both datasets
2. **Exact matching:** Join on cleaned act names
3. **Fuzzy matching:** For unmatched acts, use Jaro-Winkler similarity (threshold = 0.85)
4. **Collapse quarters:** Use first quarter's data as representative (us_shocks has multiple rows per act)
5. **Group passages:** Concatenate all passages for each act with `\n\n` separator

**Assumptions:**

- **One representative quarter:** When acts have multiple quarters of impact (us_shocks), we use the first quarter's magnitude and timing as the "primary" shock. This is an acceptable simplification because the LOOCV evaluation tests measure identification, not multi-quarter extraction.
- **Passage concatenation:** All passages describing the same act are concatenated into a single text field. This preserves all human-labeled context while creating a single observation per act.
- **Fuzzy matching threshold (0.85):** Acts with name similarity of at least 0.85 are considered matches. This accommodates minor spelling variations (e.g., "Revenue Act of 1964" vs "Revenue Act, 1964").

```{r aligned-data-structure}
# Show transformation statistics
aligned_stats <- tibble(
  Stage = c("us_labels (passages)", "us_shocks (quarters)", "aligned_data (acts)"),
  `Observation Level` = c("Passage", "Quarter", "Act"),
  `Row Count` = c(nrow(us_labels), nrow(us_shocks), nrow(aligned_data)),
  `Unique Acts` = c(
    n_distinct(us_labels$act_name),
    n_distinct(us_shocks$act_name),
    nrow(aligned_data)
  )
)

aligned_stats %>%
  mutate(`Row Count` = scales::comma(`Row Count`)) %>%
  gt() %>%
  tab_header(
    title = "Transformation: Passage/Quarter to Act Level",
    subtitle = "Observation level change in aligned_data"
  ) %>%
  cols_label(
    Stage = "Dataset",
    `Observation Level` = "Unit",
    `Row Count` = "Rows",
    `Unique Acts` = "Acts"
  ) %>%
  gt_theme_report()

# Show passage concatenation statistics
aligned_data %>%
  select(act_name, n_passages, passages_text) %>%
  slice_head(n = 5) %>%
  mutate(
    text_chars = nchar(passages_text),
    text_preview = str_trunc(passages_text, 150)
  ) %>%
  select(act_name, n_passages, text_chars, text_preview) %>%
  gt() %>%
  tab_header(
    title = "Example: Concatenated Passages per Act",
    subtitle = "Multiple passages combined into single text field"
  ) %>%
  cols_label(
    act_name = "Act Name",
    n_passages = "# Passages",
    text_chars = "Chars",
    text_preview = "Text Preview"
  ) %>%
  gt_theme_report()

# Distribution of passages per act
aligned_data %>%
  ggplot(aes(x = n_passages)) +
  geom_histogram(binwidth = 1, fill = "purple", alpha = 0.7) +
  labs(
    title = "Passages per Act in aligned_data",
    subtitle = "Distribution shows how many passages were concatenated",
    x = "Number of Passages Concatenated",
    y = "Number of Acts"
  )
```

**Example: Before and After Alignment**

```{r aligned-example}
#| results: asis

# Pick an act with multiple passages
example_act_name <- aligned_data %>%
  filter(n_passages >= 3) %>%
  slice(1) %>%
  pull(act_name)

# Show original passages
original_passages <- us_labels %>%
  filter(act_name == example_act_name) %>%
  select(act_name, motivation, category)

cat(sprintf("#### Before Alignment: %s\n\n", example_act_name))
cat(sprintf("**Source:** us_labels (passage-level, %d rows)\n\n", nrow(original_passages)))

for (i in 1:nrow(original_passages)) {
  cat(sprintf("**Passage %d:**\n\n", i))
  cat(sprintf("> %s\n\n", str_trunc(original_passages$motivation[i], 300)))
}

# Show aligned data
aligned_example <- aligned_data %>%
  filter(act_name == example_act_name)

cat("#### After Alignment\n\n")
cat(sprintf("**Source:** aligned_data (act-level, 1 row)\n\n"))

cat("- **Passages concatenated:** ", aligned_example$n_passages, "\n")
cat("- **Total characters:** ", scales::comma(nchar(aligned_example$passages_text)), "\n")
cat("- **Motivation category:** ", aligned_example$motivation_category, "\n")
cat("- **Magnitude:** ", sprintf("$%.2fB", aligned_example$magnitude_billions), "\n")
cat("- **Exogenous:** ", aligned_example$exogenous, "\n\n")

cat("**Concatenated text (first 500 chars):**\n\n")
cat(sprintf("> %s...\n\n", str_trunc(aligned_example$passages_text, 500)))
```

**Alignment Quality:**

```{r aligned-quality}
# Show matching statistics
tibble(
  Metric = c(
    "Acts in us_labels",
    "Acts in us_shocks",
    "Acts successfully aligned",
    "Alignment rate"
  ),
  Value = c(
    as.character(n_distinct(us_labels$act_name)),
    as.character(n_distinct(us_shocks$act_name)),
    as.character(nrow(aligned_data)),
    sprintf("%.1f%%", nrow(aligned_data) / n_distinct(us_labels$act_name) * 100)
  )
) %>%
  gt() %>%
  tab_header(title = "Alignment Statistics") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()
```


### Component 5: chunks (Sliding Window Document Chunks)

**Transformation:** Document-level to Chunk-level (sliding window)

**Process:** Split each document in `us_body` into overlapping chunks that fit within the LLM context window, using `make_chunks()` in `R/make_chunks.R`.

**Key Logic:**

1. **Sliding window:** 50-page window with 10-page overlap between consecutive chunks
2. **Token limit:** 160,000 tokens (advisory; warns but does not split)
3. **Page separators:** Pages joined with `--- PAGE BREAK ---` markers for page number tracking
4. **Token estimation:** Characters / 4 as approximate token count

**Design Rationale:**

Chunks are the unit of input for C1 evaluation and production. Chunk-based evaluation tests the model under production conditions: long documents containing fiscal measures amid economic commentary, forecasts, and administrative content. This matches the H&K "eval-must-match-production" principle.

**Context window budget:**

The largest chunks approach 155K tokens. With prompt overhead (~9K tokens for codebook, few-shot examples, and system instructions) and a 2K output buffer, all chunks fit within the 200K context window used by both Haiku 3.5 (S1/S2/S3 evaluation) and Sonnet/Opus (production).

| Component | Approx. tokens |
|:----------|---------------:|
| Codebook (YAML) | 4,100 |
| Few-shot examples (~5) | 3,100 |
| System prompt | 125 |
| Output buffer | 2,000 |
| **Overhead total** | **9,300** |
| **Available for chunk** | **190,700** |
| **Largest chunk in corpus** | **155,277** |

```{r chunks-structure}
chunk_stats <- tibble(
  Metric = c(
    "Total chunks",
    "Source PDF volumes (rows in us_body)",
    "Unique documents (package_id)",
    "Chunks per document (min/median/max)",
    "Token count (min/median/max)",
    "Pages per chunk (min/median/max)"
  ),
  Value = c(
    scales::comma(nrow(chunks)),
    sprintf("%s (\u2192 all %s chunked)",
            scales::comma(nrow(us_body)),
            scales::comma(nrow(us_body))),
    sprintf("%s (matches us_body above)",
            scales::comma(n_distinct(chunks$doc_id))),
    sprintf("%d / %.0f / %d",
            min(table(chunks$doc_id)),
            median(table(chunks$doc_id)),
            max(table(chunks$doc_id))),
    sprintf("%s / %s / %s",
            scales::comma(min(chunks$approx_tokens)),
            scales::comma(median(chunks$approx_tokens)),
            scales::comma(max(chunks$approx_tokens))),
    sprintf("%d / %.0f / %d",
            min(chunks$n_pages),
            median(chunks$n_pages),
            max(chunks$n_pages))
  )
)

chunk_stats %>%
  gt() %>%
  tab_header(title = "chunks: Sliding Window Structure") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()
```

```{r chunks-token-dist}
chunks %>%
  ggplot(aes(x = approx_tokens)) +
  geom_histogram(bins = 40, fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = 190700, linetype = "dashed", color = "red") +
  annotate("text", x = 190700, y = Inf, label = "Available after overhead",
           hjust = 1.1, vjust = 1.5, color = "red", size = 3) +
  labs(
    title = "Token Count Distribution Across Chunks",
    x = "Approximate Tokens",
    y = "Number of Chunks"
  )
```

```{r chunks-by-year}
chunks %>%
  count(year) %>%
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = "darkgreen", alpha = 0.7) +
  labs(
    title = "Chunks by Year",
    subtitle = "Coverage across the document corpus",
    x = "Year",
    y = "Number of Chunks"
  )
```


### Component 6: c1_chunk_data (Positive and Negative Chunks for C1 Evaluation)

**Transformation:** Identify which chunks are positive (contain a known fiscal measure) and which are negative (contain no fiscal policy discussion), for use in C1 LOOCV evaluation.

**Process:** `prepare_c1_chunk_data()` in `R/identify_chunk_tiers.R` classifies each chunk based on its relationship to the 44 labeled acts in `aligned_data`.

**Positive identification (two tiers):**

We identify positive chunks at two levels of stringency:

1. **Tier 1 (verbatim passage):** Search for the first 100 characters of each labeled passage (from `aligned_data$passages_text`) as a substring within year-adjacent chunks. Fallback to 60 characters if no match. These are gold-standard positives: we know the exact @romer2010 passage text appears in the chunk.
2. **Tier 2 (act name mention):** Search remaining chunks for act name matches using three strategies: whitespace-normalized substring matching (handles OCR line breaks), subcomponent decomposition for compound act names (split on "and", parenthetical extraction, embedded formal names, Public Law numbers), and co-occurrence matching for event-type names (e.g., "expiration" AND "excess profits"). All strategies require at least 2 fiscal keyword co-occurrences to filter incidental mentions.

Tier 2 adopts a deliberately lax inclusion criterion. Relying on Tier 1 alone would limit positive chunks to those containing verbatim passages from @romer2010, which are short excerpts that the original authors selected to illustrate each act's motivation. Many chunks that substantively discuss an act's magnitude, timing, or implementation would be missed. By including Tier 2, we ensure the model sees the full range of fiscal discussion surrounding each act, which is essential for downstream codebooks (C2 motivation, C3 timing, C4 magnitude) that require richer context than the labeled passages alone provide.

**Negative identification:**

3. **Negative:** All chunks not assigned to Tier 1 or Tier 2. Each negative is tagged with a continuous `key_density` score (fiscal keyword count / total words) measuring fiscal vocabulary concentration. This replaces the previous binary relevance-key exclusion with a gradient suitable for stratified error analysis in S3.

**Evaluation targets:**

- **Tier 1 Recall** (target: 95%) measures whether the model can find known fiscal measures embedded in long documents. Failures here indicate a fundamental problem.
- **Tier 2 Recall** measures generalization beyond verbatim labeled text. Lower recall is expected and informative.
- **Combined Recall** (target: 90%) is the primary gating metric for the C1 funnel.
- **Negative chunks** test precision: can the model correctly reject chunks that do not describe a specific fiscal measure? Negatives span a gradient from clean (zero fiscal vocabulary) to fiscally rich (high key density), and `key_density` enables stratified precision analysis in S3 to identify the threshold where the model's discrimination breaks down.

```{r c1-tier-summary}
c1_chunk_data$summary %>%
  gt() %>%
  tab_header(
    title = "C1 Chunk Tier Distribution",
    subtitle = "How chunks are assigned to evaluation tiers"
  ) %>%
  cols_label(
    category = "Tier",
    n_chunks = "Chunks",
    pct = "Percent"
  ) %>%
  fmt_number(columns = pct, decimals = 1, pattern = "{x}%") %>%
  gt_theme_report()
```

```{r c1-tier1-detail}
tier1_stats <- c1_chunk_data$tier1 %>%
  group_by(act_name) %>%
  summarize(
    n_chunks = n(),
    n_passages = max(passage_idx),
    avg_tokens = mean(approx_tokens),
    .groups = "drop"
  )

tibble(
  Metric = c(
    "Tier 1 chunks (total)",
    "Acts with Tier 1 matches",
    "Acts without Tier 1 matches",
    "Chunks per act (min/median/max)",
    "Avg tokens per Tier 1 chunk"
  ),
  Value = c(
    as.character(nrow(c1_chunk_data$tier1)),
    as.character(nrow(tier1_stats)),
    as.character(nrow(aligned_data) - nrow(tier1_stats)),
    sprintf("%d / %.0f / %d",
            min(tier1_stats$n_chunks),
            median(tier1_stats$n_chunks),
            max(tier1_stats$n_chunks)),
    scales::comma(mean(c1_chunk_data$tier1$approx_tokens))
  )
) %>%
  gt() %>%
  tab_header(title = "Tier 1: Verbatim Passage Matches") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()
```

```{r c1-tier2-detail}
tier2_stats <- c1_chunk_data$tier2 %>%
  group_by(act_name) %>%
  summarize(
    n_chunks = n(),
    avg_tokens = mean(approx_tokens),
    .groups = "drop"
  )

tibble(
  Metric = c(
    "Tier 2 chunks (total)",
    "Acts with Tier 2 matches",
    "Chunks per act (min/median/max)",
    "Avg tokens per Tier 2 chunk"
  ),
  Value = c(
    as.character(nrow(c1_chunk_data$tier2)),
    as.character(nrow(tier2_stats)),
    if (nrow(tier2_stats) > 0)
      sprintf("%d / %.0f / %d",
              min(tier2_stats$n_chunks),
              median(tier2_stats$n_chunks),
              max(tier2_stats$n_chunks))
    else "N/A",
    if (nrow(c1_chunk_data$tier2) > 0)
      scales::comma(mean(c1_chunk_data$tier2$approx_tokens))
    else "N/A"
  )
) %>%
  gt() %>%
  tab_header(title = "Tier 2: Act Name Mention Matches") %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()
```

```{r c1-negative-detail}
neg_by_source <- c1_chunk_data$negatives %>%
  count(source_type, sort = TRUE) %>%
  mutate(pct = n / sum(n) * 100)

neg_by_source %>%
  gt() %>%
  tab_header(
    title = "Negative Chunks by Source Type",
    subtitle = "Stratified by document body"
  ) %>%
  cols_label(
    source_type = "Source",
    n = "Chunks",
    pct = "Percent"
  ) %>%
  fmt_number(columns = pct, decimals = 1, pattern = "{x}%") %>%
  gt_theme_report()
```

```{r}
#| label: tbl-negative-density
#| tbl-cap: "Negative Chunk Key Density Distribution"

c1_chunk_data$negative_density_summary %>%
  gt() %>%
  cols_label(
    density_bin = "Key Density Bin",
    n = "Chunks",
    pct = "Percent"
  ) %>%
  fmt_number(columns = pct, decimals = 1, pattern = "{x}%") %>%
  gt_theme_report()
```

**Verification: Are Negatives Correctly Excluded from Positives?**

Negatives now intentionally include chunks with fiscal vocabulary (measured by `key_density`). The verification question is not "are negatives clean?" but "are negatives correctly excluded from positives?" We check whether any negative chunks contain a known act name that should have been caught by Tier 2. A small number is acceptable: act names appearing outside the ±5 year window or without 2+ fiscal keywords are correctly excluded from Tier 2.

```{r c1-neg-verification}
# Check a sample of negative chunks for act name contamination
act_names_pattern <- paste0(
  "(",
  paste(tolower(aligned_data$act_name), collapse = "|"),
  ")"
)

neg_sample <- c1_chunk_data$negatives %>%
  slice_sample(n = min(100, nrow(c1_chunk_data$negatives)))

neg_contamination <- neg_sample %>%
  mutate(
    has_act_name = str_detect(tolower(text), act_names_pattern),
    has_enacted = str_detect(text, regex("\\b(enacted|signed\\s+into\\s+law)\\b",
                                          ignore_case = TRUE))
  ) %>%
  summarize(
    n_checked = n(),
    n_with_act_name = sum(has_act_name),
    n_with_enacted = sum(has_enacted)
  )

tibble(
  Check = c(
    "Negative chunks sampled",
    "Contains known act name",
    "Contains 'enacted/signed into law'"
  ),
  Result = c(
    as.character(neg_contamination$n_checked),
    sprintf("%d (%.1f%%)", neg_contamination$n_with_act_name,
            100 * neg_contamination$n_with_act_name / neg_contamination$n_checked),
    sprintf("%d (%.1f%%)", neg_contamination$n_with_enacted,
            100 * neg_contamination$n_with_enacted / neg_contamination$n_checked)
  ),
  Status = c(
    "\u2014",
    ifelse(neg_contamination$n_with_act_name == 0, "Pass", "Review"),
    ifelse(neg_contamination$n_with_enacted == 0, "Pass", "Review")
  )
) %>%
  gt() %>%
  tab_header(
    title = "Negative Chunk Contamination Check",
    subtitle = "Verify no act names were missed by Tier 2 matching"
  ) %>%
  cols_label(Check = "Check", Result = "Result", Status = "Status") %>%
  gt_theme_report()
```

::: {.callout-note}
Negatives are all chunks not assigned to Tier 1 or Tier 2. Each negative is tagged with `key_density` (fiscal keyword concentration) for downstream stratification. Some negatives contain fiscal vocabulary by design; the key question is whether any contain a known act name that Tier 2 should have caught. Act name mentions in negatives are expected when they fall outside the ±5 year window or lack the required 2+ fiscal keyword co-occurrences.
:::


## Summary: Key Transformations

```{r transformation-summary}
tibble(
  Transformation = c(
    "us_labels \u2192 aligned_data",
    "us_shocks \u2192 aligned_data",
    "us_body \u2192 chunks",
    "chunks \u2192 c1_chunk_data"
  ),
  `Observation Change` = c(
    "Passage-level \u2192 Act-level",
    "Quarter-level \u2192 Act-level",
    "Document-level \u2192 Chunk-level",
    "Chunk-level \u2192 Tiered evaluation sets"
  ),
  `Key Operation` = c(
    "Group passages by act, concatenate",
    "Use first quarter as representative",
    "Sliding window (50 pages, 10 overlap, 40K token limit)",
    "Verbatim match (T1), name/subcomponent/co-occurrence match (T2), all remaining (Neg)"
  ),
  `Rows In` = c(
    as.character(nrow(us_labels)),
    as.character(nrow(us_shocks)),
    sprintf("%d docs", nrow(us_body)),
    scales::comma(nrow(chunks))
  ),
  `Rows Out` = c(
    as.character(nrow(aligned_data)),
    as.character(nrow(aligned_data)),
    scales::comma(nrow(chunks)),
    sprintf("T1: %d, T2: %d, Neg: %d",
            nrow(c1_chunk_data$tier1),
            nrow(c1_chunk_data$tier2),
            nrow(c1_chunk_data$negatives))
  )
) %>%
  gt() %>%
  tab_header(
    title = "Data Transformation Summary",
    subtitle = "Key changes in observation level and row counts"
  ) %>%
  cols_label(
    Transformation = "Transformation",
    `Observation Change` = "Level Change",
    `Key Operation` = "Operation",
    `Rows In` = "Input Rows",
    `Rows Out` = "Output Rows"
  ) %>%
  gt_theme_report()
```


## Critical Findings

### Finding 1: Dataset Size {.unnumbered}

**44 labeled acts** are available in `us_labels`, not 126 from the full `us_shocks` dataset. This is the number of acts with human-labeled motivation passages, which is the ground truth for all codebook evaluation.

**Impact on C1-C4:**

- **C1 (Measure ID):** LOOCV on 44 acts with chunk-based inputs. Targets: Combined Recall at least 90%, Tier 1 Recall at least 95%, Precision at least 70%.
- **C2 (Motivation):** With only 6 Countercyclical acts, this category will have limited evaluation power. LOOCV helps by using all 44 acts.
- **C3/C4 (Timing/Magnitude):** 41 of 44 acts have complete timing and magnitude data.
- **Few-shot budget:** Each LOOCV fold uses 43 acts for training. With 5 examples per class, this consumes only ~12% of available acts.

### Finding 2: Passage Concatenation Creates Richer Context {.unnumbered}

- **Median 8 passages per act** (range 1-25), yielding roughly 5K tokens per act
- More context improves motivation classification (C2) and information extraction (C3/C4)
- Claude's 200K context window easily handles the longest acts
- Higher API costs per call, but LOOCV keeps total calls manageable (44 folds)

### Finding 3: Multi-Quarter Acts {.unnumbered}

- us_shocks shows **median 7 quarters per act**, max 32 quarters
- aligned_data uses only the **first quarter** as representative
- C3 (Timing) and C4 (Magnitude) will need to handle multi-quarter extraction in their codebook design
- This is a known simplification; the C3/C4 output schemas should support multiple implementation phases

### Finding 4: Chunk Tier Coverage {.unnumbered}

```{r finding-tier-coverage}
# Acts with and without Tier 1 coverage
tier1_acts <- n_distinct(c1_chunk_data$tier1$act_name)
tier2_acts <- n_distinct(c1_chunk_data$tier2$act_name)
total_acts <- nrow(aligned_data)

tibble(
  Metric = c(
    "Acts with Tier 1 (verbatim) coverage",
    "Acts with Tier 2 (name mention) coverage",
    "Acts with any chunk coverage",
    "Total labeled acts"
  ),
  Value = c(
    sprintf("%d / %d (%.0f%%)", tier1_acts, total_acts, 100 * tier1_acts / total_acts),
    sprintf("%d / %d (%.0f%%)", tier2_acts, total_acts, 100 * tier2_acts / total_acts),
    sprintf("%d / %d (%.0f%%)",
            n_distinct(c(c1_chunk_data$tier1$act_name, c1_chunk_data$tier2$act_name)),
            total_acts,
            100 * n_distinct(c(c1_chunk_data$tier1$act_name, c1_chunk_data$tier2$act_name)) / total_acts),
    as.character(total_acts)
  )
) %>%
  gt() %>%
  tab_header(
    title = "C1 Chunk Tier Coverage",
    subtitle = "How many acts have chunks assigned to each tier"
  ) %>%
  cols_label(Metric = "Metric", Value = "Value") %>%
  gt_theme_report()
```

Tier 1 coverage indicates how many labeled acts have their verbatim passage text recoverable in the chunked corpus. Acts without Tier 1 coverage may have passages that span chunk boundaries or come from documents not yet extracted. Tier 2 provides additional coverage through whitespace-normalized substring matching, subcomponent decomposition for compound act names, and co-occurrence matching for event-type names. See `notebooks/verify_chunk_tiers.qmd` for detailed per-act analysis of matching mechanisms and temporal consistency.


## Conclusion

The training data is **ready for Phase 0 codebook evaluation** with the C1-C4 framework.

**Strengths:**

- 100% alignment rate between labels and shocks
- Chunk-based evaluation matches production conditions (long documents, fiscal measures amid noise)
- Three-tier system provides diagnostic granularity (Tier 1 gold standard, Tier 2 generalization, Negatives for precision)
- Tier 2 matching uses whitespace normalization, subcomponent decomposition, and co-occurrence matching to achieve near-complete act coverage
- Negative chunks tagged with continuous `key_density` for stratified precision analysis in S3 (no arbitrary relevance-key exclusion)
- Rich passage context (median 8 passages per act) preserved in aligned_data for few-shot examples
- LOOCV design uses all 44 acts for both training and evaluation

**Known Limitations:**

- **44 acts total:** Small sample, but LOOCV maximizes use of available data
- **Countercyclical under-representation:** Only 6 acts in this motivation category
- **First-quarter simplification:** Multi-quarter acts reduced to single representative
- **Tier 1 coverage gaps:** Some acts may lack verbatim passage matches if passages span chunk boundaries or come from unextracted documents
- **No ground truth for Malaysia:** Phase 2 requires expert validation (target: at least 80% agreement on C1)

**Evaluation Strategy:** Each codebook (C1-C4) is evaluated through the Halterman & Keith 5-stage framework: S0 (codebook prep), S1 (behavioral tests), S2 (LOOCV on 44 acts with bootstrap CIs), S3 (error analysis and ablation), with S4 (fine-tuning) reserved as a last resort. C1 uses chunk-level inputs for S1-S3, with passage-level few-shot examples and chunk-level test inputs.
