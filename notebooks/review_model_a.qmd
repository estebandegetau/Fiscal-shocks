---
title: "Model A Evaluation: Act Detection"
subtitle: "Performance Assessment Against Phase 0 Success Criteria"
date: today
execute:
  cache: refresh
  warning: false
  message: false
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
---

## Executive Summary

This notebook evaluates **Model A (Act Detection)**, a binary classifier that determines whether a text passage describes a specific fiscal policy act.

**Primary Success Criterion:** F1 Score > 0.85 on test set

**Model Configuration:**

- LLM: Claude Sonnet 4 (claude-sonnet-4-20250514)
- Approach: Few-shot prompting (10 positive + 10 negative examples)
- Temperature: 0.0 (deterministic)
- Classification threshold: 0.5 (confidence)

**Datasets:**

- Training: Used for few-shot example selection
- Validation: 55 passages (10 acts + 45 negatives) - for model tuning
- Test: 34 passages (6 acts + 28 negatives) - final evaluation

**Results Summary:**

**Test Set Performance:**

- F1 Score: **0.857** ✅ (narrowly exceeds 0.85 threshold)
- Precision: **0.750** ❌ (below 0.80 target)
- Recall: **1.000** ✅ (perfect - no acts missed)
- Accuracy: **0.941** (94.1% correct)

**Validation Set Performance:**

- F1 Score: **0.833** (slightly below threshold)
- Precision: **0.714** (below target)
- Recall: **1.000** (perfect)
- Accuracy: **0.927** (92.7% correct)

**Key Finding:** Model achieves the primary success criterion (F1 > 0.85) on the test set but with a narrow margin (0.007). Perfect recall across both datasets ensures no fiscal acts are missed, but moderate precision (7-9% false positive rate) indicates some non-acts are incorrectly flagged. The model is **conditionally acceptable** for Phase 0 with recommendations for precision improvement before production scaling.

---

```{r setup}
library(targets)
library(tidyverse)
library(gt)
library(here)

here::i_am("notebooks/review_model_a.qmd")
tar_config_set(store = here("_targets"))

# Load evaluation results
model_a_eval_val <- tar_read(model_a_eval_val)
model_a_eval_test <- tar_read(model_a_eval_test)
model_a_predictions_val <- tar_read(model_a_predictions_val)
model_a_predictions_test <- tar_read(model_a_predictions_test)

# Helper function for status badges
status_badge <- function(condition, target) {
  if (condition) {
    sprintf("✅ PASS (%.3f > %.2f)", condition, target)
  } else {
    sprintf("❌ FAIL (%.3f < %.2f)", condition, target)
  }
}
```

---

## Performance Metrics

### Validation Set Results

The validation set is used for iterative model improvement before touching the test set.

```{r val-metrics}
# Extract metrics
val_metrics <- tibble(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(
    model_a_eval_val$precision,
    model_a_eval_val$recall,
    model_a_eval_val$f1_score,
    model_a_eval_val$accuracy
  ),
  Target = c(0.80, 0.90, 0.85, NA),
  `Pass/Fail` = c(
    ifelse(model_a_eval_val$precision >= 0.80, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_val$recall >= 0.90, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_val$f1_score >= 0.85, "✅ PASS", "❌ FAIL"),
    "—"
  )
)

val_metrics %>%
  mutate(
    Value = sprintf("%.3f", Value),
    Target = ifelse(is.na(Target), "—", sprintf("%.2f", Target))
  ) %>%
  gt() %>%
  tab_header(
    title = "Validation Set Performance",
    subtitle = sprintf("N = %d passages (%d acts, %d negatives)",
                      model_a_eval_val$n_total,
                      model_a_eval_val$n_positive,
                      model_a_eval_val$n_negative)
  ) %>%
  cols_label(
    Metric = "Metric",
    Value = "Observed",
    Target = "Target",
    `Pass/Fail` = "Status"
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f5e9"),
    locations = cells_body(rows = grepl("PASS", `Pass/Fail`))
  ) %>%
  tab_style(
    style = cell_fill(color = "#ffebee"),
    locations = cells_body(rows = grepl("FAIL", `Pass/Fail`))
  )
```

**Interpretation (Validation Set):**

The validation set shows F1 = 0.833, falling just short of the 0.85 target. The model demonstrates a clear performance pattern:

**✅ Strengths:**

1. **Perfect Recall (1.0):** The model identifies all 10 fiscal acts without missing a single one (0 false negatives). This is critical for the research application, as missing fiscal shocks would create gaps in the dataset.

2. **High Accuracy (0.927):** Overall, 51 of 55 passages are classified correctly, indicating the model makes correct decisions most of the time.

3. **Excellent Calibration:** High-confidence predictions (≥0.8) achieve 94.9% accuracy on 39 passages, showing the model's confidence scores are well-calibrated and trustworthy.

**⚠️ Areas for Improvement:**

1. **Moderate Precision (0.714):** The model produces 4 false positives out of 45 negative examples (8.9% false positive rate). These are non-acts incorrectly flagged as containing fiscal acts.

2. **Below F1 Target:** At 0.833, the F1 score is 0.017 points below the 0.85 threshold, driven primarily by the precision shortfall.

**Root Cause Analysis:**

The false positives likely stem from passages that:

- Mention fiscal acts in historical or comparative context without describing the actual policy change
- Discuss proposals or recommendations that were never enacted
- Describe existing policies without new legislative action

This pattern suggests the few-shot examples may need additional negative examples demonstrating these edge cases.

```{r val-interpretation, results='asis'}
val_f1 <- model_a_eval_val$f1_score
val_precision <- model_a_eval_val$precision
val_recall <- model_a_eval_val$recall

if (val_f1 >= 0.85) {
  cat("✅ **Model exceeds success criterion** (F1 =", sprintf("%.3f", val_f1), "> 0.85)\n\n")
} else {
  cat("❌ **Model below success criterion** (F1 =", sprintf("%.3f", val_f1), "< 0.85)\n\n")
}

if (val_precision >= 0.80 && val_recall >= 0.90) {
  cat("- **Balanced performance:** Both precision and recall meet targets.\n")
} else if (val_precision < 0.80) {
  cat("- ⚠️ **Low precision:** Model is flagging too many false positives (incorrectly identifying non-acts as acts).\n")
} else if (val_recall < 0.90) {
  cat("- ⚠️ **Low recall:** Model is missing real fiscal acts (false negatives).\n")
}

# Calculate false positive and false negative rates
cm_val <- model_a_eval_val$confusion_matrix
fp_rate <- cm_val[2,1] / sum(cm_val[,1])
fn_rate <- cm_val[1,2] / sum(cm_val[,2])

cat(sprintf("- **False positive rate:** %.1f%% (%d/%d negatives incorrectly flagged)\n",
            fp_rate * 100, cm_val[2,1], sum(cm_val[,1])))
cat(sprintf("- **False negative rate:** %.1f%% (%d/%d acts missed)\n",
            fn_rate * 100, cm_val[1,2], sum(cm_val[,2])))
```

---

### Test Set Results

The test set provides the **final, unbiased evaluation** of Model A performance.

```{r test-metrics}
# Extract metrics
test_metrics <- tibble(
  Metric = c("Precision", "Recall", "F1 Score", "Accuracy"),
  Value = c(
    model_a_eval_test$precision,
    model_a_eval_test$recall,
    model_a_eval_test$f1_score,
    model_a_eval_test$accuracy
  ),
  Target = c(0.80, 0.90, 0.85, NA),
  `Pass/Fail` = c(
    ifelse(model_a_eval_test$precision >= 0.80, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_test$recall >= 0.90, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_test$f1_score >= 0.85, "✅ PASS", "❌ FAIL"),
    "—"
  )
)

test_metrics %>%
  mutate(
    Value = sprintf("%.3f", Value),
    Target = ifelse(is.na(Target), "—", sprintf("%.2f", Target))
  ) %>%
  gt() %>%
  tab_header(
    title = "Test Set Performance (FINAL EVALUATION)",
    subtitle = sprintf("N = %d passages (%d acts, %d negatives)",
                      model_a_eval_test$n_total,
                      model_a_eval_test$n_positive,
                      model_a_eval_test$n_negative)
  ) %>%
  cols_label(
    Metric = "Metric",
    Value = "Observed",
    Target = "Target",
    `Pass/Fail` = "Status"
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f5e9"),
    locations = cells_body(rows = grepl("PASS", `Pass/Fail`))
  ) %>%
  tab_style(
    style = cell_fill(color = "#ffebee"),
    locations = cells_body(rows = grepl("FAIL", `Pass/Fail`))
  ) %>%
  tab_options(
    table.background.color = "#fffef0"
  )
```

**Interpretation (Test Set):**

The test set provides the final, unbiased evaluation of Model A. With F1 = 0.857, the model **narrowly passes** the primary success criterion (F1 > 0.85), but the results reveal important nuances:

**✅ Major Strengths:**

1. **Perfect Recall (1.0):** The model successfully identifies all 6 fiscal acts in the test set without missing any. This is the most critical metric for the research application, as false negatives would create data gaps. Zero false negatives demonstrates the model is conservative in rejecting passages - it doesn't prematurely dismiss potential fiscal acts.

2. **High Accuracy (0.941):** With 32 of 34 passages classified correctly, the overall error rate is only 5.9%, showing the model makes sound decisions in the vast majority of cases.

3. **Consistent with Validation Set:** The test set shows similar patterns to validation (perfect recall, moderate precision), suggesting the model's behavior is stable and generalizes across data splits.

4. **Well-Calibrated Confidence:** All test set predictions fall in the high-confidence range (0.8-1.0), with 100% accuracy in the (0.9, 1.0] bin. This indicates the model is appropriately confident when making predictions.

**⚠️ Limitations:**

1. **Marginal Precision (0.75):** With 2 false positives out of 28 negative examples (7.1% false positive rate), precision falls short of the 0.80 target. This means approximately 1 in 14 negative passages is incorrectly flagged as containing a fiscal act.

2. **Barely Passes F1 Threshold:** At 0.857, the F1 score exceeds 0.85 by only 0.007 points. This narrow margin suggests the model is on the borderline of acceptable performance.

3. **Small Test Set Size:** With only 6 acts, the test set has limited statistical power. The 95% confidence interval for F1 is likely ±0.10, meaning the true performance could range from 0.75-0.95. The validation set (10 acts) provides additional evidence, but both sets are small.

**Precision-Recall Tradeoff:**

The model exhibits a **high-recall, moderate-precision** profile:

- **Advantage:** No fiscal acts are missed, ensuring complete data coverage
- **Cost:** ~7-9% of reviewed passages will be false positives, requiring manual filtering

For the research application, this tradeoff is **acceptable** because:

- False negatives (missing acts) would be unrecoverable errors in the dataset
- False positives can be filtered out in downstream manual review
- The 7% false positive rate is manageable (vs. reviewing 100% of passages manually)

**Statistical Caveat:**

The test set contains only 6 positive examples, which limits the reliability of precision and recall estimates. However, the validation set (10 acts) shows nearly identical patterns (F1=0.833, recall=1.0, precision=0.714), providing corroborating evidence that the model's behavior is consistent.

```{r test-interpretation, results='asis'}
test_f1 <- model_a_eval_test$f1_score
test_precision <- model_a_eval_test$precision
test_recall <- model_a_eval_test$recall

if (test_f1 >= 0.85) {
  cat("✅ **PRIMARY SUCCESS CRITERION MET** (F1 =", sprintf("%.3f", test_f1), "> 0.85)\n\n")
  cat("Model A achieves the target performance for Phase 0. This indicates:\n\n")
  cat("- Few-shot prompting is effective for fiscal act detection\n")
  cat("- System prompt criteria are well-calibrated\n")
  cat("- Ready to proceed to Model B (Motivation Classification)\n\n")
} else {
  cat("❌ **PRIMARY SUCCESS CRITERION NOT MET** (F1 =", sprintf("%.3f", test_f1), "< 0.85)\n\n")
  cat("Model requires improvement before proceeding. Recommendations:\n\n")
  cat("- Add more few-shot examples (increase from 10 to 15-20 per class)\n")
  cat("- Refine system prompt with clearer edge case handling\n")
  cat("- Consider adjusting confidence threshold\n")
  cat("- Review false positives and false negatives for pattern identification\n\n")
}

# Calculate error rates
cm_test <- model_a_eval_test$confusion_matrix
fp_rate_test <- cm_test[2,1] / sum(cm_test[,1])
fn_rate_test <- cm_test[1,2] / sum(cm_test[,2])

cat(sprintf("- **False positive rate:** %.1f%% (%d/%d negatives incorrectly flagged)\n",
            fp_rate_test * 100, cm_test[2,1], sum(cm_test[,1])))
cat(sprintf("- **False negative rate:** %.1f%% (%d/%d acts missed)\n",
            fn_rate_test * 100, cm_test[1,2], sum(cm_test[,2])))

# Sample size caveat
if (model_a_eval_test$n_positive < 10) {
  cat("\n⚠️ **Note:** Small test set size (", model_a_eval_test$n_positive,
      " acts) means metrics have wide confidence intervals. Validation set results provide additional evidence.\n", sep = "")
}
```

---

## Confusion Matrices

### Validation Set

```{r val-confusion-matrix, fig.width=6, fig.height=5}
# Convert confusion matrix to data frame
cm_val_df <- as.data.frame(as.table(model_a_eval_val$confusion_matrix))

ggplot(cm_val_df, aes(x = Predicted, y = True, fill = Freq)) +
  geom_tile(color = "white", linewidth = 1.5) +
  geom_text(aes(label = Freq), color = "white", size = 12, fontface = "bold") +
  scale_fill_gradient(low = "#2c7bb6", high = "#d7191c", name = "Count") +
  scale_x_discrete(labels = c("No Act", "Act")) +
  scale_y_discrete(labels = c("No Act", "Act")) +
  labs(
    title = "Validation Set Confusion Matrix",
    subtitle = sprintf("F1 = %.3f | Precision = %.3f | Recall = %.3f",
                      model_a_eval_val$f1_score,
                      model_a_eval_val$precision,
                      model_a_eval_val$recall),
    x = "Predicted",
    y = "True Label"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    panel.grid = element_blank(),
    legend.position = "right"
  ) +
  coord_fixed()
```

### Test Set

```{r test-confusion-matrix, fig.width=6, fig.height=5}
# Convert confusion matrix to data frame
cm_test_df <- as.data.frame(as.table(model_a_eval_test$confusion_matrix))

ggplot(cm_test_df, aes(x = Predicted, y = True, fill = Freq)) +
  geom_tile(color = "white", linewidth = 1.5) +
  geom_text(aes(label = Freq), color = "white", size = 12, fontface = "bold") +
  scale_fill_gradient(low = "#2c7bb6", high = "#d7191c", name = "Count") +
  scale_x_discrete(labels = c("No Act", "Act")) +
  scale_y_discrete(labels = c("No Act", "Act")) +
  labs(
    title = "Test Set Confusion Matrix (FINAL)",
    subtitle = sprintf("F1 = %.3f | Precision = %.3f | Recall = %.3f",
                      model_a_eval_test$f1_score,
                      model_a_eval_test$precision,
                      model_a_eval_test$recall),
    x = "Predicted",
    y = "True Label"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    panel.grid = element_blank(),
    legend.position = "right"
  ) +
  coord_fixed()
```

**Confusion Matrix Interpretation:**

```{r cm-interpretation, results='asis'}
tp_test <- cm_test[2,2]
fp_test <- cm_test[2,1]
fn_test <- cm_test[1,2]
tn_test <- cm_test[1,1]

cat("**True Positives (TP):**", tp_test, "- Acts correctly identified\n\n")
cat("**True Negatives (TN):**", tn_test, "- Non-acts correctly rejected\n\n")
cat("**False Positives (FP):**", fp_test, "- Non-acts incorrectly flagged as acts\n")
if (fp_test > 0) {
  cat("  - Impact: Wasted effort reviewing passages that aren't actually fiscal acts\n")
  cat("  - Mitigation: Increase confidence threshold or add more negative examples\n\n")
} else {
  cat("  - ✅ Perfect precision on test set\n\n")
}

cat("**False Negatives (FN):**", fn_test, "- Acts incorrectly missed\n")
if (fn_test > 0) {
  cat("  - Impact: Missing real fiscal shocks in the dataset (critical error)\n")
  cat("  - Mitigation: Add more positive examples or lower confidence threshold\n\n")
} else {
  cat("  - ✅ Perfect recall on test set\n\n")
}
```

---

## Confidence Calibration

A well-calibrated model should have confidence scores that match actual accuracy. For example, passages classified with 90% confidence should be correct ~90% of the time.

### Validation Set Calibration

```{r val-calibration}
if (!is.null(model_a_eval_val$calibration) && nrow(model_a_eval_val$calibration) > 0) {
  model_a_eval_val$calibration %>%
    filter(n > 0) %>%
    mutate(
      confidence_bin = as.character(confidence_bin),
      accuracy = sprintf("%.1f%%", accuracy * 100),
      n = as.integer(n)
    ) %>%
    gt() %>%
    tab_header(
      title = "Confidence Calibration (Validation Set)",
      subtitle = "Does confidence match actual accuracy?"
    ) %>%
    cols_label(
      confidence_bin = "Confidence Range",
      n = "Count",
      accuracy = "Actual Accuracy"
    )
} else {
  cat("Calibration data not available for validation set.\n")
}
```

### Test Set Calibration

```{r test-calibration}
if (!is.null(model_a_eval_test$calibration) && nrow(model_a_eval_test$calibration) > 0) {
  model_a_eval_test$calibration %>%
    filter(n > 0) %>%
    mutate(
      confidence_bin = as.character(confidence_bin),
      accuracy = sprintf("%.1f%%", accuracy * 100),
      n = as.integer(n)
    ) %>%
    gt() %>%
    tab_header(
      title = "Confidence Calibration (Test Set)",
      subtitle = "Does confidence match actual accuracy?"
    ) %>%
    cols_label(
      confidence_bin = "Confidence Range",
      n = "Count",
      accuracy = "Actual Accuracy"
    )
} else {
  cat("Calibration data not available for test set.\n")
}
```

**Calibration Interpretation:**

Well-calibrated models show:

- High confidence (>0.8) → High accuracy (>90%)
- Low confidence (<0.6) → Lower accuracy (<70%)

If the model is **over-confident**, it assigns high confidence to incorrect predictions.
If the model is **under-confident**, it assigns low confidence to correct predictions.

For binary classification with temperature=0.0, Claude tends to be well-calibrated with confidence scores clustering near 0.9+ for clear cases.

---

## Error Analysis

### False Positives (Non-acts incorrectly flagged as acts)

```{r false-positives}
# False positives: negatives incorrectly flagged as acts
# We want the PREDICTED act name (act_name...10)
fp_examples <- model_a_predictions_test %>%
  filter(is_fiscal_act == 0, contains_act == TRUE) %>%
  mutate(
    text_preview = str_trunc(text, width = 100),
    confidence_fmt = sprintf("%.2f", confidence)
  ) %>%
  # Select predicted act name (from model output)
  select(text_preview, predicted_act = act_name...10, confidence = confidence_fmt, reasoning)

if (nrow(fp_examples) > 0) {
  fp_examples %>%
    gt() %>%
    tab_header(
      title = "False Positives (Test Set)",
      subtitle = sprintf("%d non-acts incorrectly identified as acts", nrow(fp_examples))
    ) %>%
    cols_label(
      text_preview = "Passage (preview)",
      predicted_act = "Predicted Act Name",
      confidence = "Confidence",
      reasoning = "Model Reasoning"
    ) %>%
    tab_options(table.width = pct(100))
} else {
  cat("✅ No false positives in test set - perfect precision!\n")
}
```

```{r fp-interpretation, results='asis'}
if (nrow(fp_examples) > 0) {
  cat("**Why did the model fail here?**\n\n")
  cat("Common patterns in false positives:\n\n")
  cat("- Passages mentioning act names in **historical context** without describing the policy change\n")
  cat("- **Proposals or recommendations** that were never enacted\n")
  cat("- General discussion of **existing policies** without new legislation\n\n")
  cat("**Recommended fixes:**\n\n")
  cat("1. Add few-shot examples of these edge cases to negative class\n")
  cat("2. Strengthen system prompt: \"Must describe actual policy CHANGE, not existing policy\"\n")
  cat("3. Add criterion: \"Must have implementation date or effective date\"\n")
}
```

---

### False Negatives (Acts incorrectly missed)

```{r false-negatives}
# False negatives: acts incorrectly missed
# We want the TRUE act name (act_name...3)
fn_examples <- model_a_predictions_test %>%
  filter(is_fiscal_act == 1, contains_act == FALSE) %>%
  mutate(
    text_preview = str_trunc(text, width = 100),
    confidence_fmt = sprintf("%.2f", confidence)
  ) %>%
  # Select true act name (from training data)
  select(text_preview, true_act = act_name...3, confidence = confidence_fmt, reasoning)

if (nrow(fn_examples) > 0) {
  fn_examples %>%
    gt() %>%
    tab_header(
      title = "False Negatives (Test Set)",
      subtitle = sprintf("%d acts incorrectly missed", nrow(fn_examples))
    ) %>%
    cols_label(
      text_preview = "Passage (preview)",
      true_act = "True Act Name",
      confidence = "Confidence",
      reasoning = "Model Reasoning"
    ) %>%
    tab_options(table.width = pct(100))
} else {
  cat("✅ No false negatives in test set - perfect recall!\n")
}
```

```{r fn-interpretation, results='asis'}
if (nrow(fn_examples) > 0) {
  cat("**Why did the model miss these acts?**\n\n")
  cat("Common patterns in false negatives:\n\n")
  cat("- Act mentioned **indirectly** or with non-standard naming\n")
  cat("- Very **short passages** lacking sufficient context\n")
  cat("- Acts described in **technical jargon** without explicit \"Act of YYYY\" language\n\n")
  cat("**Recommended fixes:**\n\n")
  cat("1. Add few-shot examples with varied act naming conventions\n")
  cat("2. Loosen system prompt criteria for exact naming (allow \"1964 tax legislation\")\n")
  cat("3. Add examples of terse, technical descriptions to training set\n")
}
```

---

## Confidence Distribution

How confident is the model in its predictions?

```{r confidence-dist, fig.width=8, fig.height=5}
# Combine val and test for larger sample
all_predictions <- bind_rows(
  model_a_predictions_val %>% mutate(dataset = "Validation"),
  model_a_predictions_test %>% mutate(dataset = "Test")
) %>%
  mutate(
    prediction_correct = (is_fiscal_act == 1 & contains_act == TRUE) |
                        (is_fiscal_act == 0 & contains_act == FALSE),
    true_class = ifelse(is_fiscal_act == 1, "Act", "Non-Act")
  )

ggplot(all_predictions, aes(x = confidence, fill = prediction_correct)) +
  geom_histogram(bins = 20, alpha = 0.7, position = "identity") +
  facet_wrap(~dataset, ncol = 1) +
  scale_fill_manual(
    values = c("TRUE" = "#4caf50", "FALSE" = "#f44336"),
    labels = c("Incorrect", "Correct"),
    name = "Prediction"
  ) +
  labs(
    title = "Confidence Distribution by Correctness",
    x = "Confidence Score",
    y = "Count"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "top"
  )
```

**Confidence Analysis:**

```{r confidence-analysis, results='asis'}
# Calculate confidence stats
high_conf_correct <- all_predictions %>%
  filter(confidence >= 0.8) %>%
  summarize(pct_correct = mean(prediction_correct))

low_conf_correct <- all_predictions %>%
  filter(confidence < 0.6) %>%
  summarize(pct_correct = mean(prediction_correct))

cat("**High confidence predictions (≥0.8):**\n\n")
if (nrow(all_predictions %>% filter(confidence >= 0.8)) > 0) {
  cat(sprintf("- %.1f%% correct\n",
              high_conf_correct$pct_correct * 100))
  cat("- These predictions are highly reliable\n\n")
} else {
  cat("- No high-confidence predictions\n\n")
}

cat("**Low confidence predictions (<0.6):**\n\n")
if (nrow(all_predictions %>% filter(confidence < 0.6)) > 0) {
  cat(sprintf("- %.1f%% correct\n",
              low_conf_correct$pct_correct * 100))
  cat("- These predictions should be manually reviewed\n\n")
} else {
  cat("- No low-confidence predictions (model is decisive)\n\n")
}

# Check for over/under-confidence
if (high_conf_correct$pct_correct < 0.85 && nrow(all_predictions %>% filter(confidence >= 0.8)) > 0) {
  cat("⚠️ **Over-confidence detected:** High-confidence predictions are not as accurate as confidence suggests.\n")
} else if (low_conf_correct$pct_correct > 0.7 && nrow(all_predictions %>% filter(confidence < 0.6)) > 0) {
  cat("⚠️ **Under-confidence detected:** Low-confidence predictions are actually quite accurate.\n")
}
```

---

## Cost Analysis

```{r cost-analysis}
# Load API cost logs (if available)
log_file <- here("logs", "api_calls.csv")

if (file.exists(log_file)) {
  api_costs <- read_csv(log_file, show_col_types = FALSE)

  # Filter for Model A calls (assuming they use the sonnet-4 model)
  model_a_costs <- api_costs %>%
    filter(grepl("sonnet-4", model)) %>%
    summarize(
      n_calls = n(),
      total_input_tokens = sum(input_tokens),
      total_output_tokens = sum(output_tokens),
      total_cost_usd = sum(cost_usd)
    )
} else {
  # Create placeholder if log doesn't exist yet
  model_a_costs <- tibble(
    n_calls = 0,
    total_input_tokens = 0,
    total_output_tokens = 0,
    total_cost_usd = 0
  )
}

model_a_costs %>%
  mutate(
    avg_cost_per_call = ifelse(n_calls > 0, total_cost_usd / n_calls, 0),
    avg_input_tokens = ifelse(n_calls > 0, total_input_tokens / n_calls, 0),
    avg_output_tokens = ifelse(n_calls > 0, total_output_tokens / n_calls, 0)
  ) %>%
  pivot_longer(everything(), names_to = "Metric", values_to = "Value") %>%
  mutate(
    Metric = case_when(
      Metric == "n_calls" ~ "API Calls",
      Metric == "total_input_tokens" ~ "Total Input Tokens",
      Metric == "total_output_tokens" ~ "Total Output Tokens",
      Metric == "total_cost_usd" ~ "Total Cost (USD)",
      Metric == "avg_cost_per_call" ~ "Avg Cost per Call (USD)",
      Metric == "avg_input_tokens" ~ "Avg Input Tokens per Call",
      Metric == "avg_output_tokens" ~ "Avg Output Tokens per Call",
      TRUE ~ Metric
    ),
    Value = ifelse(
      grepl("Cost", Metric),
      sprintf("$%.4f", Value),
      sprintf("%.0f", Value)
    )
  ) %>%
  gt() %>%
  tab_header(
    title = "Model A API Cost Summary",
    subtitle = "Claude Sonnet 4 costs"
  ) %>%
  cols_label(
    Metric = "Metric",
    Value = "Value"
  )
```

**Cost Interpretation:**

```{r cost-interp, results='asis'}
if (model_a_costs$n_calls > 0) {
  cat(sprintf("Actual costs for Model A evaluation: **$%.4f** for %d API calls\n\n",
              model_a_costs$total_cost_usd,
              model_a_costs$n_calls))
} else {
  cat("⚠️ No API cost data available yet. Costs will be logged after running predictions.\n\n")
}

cat("Expected costs per phase 0 plan (line 310):\n\n")
cat("- Validation + Test: ~$0.25 total\n")
cat("- Production scaling factor: 244 training examples → ~$0.60 for full dataset\n")
```

---

## Overall Assessment

```{r overall-assessment}
# Determine overall status
overall_pass <- model_a_eval_test$f1_score >= 0.85 &&
                model_a_eval_test$precision >= 0.80 &&
                model_a_eval_test$recall >= 0.90

assessment_df <- tibble(
  Criterion = c(
    "F1 Score > 0.85",
    "Precision > 0.80",
    "Recall > 0.90",
    "Overall Model A"
  ),
  Target = c(0.85, 0.80, 0.90, NA),
  Observed = c(
    model_a_eval_test$f1_score,
    model_a_eval_test$precision,
    model_a_eval_test$recall,
    NA
  ),
  Status = c(
    ifelse(model_a_eval_test$f1_score >= 0.85, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_test$precision >= 0.80, "✅ PASS", "❌ FAIL"),
    ifelse(model_a_eval_test$recall >= 0.90, "✅ PASS", "❌ FAIL"),
    ifelse(overall_pass, "✅ READY", "❌ NEEDS WORK")
  )
) %>%
  mutate(
    Observed = ifelse(is.na(Observed), "—", sprintf("%.3f", Observed)),
    Target = ifelse(is.na(Target), "—", sprintf("%.2f", Target))
  )

assessment_df %>%
  gt() %>%
  tab_header(
    title = "Phase 0 Success Criteria Assessment",
    subtitle = "Model A: Act Detection"
  ) %>%
  cols_label(
    Criterion = "Success Criterion",
    Target = "Target",
    Observed = "Test Set Result",
    Status = "Status"
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f5e9"),
    locations = cells_body(rows = grepl("PASS|READY", Status))
  ) %>%
  tab_style(
    style = cell_fill(color = "#ffebee"),
    locations = cells_body(rows = grepl("FAIL|NEEDS", Status))
  ) %>%
  tab_style(
    style = list(
      cell_fill(color = "#fff9c4"),
      cell_text(weight = "bold")
    ),
    locations = cells_body(rows = Criterion == "Overall Model A")
  )
```

---

## Recommendations

```{r recommendations, results='asis'}
if (overall_pass) {
  cat("### ✅ Model A Meets All Success Criteria\n\n")
  cat("**Next Steps:**\n\n")
  cat("1. **Proceed to Model B (Motivation Classification)** - Days 4-6 of Phase 0 plan\n")
  cat("2. **Archive Model A artifacts:**\n")
  cat("   - Save predictions: `tar_read(model_a_predictions_test)`\n")
  cat("   - Document few-shot examples used\n")
  cat("   - Export confusion matrix and metrics for final report\n")
  cat("3. **Production deployment notes:**\n")
  cat("   - Use confidence threshold = 0.5 (current default)\n")
  cat(sprintf("   - Expected precision: ~%.0f%%, recall: ~%.0f%%\n",
              model_a_eval_test$precision * 100,
              model_a_eval_test$recall * 100))
  cat("   - Flag predictions with confidence < 0.7 for manual review\n")
  cat("4. **Cost projections:**\n")
  if (model_a_costs$n_calls > 0) {
    cat(sprintf("   - Full US dataset (244 passages): ~$%.2f\n",
                model_a_costs$total_cost_usd / model_a_costs$n_calls * 244))
  } else {
    cat("   - Full US dataset (244 passages): ~$0.60 (estimated)\n")
  }
  cat("   - Malaysia dataset (est. 500 passages): ~$1.00\n\n")

} else {
  cat("### ❌ Model A Needs Improvement\n\n")
  cat("**Required Actions Before Proceeding:**\n\n")

  if (model_a_eval_test$precision < 0.80) {
    cat("**Fix Low Precision:**\n\n")
    cat("1. Add 5-10 more negative examples to few-shot set\n")
    cat("2. Focus on edge cases: proposals, historical mentions, existing policies\n")
    cat("3. Strengthen system prompt criteria (e.g., must mention implementation date)\n")
    cat("4. Consider increasing confidence threshold to 0.6-0.7\n\n")
  }

  if (model_a_eval_test$recall < 0.90) {
    cat("**Fix Low Recall:**\n\n")
    cat("1. Add 5-10 more positive examples with varied naming conventions\n")
    cat("2. Include examples of terse/technical act descriptions\n")
    cat("3. Relax system prompt to allow non-standard act naming\n")
    cat("4. Consider lowering confidence threshold to 0.4\n\n")
  }

  cat("**Testing Procedure:**\n\n")
  cat("1. Implement fixes above\n")
  cat("2. Regenerate few-shot examples: `tar_make(model_a_examples_file)`\n")
  cat("3. Re-run on validation set first: `tar_make(model_a_eval_val)`\n")
  cat("4. Only re-run test set after validation passes\n")
  cat("5. Document changes and re-evaluate\n\n")
}
```

---

## Overall Interpretation

**Model A Performance Summary:**

Model A (Act Detection) demonstrates **marginally acceptable performance** that technically meets the Phase 0 success criterion but reveals important areas requiring attention:

**Primary Finding: Narrow Pass with Consistent Patterns**

- **Test Set F1: 0.857** - Passes the 0.85 threshold by only 0.007 points
- **Validation Set F1: 0.833** - Falls short by 0.017 points
- **Pattern Consistency:** Both datasets show identical behavior (recall=1.0, precision=0.71-0.75)

**Key Performance Characteristics:**

1. **Perfect Recall (1.0) Across Both Sets**
   - Zero false negatives - no fiscal acts are missed
   - This is the most important metric for research completeness
   - Indicates the model is conservative in rejecting passages

2. **Moderate Precision (0.71-0.75)**
   - False positive rate: 7-9% of negative passages incorrectly flagged
   - Falls short of 0.80 precision target
   - Likely caused by edge cases: historical mentions, proposals, existing policies

3. **Small Sample Concerns**
   - Test set: 6 acts, 28 negatives (34 total)
   - Validation set: 10 acts, 45 negatives (55 total)
   - Wide confidence intervals (±10%) mean true F1 could be 0.75-0.95
   - Consistency across both sets provides some reassurance

**Research Application Suitability:**

The model's **high-recall, moderate-precision** profile is appropriate for the research task:

✅ **Acceptable for Phase 0 goals:**
- False negatives (missed acts) would create unrecoverable data gaps → Model avoids this
- False positives can be filtered in manual review → 7-9% FP rate is manageable
- Few-shot prompting approach is validated as viable
- Provides ~93% reduction in manual review burden (vs. 100% manual)

⚠️ **Concerns for production scaling:**
- Narrow margin (0.007 above threshold) suggests fragility
- Precision below target means downstream filtering burden
- Small test sets limit confidence in generalization
- May require refinement before scaling to new countries

**Decision Point:**

**Recommendation: Conditional Proceed to Model B**

Given that:
1. F1 technically meets the 0.85 threshold (though narrowly)
2. Perfect recall is maintained across both sets
3. Validation and test sets show consistent behavior
4. Phase 0 timeline constraints exist

We recommend **proceeding to Model B** while noting the following:

**Accept for Phase 0:**
- The high-recall profile fits research requirements
- False positives are acceptable with manual review
- Few-shot approach is validated

**Monitor for Phase 1:**
- Add more negative examples for precision improvement
- Test on larger validation sets before production deployment
- Consider ensemble approaches if scaling to new domains
- Document the 7-9% false positive rate in research methods

**What Success Looks Like vs. What We Achieved:**

| Criterion | Target | Achieved | Status |
|-----------|--------|----------|--------|
| F1 Score | > 0.85 | 0.857 | ✅ Marginal Pass |
| Precision | > 0.80 | 0.750 | ❌ Below Target |
| Recall | > 0.90 | 1.000 | ✅ Exceeds Target |
| Overall | All pass | 2/3 pass | ⚠️ Conditional |

---

## Conclusion

```{r conclusion, results='asis'}
cat(sprintf("Model A (Act Detection) achieved **F1 = %.3f** on the test set.\n\n",
            model_a_eval_test$f1_score))

if (overall_pass) {
  cat("✅ This **exceeds the Phase 0 success criterion** (F1 > 0.85), validating the few-shot prompting approach for fiscal act identification.\n\n")
  cat("The model demonstrates:\n\n")
  cat("- Effective discrimination between fiscal acts and general economic commentary\n")
  cat("- Well-calibrated confidence scores\n")
  cat("- Production-ready performance for Phase 1 scaling to Southeast Asia\n\n")
  cat("**Phase 0 Timeline Progress:**\n\n")
  cat("- ✅ Days 1-2: PDF Extraction (complete)\n")
  cat("- ✅ Days 2-3: Training Data Preparation (complete)\n")
  cat("- ✅ Days 3-4: Model A - Act Detection (complete)\n")
  cat("- ⏭️ Days 4-6: Model B - Motivation Classification (NEXT)\n")
} else {
  cat("❌ This **falls short of the Phase 0 success criterion** (F1 > 0.85).\n\n")
  cat("Before proceeding to Model B, invest time in improving Model A through:\n\n")
  cat("1. Enhanced few-shot examples\n")
  cat("2. Refined system prompt\n")
  cat("3. Confidence threshold tuning\n\n")
  cat("Model A is the foundation for the entire pipeline - getting it right is essential.\n")
}
```

---

## Appendix: Sample Predictions

### Correct Act Identifications (True Positives)

```{r sample-tp}
# Sample correct act identifications
# Use TRUE act name (act_name...3) and predicted act name (act_name...10)
tp_filtered <- model_a_predictions_test %>%
  filter(is_fiscal_act == 1, contains_act == TRUE)

tp_samples <- tp_filtered %>%
  slice_sample(n = min(3, nrow(tp_filtered))) %>%
  mutate(
    text = str_trunc(text, width = 200),
    confidence_fmt = sprintf("%.2f", confidence)
  ) %>%
  select(act = act_name...3, confidence = confidence_fmt, reasoning, text)

if (nrow(tp_samples) > 0) {
  tp_samples %>%
    gt() %>%
    tab_header(title = "Sample Correct Act Identifications") %>%
    cols_label(
      act = "Identified Act",
      confidence = "Confidence",
      reasoning = "Model Reasoning",
      text = "Passage (truncated)"
    ) %>%
    tab_options(table.width = pct(100))
}
```

### Correct Non-Act Rejections (True Negatives)

```{r sample-tn}
tn_filtered <- model_a_predictions_test %>%
  filter(is_fiscal_act == 0, contains_act == FALSE)

tn_samples <- tn_filtered %>%
  slice_sample(n = min(3, nrow(tn_filtered))) %>%
  mutate(
    text = str_trunc(text, width = 200),
    confidence_fmt = sprintf("%.2f", confidence)
  ) %>%
  select(confidence = confidence_fmt, reasoning, text)

if (nrow(tn_samples) > 0) {
  tn_samples %>%
    gt() %>%
    tab_header(title = "Sample Correct Non-Act Rejections") %>%
    cols_label(
      confidence = "Confidence",
      reasoning = "Model Reasoning",
      text = "Passage (truncated)"
    ) %>%
    tab_options(table.width = pct(100))
}
```

---

**Report Generated:** `r Sys.time()`

**Targets Pipeline Store:** `r tar_config_get("store")`

**Model:** Claude Sonnet 4 (claude-sonnet-4-20250514)
