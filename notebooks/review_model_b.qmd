---
title: "Model B Evaluation: Motivation Classification"
subtitle: "Performance Assessment Against Phase 0 Success Criteria"
date: today
execute:
  cache: refresh
  warning: false
  message: false
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
---

## Executive Summary

This notebook evaluates **Model B (Motivation Classification)**, a multi-class classifier that categorizes fiscal acts by their primary motivation and determines if they are exogenous to the business cycle.

**Primary Success Criteria:**

- Overall Accuracy > 0.75 on test set
- Per-class F1 Score > 0.70 for each motivation category
- Exogenous flag accuracy > 0.85

**Model Configuration:**

- LLM: Claude Sonnet 4 (claude-sonnet-4-20250514)
- Approach: Few-shot prompting (5 examples per class = 20 total)
- Temperature: 0.0 (deterministic)
- Categories: Spending-driven, Countercyclical, Deficit-driven, Long-run

**Datasets:**

- Training: Used for few-shot example selection
- Validation: [N] acts stratified by motivation category
- Test: [N] acts stratified by motivation category

**Results Summary:**

*To be filled after pipeline execution*

---

```{r setup}
library(targets)
library(tidyverse)
library(gt)
library(here)

here::i_am("notebooks/review_model_b.qmd")
tar_config_set(store = here("_targets"))

# Load evaluation results
model_b_eval_val <- tar_read(model_b_eval_val)
model_b_eval_test <- tar_read(model_b_eval_test)
model_b_predictions_val <- tar_read(model_b_predictions_val)
model_b_predictions_test <- tar_read(model_b_predictions_test)

# Helper function for status badges
status_badge <- function(value, target, higher_better = TRUE) {
  if (higher_better) {
    if (value >= target) {
      sprintf("✅ PASS (%.3f ≥ %.2f)", value, target)
    } else {
      sprintf("❌ FAIL (%.3f < %.2f)", value, target)
    }
  } else {
    if (value <= target) {
      sprintf("✅ PASS (%.3f ≤ %.2f)", value, target)
    } else {
      sprintf("❌ FAIL (%.3f > %.2f)", value, target)
    }
  }
}
```

---

## Performance Metrics

### Validation Set Results

The validation set is used for iterative model improvement before touching the test set.

```{r val-metrics}
# Extract overall metrics
val_overall <- tibble(
  Metric = c("Overall Accuracy", "Macro F1 Score", "Exogenous Accuracy"),
  Value = c(
    model_b_eval_val$accuracy,
    model_b_eval_val$macro_f1,
    model_b_eval_val$exogenous_accuracy
  ),
  Target = c(0.75, 0.70, 0.85),
  Status = c(
    status_badge(model_b_eval_val$accuracy, 0.75),
    status_badge(model_b_eval_val$macro_f1, 0.70),
    status_badge(model_b_eval_val$exogenous_accuracy, 0.85)
  )
)

val_overall %>%
  gt() %>%
  cols_label(
    Metric = "Metric",
    Value = "Value",
    Target = "Target",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Value, Target),
    decimals = 3
  ) %>%
  tab_header(
    title = "Validation Set: Overall Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Validation Set Interpretation:**

The validation set shows **strong performance** that **meets most Phase 0 success criteria**:

- **Overall Accuracy: 90%** ✅ Exceeds the 75% target by +15 percentage points
- **Macro F1: 0.881** ✅ Exceeds the 0.70 target, showing good balance across classes
- **Exogenous Accuracy: 90%** ✅ Exceeds the 85% target by +5 percentage points

The model correctly classified 9 out of 10 acts in the validation set. The single misclassification was a Countercyclical act predicted as Long-run, indicating some difficulty distinguishing between cycle-motivated and efficiency-motivated reforms.

### Per-Class Performance (Validation)

```{r val-per-class}
# Per-class metrics
model_b_eval_val$per_class_metrics %>%
  mutate(
    Status = case_when(
      is.na(f1_score) ~ "N/A (no support)",
      f1_score >= 0.70 ~ sprintf("✅ PASS (%.3f ≥ 0.70)", f1_score),
      TRUE ~ sprintf("❌ FAIL (%.3f < 0.70)", f1_score)
    )
  ) %>%
  gt() %>%
  cols_label(
    class = "Motivation Category",
    precision = "Precision",
    recall = "Recall",
    f1_score = "F1 Score",
    support = "N",
    Status = "Status (F1 > 0.70)"
  ) %>%
  fmt_number(
    columns = c(precision, recall, f1_score),
    decimals = 3
  ) %>%
  tab_header(
    title = "Validation Set: Per-Class Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Per-Class Interpretation:**

Class-level performance on the validation set:

- **Spending-driven (n=3):** Perfect classification (F1=1.0, Precision=1.0, Recall=1.0) ✅
- **Countercyclical (n=2):** F1=0.667 ⚠️ Slightly below 0.70 target
  - Missed 1 out of 2 acts (50% recall)
  - The missed act was classified as Long-run instead
- **Deficit-driven (n=2):** Perfect classification (F1=1.0) ✅
- **Long-run (n=3):** Strong performance (F1=0.857) ✅
  - Precision=0.75 (1 false positive: Countercyclical act misclassified as Long-run)
  - Recall=1.0 (found all Long-run acts)

**Key Finding:** Countercyclical classification is the weak point, with one instance confused with Long-run. This suggests the model has difficulty distinguishing cycle-motivated reforms from efficiency-motivated reforms when the language is ambiguous.

### Confusion Matrix (Validation)

```{r val-confusion}
# Confusion matrix as table
cm_val <- as.data.frame(model_b_eval_val$confusion_matrix)

cm_val %>%
  pivot_wider(names_from = Predicted, values_from = Freq, values_fill = 0) %>%
  gt(rowname_col = "True") %>%
  tab_header(
    title = "Validation Set: Confusion Matrix",
    subtitle = "Rows = True Labels, Columns = Predictions"
  ) %>%
  tab_options(
    table.width = pct(100)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f4f8"),
    locations = cells_body(
      rows = everything(),
      columns = everything()
    )
  )
```

**Common Misclassifications:**

The validation set confusion matrix shows **1 misclassification pattern**:

- **Countercyclical → Long-run (1 instance):** One cycle-motivated reform was classified as a long-run efficiency reform

This error pattern suggests the model may struggle when:
- Acts have mixed motivations (e.g., recession response + structural reform)
- The contemporaneous language emphasizes efficiency gains over cycle stabilization
- The distinction between "improving the economy now" vs. "improving long-run growth" is subtle

**Overall:** With only 1 error out of 10 predictions, the validation set demonstrates strong generalization.

---

## Test Set Results

### Overall Metrics

The test set provides the final, unbiased evaluation of model performance.

```{r test-metrics}
# Extract overall metrics
test_overall <- tibble(
  Metric = c("Overall Accuracy", "Macro F1 Score", "Exogenous Accuracy"),
  Value = c(
    model_b_eval_test$accuracy,
    model_b_eval_test$macro_f1,
    model_b_eval_test$exogenous_accuracy
  ),
  Target = c(0.75, 0.70, 0.85),
  Status = c(
    status_badge(model_b_eval_test$accuracy, 0.75),
    status_badge(model_b_eval_test$macro_f1, 0.70),
    status_badge(model_b_eval_test$exogenous_accuracy, 0.85)
  )
)

test_overall %>%
  gt() %>%
  cols_label(
    Metric = "Metric",
    Value = "Value",
    Target = "Target",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Value, Target),
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Overall Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Test Set Interpretation:**

The test set shows **below-target performance** that **does NOT meet Phase 0 success criteria**:

- **Overall Accuracy: 66.7%** ❌ Below the 75% target by -8.3 percentage points
- **Macro F1: 1.0** ✅ This metric is misleading due to missing categories (see per-class analysis)
- **Exogenous Accuracy: 66.7%** ❌ Below the 85% target by -18.3 percentage points

**Critical Issue:** The model correctly classified only 4 out of 6 acts (66.7%). The **two misclassifications were both Long-run acts predicted as Countercyclical**, creating a systematic error pattern.

**Cascading Error Impact:** Because Long-run acts should be classified as exogenous (TRUE) but Countercyclical is endogenous (FALSE), these motivation errors automatically create exogenous flag errors, dropping exogenous accuracy to 66.7%.

**Important Context:** The test set contains only 6 acts with an imbalanced distribution (3 Spending-driven, 0 Countercyclical, 1 Deficit-driven, 2 Long-run). Small sample size means each error has outsized impact (1 error = 16.7% drop in accuracy).

### Per-Class Performance (Test)

```{r test-per-class}
# Per-class metrics
model_b_eval_test$per_class_metrics %>%
  mutate(
    Status = case_when(
      is.na(f1_score) ~ "N/A (no support or 0 recall)",
      f1_score >= 0.70 ~ sprintf("✅ PASS (%.3f ≥ 0.70)", f1_score),
      TRUE ~ sprintf("❌ FAIL (%.3f < 0.70)", f1_score)
    )
  ) %>%
  gt() %>%
  cols_label(
    class = "Motivation Category",
    precision = "Precision",
    recall = "Recall",
    f1_score = "F1 Score",
    support = "N",
    Status = "Status (F1 > 0.70)"
  ) %>%
  fmt_number(
    columns = c(precision, recall, f1_score),
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Per-Class Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

### Confusion Matrix (Test)

```{r test-confusion}
# Confusion matrix as table
cm_test <- as.data.frame(model_b_eval_test$confusion_matrix)

cm_test %>%
  pivot_wider(names_from = Predicted, values_from = Freq, values_fill = 0) %>%
  gt(rowname_col = "True") %>%
  tab_header(
    title = "Test Set: Confusion Matrix",
    subtitle = "Rows = True Labels, Columns = Predictions"
  ) %>%
  tab_options(
    table.width = pct(100)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f4f8"),
    locations = cells_body(
      rows = everything(),
      columns = everything()
    )
  )
```

**Common Misclassifications:**

The test set confusion matrix reveals a **systematic misclassification pattern**:

- **Long-run → Countercyclical (2 instances):** BOTH Long-run acts in the test set were incorrectly classified as Countercyclical
  - This represents 100% failure rate on Long-run classification (0/2 recall)
  - Both predictions had high confidence (0.85 and 0.90), suggesting the model is confidently wrong

**Root Cause Hypothesis:** The model appears to confuse:
- **Long-run reforms** aimed at improving efficiency and fairness (exogenous)
- **Countercyclical reforms** aimed at stimulating the economy during recessions (endogenous)

When an act discusses "improving economic conditions" or "raising growth," the model may incorrectly interpret this as countercyclical stabilization rather than long-run structural reform.

**Correct Classifications:**
- Spending-driven: 3/3 perfect (100% precision and recall)
- Deficit-driven: 1/1 perfect (100% precision and recall)

---

## Error Analysis

### Misclassified Acts (Test Set)

```{r test-errors}
# Identify misclassified acts
test_errors <- model_b_predictions_test %>%
  filter(motivation != pred_motivation) %>%  # pred_motivation is predicted
  select(
    act_name,
    year,
    true_motivation = motivation,
    predicted_motivation = pred_motivation,
    confidence = pred_confidence,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous
  ) %>%
  arrange(desc(confidence))

if (nrow(test_errors) > 0) {
  test_errors %>%
    gt() %>%
    cols_label(
      act_name = "Act Name",
      year = "Year",
      true_motivation = "True",
      predicted_motivation = "Predicted",
      confidence = "Confidence",
      exogenous_true = "True Exo",
      exogenous_pred = "Pred Exo"
    ) %>%
    fmt_number(
      columns = confidence,
      decimals = 2
    ) %>%
    tab_header(
      title = "Misclassified Acts (Test Set)"
    ) %>%
    tab_options(
      table.width = pct(100)
    )
} else {
  cat("✅ No misclassifications on test set!\n")
}
```

**Error Patterns:**

Analyzing the 2 misclassified acts reveals a clear pattern:

**Both errors follow the same pattern: Long-run → Countercyclical**

1. **Public Law 99-514** (likely Tax Reform Act of 1986)
   - True: Long-run (exogenous=TRUE)
   - Predicted: Countercyclical (exogenous=FALSE)
   - Confidence: 0.85

2. **Revenue Act** (year and context needed)
   - True: Long-run (exogenous=TRUE)
   - Predicted: Countercyclical (exogenous=FALSE)
   - Confidence: 0.90

**Why This Matters:**
- Both acts are major tax reforms aimed at long-term efficiency gains
- The model incorrectly interpreted them as recession-fighting measures
- High confidence scores (0.85-0.90) indicate the model is *systematically* wrong, not uncertain
- This creates cascading errors: motivation error → automatic exogenous flag error

### Confidence Calibration

```{r calibration}
# Confidence calibration for test set
model_b_eval_test$calibration %>%
  filter(!is.na(confidence_bin)) %>%
  gt() %>%
  cols_label(
    confidence_bin = "Confidence Range",
    n = "N Predictions",
    accuracy = "Actual Accuracy"
  ) %>%
  fmt_number(
    columns = accuracy,
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Confidence Calibration",
    subtitle = "Does predicted confidence match actual accuracy?"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Calibration Interpretation:**

Well-calibrated model: predictions with 90% confidence should be 90% accurate.

The test set shows **poor calibration**:
- 5 predictions at 80-90% confidence → 60% actual accuracy (should be ~85%)
- 1 prediction at 90-100% confidence → 100% actual accuracy ✅

**Key Finding:** The model is **overconfident** in its incorrect predictions. The two Long-run misclassifications had 0.85-0.90 confidence, yet were wrong. This indicates the model doesn't recognize when it's uncertain about Long-run vs. Countercyclical distinctions.

**Implication:** We cannot rely on confidence scores to filter questionable predictions—the model is confident even when systematically wrong.

---

## Exogenous Flag Analysis

### Exogenous Flag Performance

```{r exogenous-analysis}
# Exogenous flag confusion
exo_confusion <- model_b_predictions_test %>%
  count(exogenous_true = exogenous, exogenous_pred = pred_exogenous) %>%
  mutate(
    exogenous_true = ifelse(exogenous_true, "Exogenous", "Endogenous"),
    exogenous_pred = ifelse(exogenous_pred, "Exogenous", "Endogenous")
  )

exo_confusion %>%
  pivot_wider(names_from = exogenous_pred, values_from = n, values_fill = 0) %>%
  gt(rowname_col = "exogenous_true") %>%
  tab_header(
    title = "Exogenous Flag Confusion Matrix",
    subtitle = sprintf("Accuracy: %.1f%%", model_b_eval_test$exogenous_accuracy * 100)
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Exogenous Flag Errors:**

```{r exo-errors}
# Acts where exogenous flag was misclassified
exo_errors <- model_b_predictions_test %>%
  filter(exogenous != pred_exogenous) %>%
  select(
    act_name,
    year,
    motivation,
    predicted_motivation = pred_motivation,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous,
    confidence = pred_confidence
  )

if (nrow(exo_errors) > 0) {
  exo_errors %>%
    gt() %>%
    cols_label(
      act_name = "Act Name",
      year = "Year",
      motivation = "True Motivation",
      predicted_motivation = "Predicted Motivation",
      exogenous_true = "True Exo",
      exogenous_pred = "Pred Exo",
      confidence = "Confidence"
    ) %>%
    fmt_number(
      columns = confidence,
      decimals = 2
    ) %>%
    tab_header(
      title = "Acts with Incorrect Exogenous Flag"
    ) %>%
    tab_options(
      table.width = pct(100)
    )
} else {
  cat("✅ No exogenous flag errors on test set!\n")
}
```

---

## Overall Interpretation

### Phase 0 Success Criteria

```{r success-criteria}
# Success criteria checklist
criteria <- tibble(
  Criterion = c(
    "Overall Accuracy > 0.75",
    "Macro F1 > 0.70",
    "All classes F1 > 0.70",
    "Exogenous Accuracy > 0.85"
  ),
  Target = c(0.75, 0.70, 0.70, 0.85),
  Achieved = c(
    model_b_eval_test$accuracy,
    model_b_eval_test$macro_f1,
    min(model_b_eval_test$per_class_metrics$f1_score, na.rm = TRUE),
    model_b_eval_test$exogenous_accuracy
  ),
  Status = c(
    status_badge(model_b_eval_test$accuracy, 0.75),
    status_badge(model_b_eval_test$macro_f1, 0.70),
    status_badge(min(model_b_eval_test$per_class_metrics$f1_score, na.rm = TRUE), 0.70),
    status_badge(model_b_eval_test$exogenous_accuracy, 0.85)
  )
)

criteria %>%
  gt() %>%
  cols_label(
    Criterion = "Success Criterion",
    Target = "Target",
    Achieved = "Achieved",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Target, Achieved),
    decimals = 3
  ) %>%
  tab_header(
    title = "Phase 0 Model B Success Criteria"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Overall Assessment:**

Model B presents a **mixed performance** with strong validation results but **failing test set performance**:

**✅ Validation Set (10 acts):**
- Accuracy: 90% (target: 75%) - Strong pass
- Macro F1: 0.881 (target: 0.70) - Strong pass
- Exogenous Accuracy: 90% (target: 85%) - Pass
- Only 1 misclassification out of 10

**❌ Test Set (6 acts):**
- Accuracy: 66.7% (target: 75%) - **FAIL by -8.3 points**
- Exogenous Accuracy: 66.7% (target: 85%) - **FAIL by -18.3 points**
- 2 misclassifications out of 6 (33% error rate)
- **Both errors follow same pattern:** Long-run → Countercyclical

**Root Cause:** The model systematically confuses Long-run efficiency reforms with Countercyclical stabilization policies. This appears to be a conceptual failure in distinguishing "improving growth" (long-run) from "fighting recession" (countercyclical).

**Small Sample Size Impact:** With only 6 test acts, each error carries heavy weight (16.7% per error). The validation set's larger size (10 acts) and different class distribution may not have exposed this weakness.

**Status:** Model B **does NOT meet Phase 0 success criteria** for production deployment and requires improvement before proceeding to Model C or Southeast Asia deployment.

---

## Detailed Predictions

### Sample Predictions (Test Set)

Show a few representative predictions to verify qualitative performance:

```{r sample-predictions}
# Sample some predictions
set.seed(20251206)
sample_preds <- model_b_predictions_test %>%
  slice_sample(n = min(5, nrow(model_b_predictions_test))) %>%
  select(
    act_name,
    year,
    true_motivation = motivation,
    predicted_motivation = pred_motivation,
    confidence = pred_confidence,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous
  )

sample_preds %>%
  gt() %>%
  cols_label(
    act_name = "Act Name",
    year = "Year",
    true_motivation = "True",
    predicted_motivation = "Predicted",
    confidence = "Confidence",
    exogenous_true = "True Exo",
    exogenous_pred = "Pred Exo"
  ) %>%
  fmt_number(
    columns = confidence,
    decimals = 2
  ) %>%
  tab_header(
    title = "Sample Predictions (Test Set)"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

---

## Recommendations

### Next Steps

Based on the test set failure, **Model B requires improvement before production deployment**. Recommended actions in priority order:

#### Immediate Actions (High Priority)

**1. Enhance System Prompt for Long-run vs. Countercyclical Distinction**

Add explicit clarification to `prompts/model_b_system.txt`:
- **Long-run:** "Improving potential GDP, efficiency, fairness" - would be enacted regardless of current cycle
- **Countercyclical:** "Responding to current recession/boom" - timing depends on cycle position
- Add contrasting examples:
  - ✓ Long-run: "Tax Reform Act of 1986 - simplify code, improve efficiency (enacted during expansion)"
  - ✓ Countercyclical: "Tax Reduction Act of 1975 - stimulate recovery (enacted during recession)"
  - ✗ Common confusion: Acts that mention "growth" aren't automatically countercyclical

**2. Add More Long-run Few-Shot Examples**

Current: 5 examples per class (20 total)
Recommended: Increase Long-run examples to 8-10, focusing on:
- Tax Reform Act of 1986 (efficiency/fairness)
- Revenue Act of 1964 (long-run growth, NOT countercyclical despite growth language)
- Other structural reforms with explicit "long-run" or "efficiency" motivation

**3. Add Negative Examples (Countercyclical → Long-run Contrasts)**

Similar to Model A's edge case strategy, add 3-5 examples showing:
- "This passage mentions growth BUT is Long-run because [efficiency focus]"
- "This passage mentions recession response, therefore Countercyclical"

#### Validation Actions (Medium Priority)

**4. Re-examine Training Data Labels**

Manually review the 2 misclassified acts:
- Public Law 99-514 (likely Tax Reform Act of 1986)
- The Revenue Act in test set

Questions to verify:
- Are the ground truth labels correct? (Could these legitimately have mixed motivations?)
- Do the source passages contain language that could mislead the model?
- Should we add contextual clues (year, economic conditions) to help disambiguation?

**5. Increase Test Set Size** (if possible)

Current test set (6 acts) is too small for reliable evaluation:
- Consider redistributing: 70% train / 15% val / 15% test (larger absolute test size)
- OR: Combine val + test for final evaluation (16 acts total) if we're confident in current approach

#### Alternative Approaches (If Simple Fixes Fail)

**6. Add Temporal Context to Model Input**

Current input: ACT + YEAR + PASSAGES

Enhanced input: ACT + YEAR + **ECONOMIC CONTEXT** + PASSAGES
- "1986: Economy in expansion, unemployment falling"
- "1975: Deep recession, unemployment 9%"

This gives the model explicit cycle context to distinguish:
- Long-run reforms during expansions (exogenous)
- Countercyclical reforms during recessions (endogenous)

**7. Two-Stage Classification**

- **Stage 1:** Exogenous vs. Endogenous (simpler binary)
- **Stage 2:** Within endogenous: Spending-driven vs. Countercyclical
- **Stage 2:** Within exogenous: Deficit-driven vs. Long-run

This may reduce confusion between categories with different exogeneity.

#### What NOT to Do

❌ **Don't proceed to Model C** until Model B passes test criteria
❌ **Don't ignore the test set failure** - validation success alone is insufficient
❌ **Don't just add more training examples** without addressing the conceptual confusion
❌ **Don't deploy to Southeast Asia** with 66.7% test accuracy

---

## Session Info

```{r session-info}
sessionInfo()
```
