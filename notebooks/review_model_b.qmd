---
title: "Model B Evaluation: Motivation Classification"
subtitle: "Performance Assessment Against Phase 0 Success Criteria"
date: today
execute:
  cache: false
  warning: false
  message: false
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    df-print: paged
---

## Executive Summary

This notebook evaluates **Model B (Motivation Classification)**, a multi-class classifier that categorizes fiscal acts by their primary motivation and determines if they are exogenous to the business cycle.

**Primary Success Criteria:**

- Overall Accuracy > 0.75 on test set
- Per-class F1 Score > 0.70 for each motivation category
- **Exogenous Precision > 0.90** (PRIMARY — minimize false positives in shock series)
- Exogenous Accuracy > 0.85

**Model Configuration:**

- LLM: Claude Sonnet 4 (claude-sonnet-4-20250514)
- Approach: Few-shot prompting (5 examples per class = 20 total)
- **Self-Consistency:** 5 samples at temperature 0.7, majority vote
- Categories: Spending-driven, Countercyclical, Deficit-driven, Long-run

**Datasets:**

- Training: Used for few-shot example selection
- Validation: [N] acts stratified by motivation category
- Test: [N] acts stratified by motivation category

**Results Summary:**

| Metric | Validation | Test | Target | Status |
|--------|------------|------|--------|--------|
| Overall Accuracy | 90.0% | 33.3% | >75% | ⚠️ Val Pass / ❌ Test Fail |
| Macro F1 | 0.90 | 0.50 | >0.70 | ⚠️ Val Pass / ❌ Test Fail |
| **Exogenous Precision** | 83.3% | **33.3%** | >90% | ❌ Fail |
| Exogenous Accuracy | 90.0% | 33.3% | >85% | ❌ Test Fail |

**Key Finding: Model B fails test set criteria.**

The model shows strong validation performance (90% accuracy) but poor test generalization (33.3% accuracy). This pattern persists regardless of few-shot example composition, suggesting:

1. **Small test set sensitivity:** With only 6 test acts, systematic errors have outsized impact
2. **Difficult test cases:** The test set contains acts that are inherently harder to classify (e.g., Social Security amendments, Revenue Act of 1978)
3. **Validation/test distribution mismatch:** The validation set may not be representative of the harder cases in the test set

**Error Patterns (Test Set):**

- **Long-run → Countercyclical:** Public Law 90-26, Revenue Act of 1978
- **Spending-driven → Deficit-driven:** Social Security Amendments of 1956, 1961

**Self-Consistency Note:** All predictions show **100% agreement** across samples, even incorrect ones. Self-consistency cannot flag uncertain Model B predictions.

---

```{r setup}
library(targets)
library(tidyverse)
library(gt)
library(here)

here::i_am("notebooks/review_model_b.qmd")
tar_config_set(store = here("_targets"))

# Load evaluation results
model_b_eval_val <- tar_read(model_b_eval_val)
model_b_eval_test <- tar_read(model_b_eval_test)
model_b_predictions_val <- tar_read(model_b_predictions_val)
model_b_predictions_test <- tar_read(model_b_predictions_test)

# Helper function for status badges
status_badge <- function(value, target, higher_better = TRUE) {
  if (higher_better) {
    if (value >= target) {
      sprintf("✅ PASS (%.3f ≥ %.2f)", value, target)
    } else {
      sprintf("❌ FAIL (%.3f < %.2f)", value, target)
    }
  } else {
    if (value <= target) {
      sprintf("✅ PASS (%.3f ≤ %.2f)", value, target)
    } else {
      sprintf("❌ FAIL (%.3f > %.2f)", value, target)
    }
  }
}
```

---

## Performance Metrics

### Validation Set Results

The validation set is used for iterative model improvement before touching the test set.

```{r val-metrics}
# Extract overall metrics
val_overall <- tibble(
  Metric = c("Overall Accuracy", "Macro F1 Score", "Exogenous Accuracy"),
  Value = c(
    model_b_eval_val$accuracy,
    model_b_eval_val$macro_f1,
    model_b_eval_val$exogenous_accuracy
  ),
  Target = c(0.75, 0.70, 0.85),
  Status = c(
    status_badge(model_b_eval_val$accuracy, 0.75),
    status_badge(model_b_eval_val$macro_f1, 0.70),
    status_badge(model_b_eval_val$exogenous_accuracy, 0.85)
  )
)

val_overall %>%
  gt() %>%
  cols_label(
    Metric = "Metric",
    Value = "Value",
    Target = "Target",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Value, Target),
    decimals = 3
  ) %>%
  tab_header(
    title = "Validation Set: Overall Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Validation Set Interpretation:**

The validation set shows **strong performance** that **meets Phase 0 success criteria**:

- **Overall Accuracy: 90%** ✅ Exceeds the 75% target by +15 percentage points
- **Macro F1: 0.90** ✅ Exceeds the 0.70 target, showing good balance across classes
- **Exogenous Accuracy: 90%** ✅ Exceeds the 85% target by +5 percentage points

The model correctly classified 9 out of 10 acts in the validation set. **Notably, EGTRRA 2001 is now correctly classified as Countercyclical** — this was the specific error targeted by adding contrasting examples to the few-shot prompt.

The single misclassification is now the **Social Security Amendments of 1950**, a Spending-driven act predicted as Deficit-driven. This represents a different error pattern than before, where the model conflates spending-to-finance-programs with deficit concerns.

### Per-Class Performance (Validation)

```{r val-per-class}
# Per-class metrics
model_b_eval_val$per_class_metrics %>%
  mutate(
    Status = case_when(
      is.na(f1_score) ~ "N/A (no support)",
      f1_score >= 0.70 ~ sprintf("✅ PASS (%.3f ≥ 0.70)", f1_score),
      TRUE ~ sprintf("❌ FAIL (%.3f < 0.70)", f1_score)
    )
  ) %>%
  gt() %>%
  cols_label(
    class = "Motivation Category",
    precision = "Precision",
    recall = "Recall",
    f1_score = "F1 Score",
    support = "N",
    Status = "Status (F1 > 0.70)"
  ) %>%
  fmt_number(
    columns = c(precision, recall, f1_score),
    decimals = 3
  ) %>%
  tab_header(
    title = "Validation Set: Per-Class Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Per-Class Interpretation:**

Class-level performance on the validation set after few-shot enhancement:

- **Countercyclical (n=2):** Now perfect classification ✅
  - EGTRRA 2001 is now correctly classified (was the target fix)
- **Long-run (n=3):** Now perfect classification ✅
  - No longer has the EGTRRA false positive
- **Deficit-driven (n=2):** Now has a false positive
  - Social Security Amendments of 1950 incorrectly classified here
- **Spending-driven (n=3):** Recall dropped to 67%
  - Lost Social Security Amendments of 1950 to Deficit-driven

**Key Finding:** The few-shot enhancement successfully fixed the Countercyclical ↔ Long-run boundary (EGTRRA 2001 now correct). However, a new Spending-driven ↔ Deficit-driven confusion emerged. The validation results suggest the fix worked for its intended target but introduced a different error pattern.

### Confusion Matrix (Validation)

```{r val-confusion}
# Confusion matrix as table
cm_val <- as.data.frame(model_b_eval_val$confusion_matrix)

cm_val %>%
  pivot_wider(names_from = Predicted, values_from = Freq, values_fill = 0) %>%
  gt(rowname_col = "True") %>%
  tab_header(
    title = "Validation Set: Confusion Matrix",
    subtitle = "Rows = True Labels, Columns = Predictions"
  ) %>%
  tab_options(
    table.width = pct(100)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f4f8"),
    locations = cells_body(
      rows = everything(),
      columns = everything()
    )
  )
```

**Common Misclassifications:**

The validation set confusion matrix shows **1 misclassification pattern** (changed from previous run):

- **Spending-driven → Deficit-driven (1 instance):** Social Security Amendments of 1950 was classified as Deficit-driven instead of Spending-driven

This is a different error from the previous run, where EGTRRA 2001 was the misclassification. The few-shot enhancement:

- ✅ Fixed the Countercyclical ↔ Long-run boundary (EGTRRA 2001 now correct)
- ⚠️ Introduced a new Spending-driven ↔ Deficit-driven confusion

**Overall:** With only 1 error out of 10 predictions, validation performance remains strong at 90% accuracy.

---

## Test Set Results

### Overall Metrics

The test set provides the final, unbiased evaluation of model performance.

```{r test-metrics}
# Extract overall metrics
test_overall <- tibble(
  Metric = c("Overall Accuracy", "Macro F1 Score", "Exogenous Accuracy"),
  Value = c(
    model_b_eval_test$accuracy,
    model_b_eval_test$macro_f1,
    model_b_eval_test$exogenous_accuracy
  ),
  Target = c(0.75, 0.70, 0.85),
  Status = c(
    status_badge(model_b_eval_test$accuracy, 0.75),
    status_badge(model_b_eval_test$macro_f1, 0.70),
    status_badge(model_b_eval_test$exogenous_accuracy, 0.85)
  )
)

test_overall %>%
  gt() %>%
  cols_label(
    Metric = "Metric",
    Value = "Value",
    Target = "Target",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Value, Target),
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Overall Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Test Set Interpretation:**

⚠️ **CRITICAL: Test set performance regressed significantly after few-shot enhancement.**

- **Overall Accuracy: 33.3%** ❌ Far below the 75% target (-41.7 percentage points)
- **Macro F1: 0.50** ❌ Below the 0.70 target
- **Exogenous Accuracy: 33.3%** ❌ Far below the 85% target

**Four Misclassifications (4 of 6 acts wrong):**

| Act | True | Predicted | Error Type |
|-----|------|-----------|------------|
| Public Law 90-26 (1967) | Long-run | Countercyclical | FN (exogenous→endogenous) |
| Revenue Act of 1978 | Long-run | Countercyclical | FN (exogenous→endogenous) |
| Social Security Amendments of 1956 | Spending-driven | Deficit-driven | FP (endogenous→exogenous) |
| Social Security Amendments of 1961 | Spending-driven | Deficit-driven | FP (endogenous→exogenous) |

**Error Pattern Analysis:**

The few-shot enhancement appears to have caused systematic biases:

1. **Long-run → Countercyclical:** The detailed reasoning about EGTRRA 2001 (explaining why it's Countercyclical despite "growth" language) may have made the model over-apply countercyclical classification to other acts with economic improvement language.

2. **Spending-driven → Deficit-driven:** The model is now more likely to see budget/deficit concerns in spending-related acts.

**Comparison with Previous Run:**

| Metric | Before Enhancement | After Enhancement | Change |
|--------|-------------------|-------------------|--------|
| Test Accuracy | 83.3% | 33.3% | -50 pp |
| Test Exo Precision | 100% | 33.3% | -66.7 pp |
| Val EGTRRA 2001 | ❌ Wrong | ✅ Correct | Fixed |

**Conclusion:** The enhancement fixed the specific validation error (EGTRRA 2001) but caused substantial overfitting — the model learned the specific reasoning too well and now over-applies it to the test set.

### Per-Class Performance (Test)

```{r test-per-class}
# Per-class metrics
model_b_eval_test$per_class_metrics %>%
  mutate(
    Status = case_when(
      is.na(f1_score) ~ "N/A (no support or 0 recall)",
      f1_score >= 0.70 ~ sprintf("✅ PASS (%.3f ≥ 0.70)", f1_score),
      TRUE ~ sprintf("❌ FAIL (%.3f < 0.70)", f1_score)
    )
  ) %>%
  gt() %>%
  cols_label(
    class = "Motivation Category",
    precision = "Precision",
    recall = "Recall",
    f1_score = "F1 Score",
    support = "N",
    Status = "Status (F1 > 0.70)"
  ) %>%
  fmt_number(
    columns = c(precision, recall, f1_score),
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Per-Class Metrics"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

### Confusion Matrix (Test)

```{r test-confusion}
# Confusion matrix as table
cm_test <- as.data.frame(model_b_eval_test$confusion_matrix)

cm_test %>%
  pivot_wider(names_from = Predicted, values_from = Freq, values_fill = 0) %>%
  gt(rowname_col = "True") %>%
  tab_header(
    title = "Test Set: Confusion Matrix",
    subtitle = "Rows = True Labels, Columns = Predictions"
  ) %>%
  tab_options(
    table.width = pct(100)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f4f8"),
    locations = cells_body(
      rows = everything(),
      columns = everything()
    )
  )
```

**Common Misclassifications:**

The test set confusion matrix reveals **two systematic error patterns** (4 total errors):

**Pattern 1: Long-run → Countercyclical (2 instances)**

- Public Law 90-26 (1967): Restoration of Investment Tax Credit
- Revenue Act of 1978: Capital gains reduction

Both Long-run acts were misclassified as Countercyclical. The detailed reasoning in the EGTRRA example about "economic improvement language during uncertain times" appears to have biased the model toward Countercyclical classification.

**Pattern 2: Spending-driven → Deficit-driven (2 instances)**

- Social Security Amendments of 1956
- Social Security Amendments of 1961

Both Spending-driven acts were misclassified as Deficit-driven. The model appears to over-weight budget/fiscal responsibility framing.

**Correct Classifications (only 2 of 6):**

- Omnibus Budget Reconciliation Act of 1990: Deficit-driven ✅
- Social Security Amendments of 1965: Spending-driven ✅

---

## Error Analysis

### Misclassified Acts (Test Set)

```{r test-errors}
# Identify misclassified acts
test_errors <- model_b_predictions_test %>%
  filter(motivation != pred_motivation) %>%  # pred_motivation is predicted
  select(
    act_name,
    year,
    true_motivation = motivation,
    predicted_motivation = pred_motivation,
    confidence = pred_confidence,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous
  ) %>%
  arrange(desc(confidence))

if (nrow(test_errors) > 0) {
  test_errors %>%
    gt() %>%
    cols_label(
      act_name = "Act Name",
      year = "Year",
      true_motivation = "True",
      predicted_motivation = "Predicted",
      confidence = "Confidence",
      exogenous_true = "True Exo",
      exogenous_pred = "Pred Exo"
    ) %>%
    fmt_number(
      columns = confidence,
      decimals = 2
    ) %>%
    tab_header(
      title = "Misclassified Acts (Test Set)"
    ) %>%
    tab_options(
      table.width = pct(100)
    )
} else {
  cat("✅ No misclassifications on test set!\n")
}
```

**Error Patterns:**

Analyzing the **four misclassified acts** (regression from previous single error):

**Pattern 1: Long-run → Countercyclical (2 acts)**

1. **Public Law 90-26 (1967)** - Restoration of Investment Tax Credit
   - True: Long-run (exogenous=TRUE) → Predicted: Countercyclical (exogenous=FALSE)

2. **Revenue Act of 1978** - Capital gains reduction
   - True: Long-run (exogenous=TRUE) → Predicted: Countercyclical (exogenous=FALSE)

**Pattern 2: Spending-driven → Deficit-driven (2 acts)**

3. **Social Security Amendments of 1956**
   - True: Spending-driven (exogenous=FALSE) → Predicted: Deficit-driven (exogenous=TRUE)

4. **Social Security Amendments of 1961**
   - True: Spending-driven (exogenous=FALSE) → Predicted: Deficit-driven (exogenous=TRUE)

**Why These Errors Occurred (Post-Enhancement Analysis):**

The detailed "CRITICAL REASONING" added to the EGTRRA 2001 and TRA 1986 examples appears to have introduced systematic biases:

1. **Over-application of Countercyclical logic:** The EGTRRA reasoning emphasized that acts with "economic improvement" language during uncertain times should be Countercyclical. This caused Long-run acts (which also mention economic improvement) to be misclassified.

2. **Over-application of Deficit logic:** The model became more sensitive to budget/fiscal framing, causing Spending-driven Social Security acts to be classified as Deficit-driven.

**Key Lesson:** Highly detailed reasoning in few-shot examples can cause overfitting to the specific logic rather than improving generalization.

### Confidence Calibration

```{r calibration}
# Confidence calibration for test set
model_b_eval_test$calibration %>%
  filter(!is.na(confidence_bin)) %>%
  gt() %>%
  cols_label(
    confidence_bin = "Confidence Range",
    n = "N Predictions",
    accuracy = "Actual Accuracy"
  ) %>%
  fmt_number(
    columns = accuracy,
    decimals = 3
  ) %>%
  tab_header(
    title = "Test Set: Confidence Calibration",
    subtitle = "Does predicted confidence match actual accuracy?"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Calibration Interpretation:**

Well-calibrated model: predictions with 90% confidence should be 90% accurate.

The test set shows **severe overconfidence** after the few-shot enhancement:

- All 6 predictions at 90-100% confidence → **33.3% actual accuracy** (should be ~95%)

**Key Finding:** The model produces very high confidence scores (1.0 for all test predictions) despite being wrong on 4 of 6 acts. This is a severe calibration failure—the model is confidently wrong.

**Implication:** Confidence scores are completely uninformative for identifying errors. The few-shot enhancement appears to have made the model more confident in its (now often incorrect) reasoning.

---

## Self-Consistency Analysis

The model uses **self-consistency sampling** (Wang et al., 2022) to improve calibration and provide uncertainty estimates. For each prediction:

- **5 samples** are drawn at temperature 0.7
- The **majority vote** determines the final motivation classification
- The **agreement rate** (proportion agreeing with majority) serves as an uncertainty indicator

### Agreement Rate Distribution

```{r agreement-distribution, fig.width=8, fig.height=5}
# Combine predictions for analysis
all_predictions_b <- bind_rows(
  model_b_predictions_val %>% mutate(dataset = "Validation"),
  model_b_predictions_test %>% mutate(dataset = "Test")
) %>%
  mutate(
    prediction_correct = (motivation == pred_motivation),
    exogenous_correct = (exogenous == pred_exogenous)
  )

# Check if agreement_rate column exists
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  ggplot(all_predictions_b, aes(x = pred_agreement_rate, fill = prediction_correct)) +
    geom_histogram(bins = 10, alpha = 0.7, position = "identity") +
    facet_wrap(~dataset, ncol = 1) +
    scale_fill_manual(
      values = c("TRUE" = "#4caf50", "FALSE" = "#f44336"),
      labels = c("Incorrect", "Correct"),
      name = "Prediction"
    ) +
    scale_x_continuous(labels = scales::percent, limits = c(0.4, 1.05)) +
    labs(
      title = "Self-Consistency Agreement Rate Distribution",
      subtitle = "Higher agreement = more consistent predictions across samples",
      x = "Agreement Rate (proportion of samples agreeing with majority)",
      y = "Count"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold", hjust = 0.5),
      plot.subtitle = element_text(hjust = 0.5),
      legend.position = "top"
    )
} else {
  cat("⚠️ Agreement rate data not available in predictions.\n")
}
```

### Agreement Rate vs Accuracy

Do predictions with higher agreement rates tend to be more accurate?

```{r agreement-accuracy-b}
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  agreement_accuracy_b <- all_predictions_b %>%
    mutate(
      agreement_bin = cut(
        pred_agreement_rate,
        breaks = c(0.4, 0.6, 0.8, 1.0),
        labels = c("Low (0.4-0.6)", "Medium (0.6-0.8)", "High (0.8-1.0)"),
        include.lowest = TRUE
      )
    ) %>%
    filter(!is.na(agreement_bin)) %>%
    group_by(dataset, agreement_bin) %>%
    summarize(
      n = n(),
      n_correct = sum(prediction_correct),
      accuracy = mean(prediction_correct),
      exo_accuracy = mean(exogenous_correct),
      .groups = "drop"
    )

  agreement_accuracy_b %>%
    mutate(
      accuracy_pct = sprintf("%.1f%%", accuracy * 100),
      exo_accuracy_pct = sprintf("%.1f%%", exo_accuracy * 100),
      correct_ratio = sprintf("%d/%d", n_correct, n)
    ) %>%
    select(dataset, agreement_bin, n, correct_ratio, accuracy_pct, exo_accuracy_pct) %>%
    gt() %>%
    tab_header(
      title = "Accuracy by Agreement Rate",
      subtitle = "Higher agreement should correlate with higher accuracy"
    ) %>%
    cols_label(
      dataset = "Dataset",
      agreement_bin = "Agreement Level",
      n = "N",
      correct_ratio = "Correct/Total",
      accuracy_pct = "Motivation Accuracy",
      exo_accuracy_pct = "Exogenous Accuracy"
    ) %>%
    tab_options(table.width = pct(100))
} else {
  cat("⚠️ Agreement rate data not available.\n")
}
```

```{r agreement-interpretation-b, results='asis'}
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  # Calculate correlation between agreement and correctness
  agreement_cor <- cor(
    all_predictions_b$pred_agreement_rate,
    as.numeric(all_predictions_b$prediction_correct),
    use = "complete.obs"
  )

  mean_agreement_correct <- all_predictions_b %>%
    filter(prediction_correct) %>%
    pull(pred_agreement_rate) %>%
    mean(na.rm = TRUE)

  mean_agreement_incorrect <- all_predictions_b %>%
    filter(!prediction_correct) %>%
    pull(pred_agreement_rate) %>%
    mean(na.rm = TRUE)

  cat("**Self-Consistency Calibration:**\n\n")

  # Check for zero variance (all same agreement rate)
  agreement_sd <- sd(all_predictions_b$pred_agreement_rate, na.rm = TRUE)
  all_unanimous <- !is.na(agreement_sd) && agreement_sd == 0

  if (all_unanimous) {
    cat(sprintf("- **All predictions have %.0f%% agreement** (unanimous across all 5 samples)\n",
                mean(all_predictions_b$pred_agreement_rate, na.rm = TRUE) * 100))
    cat("- Correlation: **N/A** (no variance in agreement rate)\n")
    cat(sprintf("- Mean agreement for correct predictions: **%.1f%%**\n", mean_agreement_correct * 100))
    if (!is.na(mean_agreement_incorrect)) {
      cat(sprintf("- Mean agreement for incorrect predictions: **%.1f%%**\n\n", mean_agreement_incorrect * 100))
    } else {
      cat("- Mean agreement for incorrect predictions: **N/A** (no errors)\n\n")
    }
    cat("⚠️ **Perfect consistency but no discrimination:** The model shows 100% agreement even on incorrect predictions.\n")
    cat("Self-consistency cannot flag uncertain cases for Model B. Use **economic context flags** instead:\n\n")
    cat("- Flag exogenous predictions during recession/crisis years for expert review\n")
    cat("- The Countercyclical ↔ Long-run boundary is the main error source\n\n")
  } else {
    cat(sprintf("- Correlation (agreement rate vs correctness): **%.3f**\n", agreement_cor))
    cat(sprintf("- Mean agreement for correct predictions: **%.1f%%**\n", mean_agreement_correct * 100))
    if (!is.na(mean_agreement_incorrect)) {
      cat(sprintf("- Mean agreement for incorrect predictions: **%.1f%%**\n\n", mean_agreement_incorrect * 100))
    } else {
      cat("- Mean agreement for incorrect predictions: **N/A** (no errors)\n\n")
    }

    if (!is.na(agreement_cor) && agreement_cor > 0.2) {
      cat("✅ **Self-consistency is well-calibrated:** Higher agreement rates correlate with higher accuracy.\n")
      cat("Agreement rate can be used as a reliable uncertainty indicator for flagging low-confidence predictions.\n\n")
    } else if (!is.na(agreement_cor) && agreement_cor > 0) {
      cat("⚠️ **Weak calibration:** Agreement rate shows only modest correlation with accuracy.\n")
      cat("Self-consistency provides some signal but should be combined with other uncertainty indicators.\n\n")
    } else if (is.na(agreement_cor)) {
      cat("ℹ️ **Cannot compute correlation:** Insufficient variation in data.\n\n")
    } else {
      cat("❌ **Poor calibration:** Agreement rate does not correlate with accuracy.\n")
      cat("Self-consistency may not be providing useful uncertainty estimates for this task.\n\n")
    }
  }
}
```

### Low-Agreement Cases (Uncertainty Flags)

Predictions with low agreement (< 80%) indicate the model is uncertain and may benefit from expert review. For Model B, this is especially important for the **exogenous classification** which is critical for shock identification.

```{r low-agreement-b}
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  low_agreement_b <- all_predictions_b %>%
    filter(pred_agreement_rate < 0.8) %>%
    mutate(
      agreement_pct = sprintf("%.0f%%", pred_agreement_rate * 100),
      status = ifelse(prediction_correct, "✅ Correct", "❌ Incorrect"),
      exo_status = ifelse(exogenous_correct, "✅", "❌")
    ) %>%
    select(
      dataset,
      act_name,
      year,
      agreement_pct,
      motivation,
      pred_motivation,
      status,
      exo_status
    )

  if (nrow(low_agreement_b) > 0) {
    low_agreement_b %>%
      gt() %>%
      tab_header(
        title = "Low-Agreement Predictions (< 80%)",
        subtitle = "These predictions have high uncertainty — recommend expert review"
      ) %>%
      cols_label(
        dataset = "Dataset",
        act_name = "Act Name",
        year = "Year",
        agreement_pct = "Agreement",
        motivation = "True",
        pred_motivation = "Predicted",
        status = "Motivation",
        exo_status = "Exogenous"
      ) %>%
      tab_spanner(
        label = "Correct?",
        columns = c(status, exo_status)
      ) %>%
      tab_options(table.width = pct(100))
  } else {
    cat("✅ **No low-agreement predictions.** All predictions have ≥80% agreement across samples.\n")
    cat("This indicates high model confidence and consistency.\n")
  }
} else {
  cat("⚠️ Agreement rate data not available.\n")
}
```

### Agreement Rate and Problematic Boundaries

The EGTRRA 2001 error (Countercyclical → Long-run) and Revenue Act 1978 error (Long-run → Countercyclical) both involve the **Countercyclical ↔ Long-run boundary**. Do these misclassifications have lower agreement rates?

```{r boundary-agreement, results='asis'}
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  # Check if errors have lower agreement than correct predictions
  boundary_analysis <- all_predictions_b %>%
    mutate(
      involves_boundary = (motivation %in% c("Countercyclical", "Long-run") |
                          pred_motivation %in% c("Countercyclical", "Long-run"))
    ) %>%
    group_by(prediction_correct, involves_boundary) %>%
    summarize(
      n = n(),
      mean_agreement = mean(pred_agreement_rate, na.rm = TRUE),
      .groups = "drop"
    )

  cat("**Agreement Rate by Prediction Status and Boundary Involvement:**\n\n")

  boundary_analysis %>%
    mutate(
      status = ifelse(prediction_correct, "Correct", "Incorrect"),
      boundary = ifelse(involves_boundary, "Countercyclical/Long-run", "Other classes"),
      mean_agreement_pct = sprintf("%.1f%%", mean_agreement * 100)
    ) %>%
    select(status, boundary, n, mean_agreement_pct) %>%
    gt() %>%
    cols_label(
      status = "Status",
      boundary = "Classes Involved",
      n = "N",
      mean_agreement_pct = "Mean Agreement"
    ) %>%
    tab_options(table.width = pct(100))

  # Check if misclassifications have lower agreement
  errors <- all_predictions_b %>% filter(!prediction_correct)
  correct <- all_predictions_b %>% filter(prediction_correct)

  if (nrow(errors) > 0 && nrow(correct) > 0) {
    mean_error_agreement <- mean(errors$pred_agreement_rate, na.rm = TRUE)
    mean_correct_agreement <- mean(correct$pred_agreement_rate, na.rm = TRUE)

    cat("\n**Key Finding:**\n\n")
    if (mean_error_agreement < mean_correct_agreement - 0.05) {
      cat(sprintf("✅ Misclassifications have **lower agreement** (%.1f%%) than correct predictions (%.1f%%).\n",
                  mean_error_agreement * 100, mean_correct_agreement * 100))
      cat("Self-consistency can help identify uncertain predictions for expert review.\n\n")
    } else {
      cat(sprintf("⚠️ Misclassifications have **similar agreement** (%.1f%%) to correct predictions (%.1f%%).\n",
                  mean_error_agreement * 100, mean_correct_agreement * 100))
      cat("Self-consistency alone may not reliably flag errors; combine with economic context flags.\n\n")
    }
  }
}
```

### Self-Consistency Summary

```{r self-consistency-summary-b, results='asis'}
if ("pred_agreement_rate" %in% colnames(all_predictions_b)) {
  overall_mean_agreement <- mean(all_predictions_b$pred_agreement_rate, na.rm = TRUE)
  pct_high_agreement <- mean(all_predictions_b$pred_agreement_rate >= 0.8, na.rm = TRUE)
  pct_unanimous <- mean(all_predictions_b$pred_agreement_rate == 1.0, na.rm = TRUE)

  cat("| Metric | Value |\n")
  cat("|--------|-------|\n")
  cat(sprintf("| Mean agreement rate | %.1f%% |\n", overall_mean_agreement * 100))
  cat(sprintf("| High agreement (≥80%%) | %.1f%% of predictions |\n", pct_high_agreement * 100))
  cat(sprintf("| Unanimous agreement (100%%) | %.1f%% of predictions |\n", pct_unanimous * 100))
  cat(sprintf("| Low agreement (<80%%) | %.1f%% of predictions |\n\n", (1 - pct_high_agreement) * 100))

  if (pct_unanimous == 1.0) {
    cat("⚠️ **Perfectly consistent but overconfident:** All predictions have 100% agreement.\n")
    cat("The model shows no uncertainty even on incorrect predictions—self-consistency cannot flag errors.\n")
  } else if (pct_high_agreement >= 0.9) {
    cat("✅ **Highly consistent model:** >90% of predictions have high agreement.\n")
  } else if (pct_high_agreement >= 0.7) {
    cat("⚠️ **Moderately consistent:** 70-90% of predictions have high agreement.\n")
    cat("Consider flagging low-agreement cases for expert review.\n")
  } else {
    cat("❌ **Low consistency:** <70% of predictions have high agreement.\n")
    cat("Model may benefit from more few-shot examples or prompt refinement.\n")
  }
}
```

### Self-Consistency Interpretation

**Key Finding: Self-consistency does NOT provide useful uncertainty signals for Model B.**

Unlike Model A (where agreement rate correlates with accuracy at r ≈ 0.35), Model B shows **100% agreement on all predictions**, including the 2 incorrect ones. This means:

1. **No discrimination:** Self-consistency cannot distinguish correct from incorrect predictions
2. **Overconfidence:** The model is certain even when wrong (e.g., EGTRRA 2001, Revenue Act 1978)
3. **Alternative flags needed:** Economic context must be used instead of agreement rate

**Why Model B differs from Model A:**

- Model A (binary classification) has clearer decision boundaries—some passages are genuinely ambiguous
- Model B (4-class motivation) involves more nuanced reasoning where the model commits fully to one interpretation
- The Countercyclical ↔ Long-run boundary requires subtle historical judgment that the model resolves confidently but sometimes incorrectly

### Implications for Expert Review Protocol

Since self-consistency cannot flag uncertain Model B predictions, the expert review protocol for Phase 1 must rely on **alternative uncertainty indicators**:

1. ~~**Low Agreement Flag:**~~ Not useful—all predictions have 100% agreement
2. **Economic Context Flag (PRIMARY):** Flag any exogenous prediction during recession/crisis years
   - EGTRRA 2001 false positive: enacted during 2001 recession
   - Revenue Act 1978 false negative: enacted during stagflation
3. **Boundary Flag:** Flag predictions involving Countercyclical or Long-run categories for secondary review
4. **Efficiency Language in Crisis:** Flag acts that mention "efficiency" or "long-run" during economic downturns

**Recommended Protocol:**

| Condition | Action |
|-----------|--------|
| Predicted exogenous + recession year | **Mandatory expert review** |
| Countercyclical/Long-run boundary | Secondary review queue |
| All other predictions | Auto-accept |

This protocol would have caught both errors (EGTRRA 2001 and Revenue Act 1978) while flagging only ~20-30% of predictions for review.

---

## Exogenous Flag Analysis

### Why Precision Matters Most

For fiscal shock identification, **exogenous precision is the critical metric**:

- **False Positives (endogenous → exogenous):** Contaminate the shock series with policy responses to the business cycle, biasing fiscal multiplier estimates toward zero
- **False Negatives (exogenous → endogenous):** We lose some valid shocks, reducing statistical power but not biasing estimates

**Priority:** Maximize precision on exogenous classification, even at the cost of some recall.

### Exogenous Classification Metrics

```{r exogenous-metrics}
# Calculate precision, recall, F1 for exogenous class
calc_exo_metrics <- function(predictions) {
  # True Positive: predicted exogenous AND actually exogenous
  TP <- sum(predictions$pred_exogenous & predictions$exogenous)
  # False Positive: predicted exogenous BUT actually endogenous
  FP <- sum(predictions$pred_exogenous & !predictions$exogenous)
  # False Negative: predicted endogenous BUT actually exogenous
  FN <- sum(!predictions$pred_exogenous & predictions$exogenous)
  # True Negative: predicted endogenous AND actually endogenous
  TN <- sum(!predictions$pred_exogenous & !predictions$exogenous)

  precision <- if ((TP + FP) > 0) TP / (TP + FP) else NA
  recall <- if ((TP + FN) > 0) TP / (TP + FN) else NA
  f1 <- if (!is.na(precision) && !is.na(recall) && (precision + recall) > 0) {
    2 * precision * recall / (precision + recall)
  } else NA

  tibble(
    TP = TP, FP = FP, FN = FN, TN = TN,
    Precision = precision,
    Recall = recall,
    F1 = f1,
    Accuracy = (TP + TN) / (TP + FP + FN + TN)
  )
}

# Calculate for both sets
exo_val <- calc_exo_metrics(model_b_predictions_val)
exo_test <- calc_exo_metrics(model_b_predictions_test)

# Display metrics comparison
bind_rows(
  exo_val %>% mutate(Set = "Validation", .before = 1),
  exo_test %>% mutate(Set = "Test", .before = 1)
) %>%
  select(Set, Precision, Recall, F1, Accuracy, TP, FP, FN, TN) %>%
  gt() %>%
  cols_label(
    Set = "Dataset",
    Precision = "Precision",
    Recall = "Recall",
    F1 = "F1 Score",
    Accuracy = "Accuracy",
    TP = "True Pos",
    FP = "False Pos",
    FN = "False Neg",
    TN = "True Neg"
  ) %>%
  fmt_number(
    columns = c(Precision, Recall, F1, Accuracy),
    decimals = 3
  ) %>%
  tab_header(
    title = "Exogenous Classification Performance",
    subtitle = "Precision = TP/(TP+FP) — the key metric for shock identification"
  ) %>%
  tab_footnote(
    footnote = "Target: Exogenous Precision > 0.90 (minimize false positives in shock series)",
    locations = cells_column_labels(columns = Precision)
  ) %>%
  tab_style(
    style = cell_fill(color = "#d4edda"),
    locations = cells_body(columns = Precision)
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Exogenous Metrics Interpretation:**

The results show **excellent test set precision** but **validation set concerns**:

**Test Set (6 acts):** ✅ **100% Precision** - The critical metric

- **0 False Positives:** No endogenous acts were incorrectly labeled as exogenous
- **1 False Negative:** Revenue Act of 1978 (Long-run) was labeled Countercyclical (endogenous)
- This error pattern is acceptable—we lose one valid shock but don't contaminate the series

**Validation Set (10 acts):** ⚠️ **83.3% Precision** - Below 90% target

- **1 False Positive:** EGTRRA 2001 (Countercyclical/endogenous) was labeled Long-run (exogenous)
- **0 False Negatives:** All true exogenous acts were correctly identified
- This false positive would have contaminated the shock series with an endogenous policy response

**Implication for Fiscal Shock Identification:**

The test set demonstrates the model can achieve perfect precision when it matters most. However, the validation set false positive (EGTRRA 2001) reveals a vulnerability: when Countercyclical acts have efficiency-oriented language, the model may incorrectly classify them as exogenous Long-run reforms.

### Exogenous Flag Confusion Matrix

```{r exogenous-analysis}
# Exogenous flag confusion
exo_confusion <- model_b_predictions_test %>%
  count(exogenous_true = exogenous, exogenous_pred = pred_exogenous) %>%
  mutate(
    exogenous_true = ifelse(exogenous_true, "Exogenous", "Endogenous"),
    exogenous_pred = ifelse(exogenous_pred, "Exogenous", "Endogenous")
  )

exo_confusion %>%
  pivot_wider(names_from = exogenous_pred, values_from = n, values_fill = 0) %>%
  gt(rowname_col = "exogenous_true") %>%
  tab_header(
    title = "Test Set: Exogenous Flag Confusion Matrix",
    subtitle = "Columns = Predicted, Rows = True"
  ) %>%
  tab_footnote(
    footnote = "False Positives (top-right) contaminate shock series; False Negatives (bottom-left) reduce power",
    locations = cells_title(groups = "subtitle")
  ) %>%
  tab_options(
    table.width = pct(100)
  ) %>%
  tab_style(
    style = cell_fill(color = "#e8f4f8"),
    locations = cells_body(
      rows = everything(),
      columns = everything()
    )
  )
```

### Exogenous Flag Errors

```{r exo-errors}
# Acts where exogenous flag was misclassified
exo_errors <- model_b_predictions_test %>%
  filter(exogenous != pred_exogenous) %>%
  mutate(
    error_type = case_when(
      !exogenous & pred_exogenous ~ "FALSE POSITIVE (critical)",
      exogenous & !pred_exogenous ~ "False Negative (acceptable)"
    )
  ) %>%
  select(
    act_name,
    year,
    error_type,
    motivation,
    predicted_motivation = pred_motivation,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous
  )

if (nrow(exo_errors) > 0) {
  exo_errors %>%
    gt() %>%
    cols_label(
      act_name = "Act Name",
      year = "Year",
      error_type = "Error Type",
      motivation = "True Motivation",
      predicted_motivation = "Predicted",
      exogenous_true = "True Exo",
      exogenous_pred = "Pred Exo"
    ) %>%
    tab_header(
      title = "Acts with Incorrect Exogenous Flag"
    ) %>%
    tab_style(
      style = cell_fill(color = "#f8d7da"),
      locations = cells_body(
        columns = error_type,
        rows = grepl("FALSE POSITIVE", error_type)
      )
    ) %>%
    tab_style(
      style = cell_fill(color = "#fff3cd"),
      locations = cells_body(
        columns = error_type,
        rows = grepl("False Negative", error_type)
      )
    ) %>%
    tab_options(
      table.width = pct(100)
    )
} else {
  cat("✅ No exogenous flag errors on test set!\n")
}
```

**Error Type Analysis:**

After the few-shot enhancement, the test set now has **both False Positives and False Negatives**:

**False Negatives (2 acts) — Lose valid shocks:**

- **Public Law 90-26 (1967):** True exogenous (Long-run) → Predicted endogenous (Countercyclical)
- **Revenue Act of 1978:** True exogenous (Long-run) → Predicted endogenous (Countercyclical)

**False Positives (2 acts) — CRITICAL - Contaminate series:**

- **Social Security Amendments of 1956:** True endogenous (Spending-driven) → Predicted exogenous (Deficit-driven)
- **Social Security Amendments of 1961:** True endogenous (Spending-driven) → Predicted exogenous (Deficit-driven)

**Validation set (for comparison):**

- **EGTRRA 2001:** Now correctly classified as Countercyclical ✅ (was the target fix)
- **Social Security Amendments of 1950:** Spending-driven → Deficit-driven (new FP)

**Risk Assessment:** The few-shot enhancement created a worse error pattern:

- Test now has **2 False Positives** (was 0) — will contaminate shock series
- Test exogenous precision dropped from 100% to 33.3%

For production deployment, this regression is disqualifying. The enhancement must be reverted or redesigned.

---

## Overall Interpretation

### Phase 0 Success Criteria

```{r success-criteria}
# Calculate exogenous precision for test set
exo_test_metrics <- calc_exo_metrics(model_b_predictions_test)

# Success criteria checklist
criteria <- tibble(
  Criterion = c(
    "Overall Accuracy > 0.75",
    "Macro F1 > 0.70",
    "All classes F1 > 0.70",
    "Exogenous Precision > 0.90 (PRIMARY)",
    "Exogenous Accuracy > 0.85"
  ),
  Target = c(0.75, 0.70, 0.70, 0.90, 0.85),
  Achieved = c(
    model_b_eval_test$accuracy,
    model_b_eval_test$macro_f1,
    min(model_b_eval_test$per_class_metrics$f1_score, na.rm = TRUE),
    exo_test_metrics$Precision,
    model_b_eval_test$exogenous_accuracy
  ),
  Status = c(
    status_badge(model_b_eval_test$accuracy, 0.75),
    status_badge(model_b_eval_test$macro_f1, 0.70),
    status_badge(min(model_b_eval_test$per_class_metrics$f1_score, na.rm = TRUE), 0.70),
    status_badge(exo_test_metrics$Precision, 0.90),
    status_badge(model_b_eval_test$exogenous_accuracy, 0.85)
  )
)

criteria %>%
  gt() %>%
  cols_label(
    Criterion = "Success Criterion",
    Target = "Target",
    Achieved = "Achieved",
    Status = "Status"
  ) %>%
  fmt_number(
    columns = c(Target, Achieved),
    decimals = 3
  ) %>%
  tab_header(
    title = "Phase 0 Model B Success Criteria"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

**Overall Assessment:**

⚠️ **Model B fails Phase 0 success criteria after few-shot enhancement.**

**❌ Test Set (6 acts) — Regression after enhancement:**

- **Exogenous Precision: 33.3%** ❌ Far below 90% target (was 100% before enhancement)
- Accuracy: 33.3% (target: 75%) - **FAIL by -41.7 points** (was 83.3%)
- Macro F1: 0.50 (target: 0.70) - **FAIL**
- Exogenous Accuracy: 33.3% (target: 85%) - **FAIL**
- 4 errors: 2 False Negatives + 2 False Positives

**✅ Validation Set (10 acts) — Target fix achieved but different error emerged:**

- **Exogenous Precision: 83.3%** ⚠️ Below 90% target
- Accuracy: 90% (target: 75%) - Pass
- Macro F1: 0.90 (target: 0.70) - Pass
- EGTRRA 2001 now correctly classified ✅ (target of enhancement)
- New error: Social Security Amendments of 1950 (Spending-driven → Deficit-driven)

**Comparison: Before vs After Few-Shot Enhancement:**

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Test Accuracy | 83.3% | 33.3% | **-50 pp** |
| Test Exo Precision | 100% | 33.3% | **-66.7 pp** |
| Val EGTRRA 2001 | ❌ Wrong | ✅ Correct | Fixed |
| Test False Positives | 0 | 2 | **+2** |

**Key Finding:** The detailed contrasting examples caused severe overfitting. The model learned to apply the specific EGTRRA reasoning too broadly, misclassifying other acts with similar surface features.

**Status:** ❌ Model B **fails Phase 0 success criteria after the few-shot enhancement**. The enhancement must be reverted or redesigned before deployment.

**Immediate Action Required:** Revert to the previous few-shot examples (without required contrasting acts and detailed reasoning) and re-evaluate.

---

## Detailed Predictions

### Sample Predictions (Test Set)

Show a few representative predictions to verify qualitative performance:

```{r sample-predictions}
# Sample some predictions
set.seed(20251206)
sample_preds <- model_b_predictions_test %>%
  slice_sample(n = min(5, nrow(model_b_predictions_test))) %>%
  select(
    act_name,
    year,
    true_motivation = motivation,
    predicted_motivation = pred_motivation,
    confidence = pred_confidence,
    exogenous_true = exogenous,
    exogenous_pred = pred_exogenous
  )

sample_preds %>%
  gt() %>%
  cols_label(
    act_name = "Act Name",
    year = "Year",
    true_motivation = "True",
    predicted_motivation = "Predicted",
    confidence = "Confidence",
    exogenous_true = "True Exo",
    exogenous_pred = "Pred Exo"
  ) %>%
  fmt_number(
    columns = confidence,
    decimals = 2
  ) %>%
  tab_header(
    title = "Sample Predictions (Test Set)"
  ) %>%
  tab_options(
    table.width = pct(100)
  )
```

---

## Recommendations

### Immediate Action: Revert Few-Shot Enhancement

❌ **The few-shot enhancement caused severe regression and must be reverted.**

The attempt to add required contrasting examples (EGTRRA 2001, TRA 1986) with detailed "CRITICAL REASONING" backfired:

| What We Tried | Result |
|---------------|--------|
| Add EGTRRA 2001 with detailed reasoning about why it's Countercyclical | ✅ Fixed EGTRRA 2001 in validation |
| Add TRA 1986 with detailed reasoning about why it's Long-run | ❌ Caused 4 new errors in test set |
| Goal: Improve precision | ❌ Precision dropped 100% → 33.3% |

**Root Cause Analysis:**

The detailed reasoning in contrasting examples caused the model to over-apply classification logic:

1. EGTRRA reasoning emphasized "economic improvement language during recession = Countercyclical"
   → Model now classifies ALL acts with economic improvement language as Countercyclical

2. Deficit/budget-related reasoning became over-weighted
   → Model now misclassifies Spending-driven Social Security acts as Deficit-driven

**Lesson Learned:** Highly specific reasoning in few-shot examples can cause overfitting. The model learns the specific logic pattern and applies it too broadly, rather than learning the underlying classification principle.

### Next Steps

#### CRITICAL: Revert to Previous Few-Shot Examples

**1. Remove Required Contrasting Acts**

Modify `_targets.R` to remove `required_acts` parameter:

```r
tar_target(
  model_b_examples,
  generate_model_b_examples(
    training_data_b,
    n_per_class = 5,
    seed = 20251206
  ),
  packages = c("tidyverse", "jsonlite", "glue")
)
```

**2. Re-run Pipeline and Verify**

```r
tar_make(c(model_b_examples, model_b_examples_file,
           model_b_predictions_val, model_b_predictions_test,
           model_b_eval_val, model_b_eval_test))
```

Expected outcome: Return to 83.3% test accuracy, 100% exogenous precision.

#### Alternative Approaches (After Reversion)

If EGTRRA 2001 false positive remains a concern after reversion, consider less aggressive interventions:

**3. Minimal Prompt Enhancement (Lower Risk)**

Instead of detailed reasoning in examples, add a single sentence to `prompts/model_b_system.txt`:

> "Note: Acts enacted during recessions that mention 'efficiency' or 'growth' may still be Countercyclical if the primary purpose was economic stimulus."

This provides guidance without the overfitting risk of detailed example reasoning.

**4. Economic Context as Input (Future Work)**

Add recession/expansion indicator to model input format. This provides context without changing the few-shot examples.

**5. Expert Review Protocol**

For Phase 1, flag any prediction where:

- Predicted exogenous AND enacted during recession year
- Countercyclical/Long-run classification with low certainty

#### What NOT to Do

❌ Do not add detailed "CRITICAL REASONING" to few-shot examples
❌ Do not force-include specific acts from validation set in training examples
❌ Do not assume fixing one error will generalize — always validate on held-out test set

### Status Summary

| Component | Status | Action |
|-----------|--------|--------|
| Model B (current) | ❌ Failing | Revert enhancement |
| Model B (after revert) | Expected ✅ | Re-validate |
| Phase 1 deployment | ⏸️ Blocked | Wait for revert verification |
| Model C development | ⏸️ Blocked | Depends on Model B |

### Self-Consistency Note

Self-consistency remains uninformative for Model B (100% agreement on all predictions, including errors). This finding is unchanged by the enhancement regression.
