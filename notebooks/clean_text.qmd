---
title: "Cleaning Unstructured Text Data"
execute:
    cache: true
---


```{r}
#| label: setup
#| include: false
#| cache: false

rm(list = ls())
gc()

pacman::p_load(
    tidyverse,
    targets,
    tidytext,
    here,
    quanteda,
    reticulate,
    tictoc,
    pdftools
)

here::i_am("notebooks/clean_text.qmd")

# Configure reticulate to use Anaconda Python 3.12
reticulate::use_python("C:/ProgramData/anaconda3/python.exe", required = TRUE)

Sys.setenv(
    DOCLING_PYTHON = reticulate::py_config()$python,
    DOCLING_SCRIPT = here::here("python", "docling_extract.py")
)

theme_set(theme_minimal())

tar_config_set(
    store = here("_targets")
)



tar_source(
    files = here("R")
)

set.seed(20251208)

```


```{python}
#| label: python-setup
#| include: false
#| cache: false
from docling.document_converter import DocumentConverter
```

```{r}
tic()
```

```{python}

# Change this to a local path or another URL if desired.
# Note: using the default URL requires network access; if offline, provide a
# local file path (e.g., Path("/path/to/file.pdf")).
source = "https://arxiv.org/pdf/2408.09869"

converter = DocumentConverter()
result = converter.convert(source)

```

```{r}
toc()
```

```{python}
#| results: "asis"
# Print Markdown to stdout.
print(result.document.export_to_markdown())
```

This is fucking amazing! I need to reparse all the pdfs I downloaded earlier.
```{r}
tic()
```

```{r}

source <- "https://arxiv.org/pdf/2408.09869"

function_result <- pull_text_docling(source)

```

```{r}

toc()


```

```{r}
#| results: "asis"
function_result$text[[1]] |>
    cat()
```

```{r}
#| include: false
#| eval: false

us_urls <- tar_read(us_urls)

pdf_link <- us_urls |>
    slice_head(n = 1) |>
    pull(pdf_url)

docling_pages <- pull_text_docling(pdf_link)

# `docling_pages$text[[1]]` is a character vector of pages (or length-1 doc text fallback)
# `docling_pages$n_pages` is the number of returned pages


# Download pdf to tempfile
temp_pdf <- tempfile(fileext = ".pdf")
download.file(pdf_link, destfile = temp_pdf, mode = "wb")

# Select 10 random pages from the pdf
num_pages <- pdftools::pdf_info(temp_pdf)$pages

random_pages <- sort(sample(1:num_pages, size = min(10, num_pages)))

# Filter the pdf to only include the selected pages
filtered_pdf <- tempfile(fileext = ".pdf")
pdftools::pdf_subset(temp_pdf, pages = random_pages, output = filtered_pdf)



tic()
```
```{python}
import json
import logging
import time
from pathlib import Path
from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend
from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    PdfPipelineOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption

_log = logging.getLogger(__name__)


def main():
    logging.basicConfig(level=logging.INFO)


    input_doc_path = "https://arxiv.org/pdf/2408.09869"

    ###########################################################################

    # The sections below demo combinations of PdfPipelineOptions and backends.
    # Tip: Uncomment exactly one section at a time to compare outputs.

    # PyPdfium without EasyOCR
    # --------------------
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    pipeline_options.table_structure_options.do_cell_matching = False

    doc_converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(
                pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend
            )
        }
    )

    # PyPdfium with EasyOCR
    # -----------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(
    #             pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend
    #         )
    #     }
    # )

    # Docling Parse without EasyOCR
    # -------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = False
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with EasyOCR (default)
    # -------------------------------
    # Enables OCR and table structure with EasyOCR, using automatic device
    # selection via AcceleratorOptions. Adjust languages as needed.
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options.lang = ["es"]
    # pipeline_options.accelerator_options = AcceleratorOptions(
    #     num_threads=4, device=AcceleratorDevice.AUTO
    # )

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with EasyOCR (CPU only)
    # -------------------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.ocr_options.use_gpu = False  # <-- set this.
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with Tesseract
    # ----------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = TesseractOcrOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with Tesseract CLI
    # --------------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = TesseractCliOcrOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    # Docling Parse with ocrmac (macOS only)
    # --------------------------------------
    # pipeline_options = PdfPipelineOptions()
    # pipeline_options.do_ocr = True
    # pipeline_options.do_table_structure = True
    # pipeline_options.table_structure_options.do_cell_matching = True
    # pipeline_options.ocr_options = OcrMacOptions()

    # doc_converter = DocumentConverter(
    #     format_options={
    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
    #     }
    # )

    ###########################################################################

    start_time = time.time()
    conv_result = doc_converter.convert(input_doc_path)
    end_time = time.time() - start_time

    _log.info(f"Document converted in {end_time:.2f} seconds.")

    ## Export results
    output_dir = Path("scratch")
    output_dir.mkdir(parents=True, exist_ok=True)
    doc_filename = conv_result.input.file.stem

    # Export Docling document JSON format:
    with (output_dir / f"{doc_filename}.json").open("w", encoding="utf-8") as fp:
        fp.write(json.dumps(conv_result.document.export_to_dict()))

    # Export Text format (plain text via Markdown export):
    with (output_dir / f"{doc_filename}.txt").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_markdown(strict_text=True))

    # Export Markdown format:
    with (output_dir / f"{doc_filename}.md").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_markdown())

    # Export Document Tags format:
    with (output_dir / f"{doc_filename}.doctags").open("w", encoding="utf-8") as fp:
        fp.write(conv_result.document.export_to_doctags())


if __name__ == "__main__":
    main()
```


```{r}
toc()
```

At 35.5 seconds per page, tidying my whole US corpus of 100,000 pages would take about 41 days. I need to figure out how to speed this up, maybe using a GPU.
