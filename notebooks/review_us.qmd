---
title: "Review US Documents"
execute: 
    cache: true
---


```{r}
#| label: setup
#| include: false
#| cache: false

pacman::p_load(
    tidyverse,
    targets,
    tidytext,
    here,
    quanteda
)

here::i_am("notebooks/review_us.qmd")

theme_set(theme_minimal())

tar_config_set(
    store = here("_targets")
)

set.seed(20251202)

```



```{r}
#| label: fig-us-body-pages
#| fig-cap: "Number of pages in US documents by year and body."

us_body <- tar_read(us_body) |>
    filter(n_pages > 0)

us_body |>
    ggplot(aes(x = year, y = n_pages, fill = body)) +
    stat_summary(fun = sum, geom = "bar", position = "stack") 



```

Note that the sources of the Annual Report of the Treasury are not available between 1980 and 2010. Otherwise, the body of documents is fairly consistent over time.

At the very least, every year has 500 words in relevant pages (save for 2025, which is to be expected).



```{r}
#| label: fig-page-word-count
#| fig-cap: "Distribution of word counts per page in US documents."
pages <- tar_read(pages)


pages |>
    ggplot(aes(x = tokens)) +
    geom_histogram() +
    geom_vline(
        xintercept = 2000,
        color = "red",
        linetype = "dashed"
    ) +
    scale_x_log10(labels = scales::comma)
```

```{r}
#| label: sample-page
#| results: asis
a <- pages |>
    filter(tokens < 2000) |>
    sample_n(1) |>
    pull(text)

cat(a)

ntoken(tokens(a))

b <- tokens(a)
b |> ntoken()
b |> as.character()

c <- pages |>
    filter(tokens > 2000) |>
    sample_n(1) |>
    pull(text)

cat(c)

```


Feeding pages as a unit to the model may be problematic, as they can cut through ideas.

Documents are the body of pages concatenated together.

```{r}
#| label: fig-document-word-count
#| fig-cap: "Distribution of word counts per document in US documents."


# Concatenate pages back into documents
documents <- tar_read(documents)

documents |>
    ggplot(aes(x = tokens)) +
    geom_histogram() +
    geom_vline(
        xintercept = 2000,
        color = "red",
        linetype = "dashed"
    ) +
    scale_x_log10(labels = scales::comma)


```


Feeding documents to the model may be problematic, as they can be very long; up to one million words.

Paragraphs can be computed using tokenization. See [tidytext::unnest_tokens()].


```{r}
#| label: fig-paragraph-word-count
#| fig-cap: "Distribution of word counts per paragraph in US documents."

paragraphs <- tar_read(paragraphs)

paragraphs |>
    ggplot(aes(x = tokens)) +
    geom_histogram() +
    geom_vline(
        xintercept = 2000,
        color = "red",
        linetype = "dashed"
    ) +
    scale_x_log10(labels = scales::comma)
```

Very few paragraphs have more than 1,000 words. Most are under 100 words. Note, however, that there are many very short paragraphs, which are likely not interesting text to feed to the model.

```{r}
#| label: sample-paragraph
#| results: asis
b <- paragraphs |>
    filter(tokens > 10) |>
    sample_n(1) |>
    pull(paragraph)


cat(b)

```



```{r}
#| label: fig-sentence-word-count
#| fig-cap: "Distribution of word counts per sentence in US documents."
#| include: false
#| eval: false
sentences <- documents |>
    unnest_tokens(
        output = sentence,
        input = text,
        token = "sentences"
    ) |>
    mutate(
        tokens = str_count(sentence, "\\S+")
    )

sentences |>
    ggplot(aes(x = tokens)) +
    geom_histogram() +
    scale_x_log10(labels = scales::comma) +
    geom_vline(
        xintercept = 2000,
        color = "red",
        linetype = "dashed"
    ) +
    scale_y_continuous(labels = scales::comma)

```

```{r}
#| label: sample-sentence
#| results: asis
#| include: false
#| eval: false
c <- sentences |>
    filter(tokens > 5) |>
    sample_n(1) |>
    pull(sentence) |>
    str_squish()

cat(c)

```

At what level of aggregation should we pass the text to the model? Page, document, paragraph?

- Paragraphs can be very short, and need not be self-contained.

- Documents can be very long, and may exceed model limits.

- Pages may cut through ideas, leaving some context out.

- Paragraphs seem like a good balance. They should be mostly self-contained, and are unlikely to exceed model limits. Moreover, it's a granular level where we can remove any irrelevant paragraphs.

## Relevant text in the body

Let's try at the paragraph level, see if we can zoom in to relevant text even more. I'm using the following keywords to identify relevant paragraphs:

```{r}
#| results: asis
tar_read(relevance_keys) |>
    str_flatten_comma(", and ")
```



What percent of the text body is relevant over time?

```{r}
#| label: fig-paragraph-relevant
#| fig-cap: "Relevance in paragraphs over time."
#| fig-subcap: 
#|  - "Percentage of relevant paragraphs by year."
#|  - "Total word count in paragraphs by year, split by relevance."


paragraphs |>
    summarise(
        .by = year,
        n_paragraphs = n(),
        n_relevant = sum(relevant_text),
        pct_relevant = n_relevant / n_paragraphs * 100
    ) |>
    ggplot(aes(x = year, y = pct_relevant)) +
    geom_line() +
    geom_point() +
    labs(
        y = "Percentage of relevant paragraphs"
    )

paragraphs |>
    ggplot(aes(x = year, y = tokens, fill = factor(relevant_text * (1 - is_duplicate) * (number_count < tokens * 0.5)))) +
    stat_summary(fun = sum, geom = "bar", position = "stack") +
    scale_y_continuous(labels = scales::comma) +
    labs(
        y = "Total word count in paragraphs",
        fill = "Relevant"
    )

```

Splitting by, relevance, we can accurately dispose the fluff, rendering a more efficient use of computational resources.

### Duplicate paragraphs

Some strings identified as paragraphs may be the headers or footers that repeat across pages. Let's identify any duplicate paragraphs.

```{r}
#| results: asis
duplicate_paragraphs <- paragraphs |>
    group_by(pdf_url, paragraph) |>
    filter(n() > 1) |>
    ungroup() |>
    distinct() |>
    arrange(pdf_url, paragraph)


duplicate_paragraphs |>
    sample_n(1) |>
    select(paragraph) |>
    map(~cat(.x, "\n\n---\n\n"))
```


Note that some "paragraphs" may actually be tables, filled with numbers. We can identify these by counting the number of numeric characters in the paragraph.

```{r}
#| results: asis
paragraphs |>
    slice_max(number_count, n = 1) |>
    pull(paragraph) |>
    cat()
```

Hence, we need to feed only **relevant paragraphs** to the model, removing duplicates and paragraphs that are mostly numbers.

### Reviewing relevant paragraphs

```{r}
#| results: asis
relevant_paragraphs <- tar_read(relevant_paragraphs)

relevant_paragraphs |>
    sample_n(1) |>
    pull(paragraph) |>
    str_squish() |>
    cat()

```

```{r}
#| eval: false
shocks <- tar_read(us_shocks)

acts <- shocks |> pull(act_name) |> unique()


# Identify paragraphs that mention specific acts
acts_paragraphs <- relevant_paragraphs |>
    # Keep only paragraph that mention the whole word "Act"
    filter(str_detect(paragraph, regex("\\bAct\\b", ignore_case = TRUE))) |>
    cross_join(
        tibble(act_name = acts)
    ) |>
    mutate(
        paragraph_squish = str_squish(paragraph),
        act_name_squish = str_squish(act_name),
        mentions_act = map2_int(
            paragraph_squish,
            act_name_squish,
            ~ as.integer(str_detect(.x, .y))),
            .progress = TRUE

        )
    
acts_paragraphs |>
    sample_n(20) |>
    View()

```