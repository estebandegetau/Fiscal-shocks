# Chunk Tier Identification for C1 Evaluation
#
# Assigns chunks to tiers for chunk-based C1 evaluation:
#   Tier 1: Contains verbatim us_labels passage text (gold standard)
#   Tier 2: Mentions known act name via substring, subcomponent decomposition,
#           or co-occurrence matching (pipeline positive)
#   Negative: All non-positive chunks, tagged with continuous key_density

# --- Module-level constants ---------------------------------------------------

# Co-occurrence rules for acts where single-term matching is insufficient.
# Each entry maps an act name to a list of term-pair vectors. A chunk matches
# if it contains ALL terms in any one rule pair.
COOCCURRENCE_RULES <- list(
  "Expiration of Excess Profits Tax and of Temporary Income Tax Increases" = list(
    c("expiration", "excess profits")
  )
)

# Terms too broad for Tier 2 matching. Generated by generate_subcomponents()
# but excluded before matching to avoid false positives.
BROAD_EXCLUSIONS <- c(
  # Topic-level phrases (match policy areas, not specific acts)
  "investment tax credit", "balanced budget",
  "taxpayer relief", "excess profits tax",
  "depreciation guidelines", "temporary income tax",
  # Fragments from splitting on "and" (too generic)
  "revenue", "economic growth", "jobs",
  "tax reduction", "job creation", "tax equity"
)

#' Generate searchable subcomponents from a (possibly compound) act name
#'
#' Decomposes act names into searchable terms using five strategies:
#' split on "and", parenthetical extraction, embedded formal names,
#' Public Law numbers, and action+subject phrases. Excludes overly
#' broad terms listed in BROAD_EXCLUSIONS.
#'
#' @param act_name Character string of the act name
#' @return Tibble with columns `term` and `mechanism`
#' @export
generate_subcomponents <- function(act_name) {
  rows <- list()

  # Full name always included
  rows[[length(rows) + 1]] <- tibble::tibble(
    term = act_name, mechanism = "Full name"
  )

  # Split on " and "
  if (stringr::str_detect(act_name, " and ")) {
    parts <- stringr::str_trim(stringr::str_split(act_name, " and ")[[1]])
    for (p in parts) {
      rows[[length(rows) + 1]] <- tibble::tibble(
        term = p, mechanism = "Split on 'and'"
      )
    }
  }

  # Parenthetical content + "of the" variant + before-parenthetical
  if (stringr::str_detect(act_name, "\\(")) {
    paren <- stringr::str_extract(act_name, "(?<=\\()([^)]+)(?=\\))")
    if (!is.na(paren)) {
      rows[[length(rows) + 1]] <- tibble::tibble(
        term = paren, mechanism = "Parenthetical"
      )
      if (stringr::str_detect(paren, " of ") &&
            !stringr::str_detect(paren, " of the ")) {
        rows[[length(rows) + 1]] <- tibble::tibble(
          term = stringr::str_replace(paren, " of ", " of the "),
          mechanism = "Parenthetical + article"
        )
      }
    }
    before_paren <- stringr::str_trim(
      stringr::str_remove(act_name, "\\s*\\([^)]+\\)")
    )
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = before_paren, mechanism = "Before parenthetical"
    )
  }

  # Embedded formal act names: "Xyz Act of YYYY"
  formal <- stringr::str_extract_all(
    act_name,
    "[A-Z][\\w-]+(?:\\s+[A-Z][\\w-]+)*\\s+Act\\s+of\\s+\\d{4}"
  )[[1]]
  for (f in formal) {
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = f, mechanism = "Embedded formal name"
    )
  }

  # Public Law numbers
  pl <- stringr::str_extract(act_name, "Public Law \\d+-\\d+")
  if (!is.na(pl)) {
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = pl, mechanism = "Public Law number"
    )
  }

  # Action + subject phrases for event-type names
  event_match <- stringr::str_match(
    act_name,
    "^(Expiration|Suspension|Restoration|Changes)\\s+(?:in\\s+)?(.*?)(?:\\s+and\\s+|$)"
  )
  if (!is.na(event_match[1, 1]) && !is.na(event_match[1, 2]) &&
        !is.na(event_match[1, 3])) {
    action <- tolower(event_match[1, 2])
    subject <- tolower(stringr::str_trim(event_match[1, 3]))
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = paste(action, "of the", subject),
      mechanism = "Action + subject"
    )
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = paste(action, "of", subject),
      mechanism = "Action + subject"
    )
  }

  result <- dplyr::bind_rows(rows)

  # Exclude overly broad terms
  result <- result |>
    dplyr::filter(!tolower(term) %in% BROAD_EXCLUSIONS)

  # Deduplicate by term (keep first mechanism label)
  dplyr::distinct(result, term, .keep_all = TRUE)
}


#' Identify Tier 1 chunks (contain verbatim labeled passages)
#'
#' For each passage in aligned_data, extracts a distinctive substring and
#' searches chunks for an exact match. Narrows search by matching document
#' source and year when possible.
#'
#' @param aligned_data Tibble from align_labels_shocks() with passages_text
#' @param chunks Tibble from make_chunks() with doc_id, year, text
#' @return Tibble with chunk_id, doc_id, year, act_name, tier, passage_idx
#' @export
identify_tier1_chunks <- function(aligned_data, chunks) {
  results <- list()

  for (i in seq_len(nrow(aligned_data))) {
    act <- aligned_data[i, ]
    passages <- stringr::str_split(act$passages_text, "\n\n")[[1]]
    passages <- stringr::str_trim(passages)
    passages <- passages[nchar(passages) > 50]

    act_year <- act$year

    # Narrow chunk search to same year +/- 1 (passages may appear in adjacent years)
    candidate_chunks <- chunks |>
      dplyr::filter(
        is.na(year) | (year >= act_year - 1 & year <= act_year + 1)
      )

    for (j in seq_along(passages)) {
      # Extract first 100 chars as search substring
      passage <- passages[j]
      search_str <- substr(stringr::str_squish(passage), 1, 100)

      # Try exact substring match
      matches <- candidate_chunks |>
        dplyr::filter(
          stringr::str_detect(text, stringr::fixed(search_str))
        )

      if (nrow(matches) == 0) {
        # Fall back to shorter substring (first 60 chars)
        search_str <- substr(stringr::str_squish(passage), 1, 60)
        matches <- candidate_chunks |>
          dplyr::filter(
            stringr::str_detect(text, stringr::fixed(search_str))
          )
      }

      if (nrow(matches) > 0) {
        # Take the first match (if multiple chunks contain the passage due to
        # overlap, any one is a valid Tier 1 representative)
        results[[length(results) + 1]] <- tibble::tibble(
          chunk_id = matches$chunk_id[1],
          doc_id = matches$doc_id[1],
          year = matches$year[1],
          act_name = act$act_name,
          tier = 1L,
          passage_idx = j
        )
      }
    }
  }

  tier1 <- if (length(results) > 0) {
    dplyr::bind_rows(results) |>
      dplyr::distinct(chunk_id, doc_id, act_name, .keep_all = TRUE)
  } else {
    tibble::tibble(
      chunk_id = integer(), doc_id = character(), year = integer(),
      act_name = character(), tier = integer(), passage_idx = integer()
    )
  }

  if (nrow(tier1) == 0) {
    warning("No Tier 1 chunks matched. Check passage text quality and year coverage.")
  }

  n_acts_matched <- dplyr::n_distinct(tier1$act_name)
  n_acts_total <- nrow(aligned_data)
  total_passages <- sum(purrr::map_int(
    stringr::str_split(aligned_data$passages_text, "\n\n"),
    ~ sum(nchar(stringr::str_trim(.x)) > 50)
  ))

  n_passages_found <- length(results)

  message(sprintf(
    "Tier 1: %d chunks matched across %d/%d acts (%d/%d passages found)",
    nrow(tier1), n_acts_matched, n_acts_total,
    n_passages_found, total_passages
  ))

  tier1
}


#' Identify Tier 2 chunks (mention act names, not already Tier 1)
#'
#' Searches all non-Tier-1 chunks for act name mentions using three strategies:
#' (1) whitespace-normalized substring matching, (2) subcomponent decomposition
#' for compound names, and (3) co-occurrence matching for event-type names.
#' No year window or keyword filters are applied; C1 maximizes recall and
#' downstream codebooks (C2-C4) handle precision filtering.
#'
#' @param aligned_data Tibble from align_labels_shocks()
#' @param chunks Tibble from make_chunks()
#' @param tier1_chunks Tibble of Tier 1 chunks (must have doc_id, chunk_id)
#' @return Tibble with chunk_id, doc_id, year, act_name, tier
#' @export
identify_tier2_chunks <- function(aligned_data, chunks, tier1_chunks) {
  act_names <- unique(aligned_data$act_name)

  # Pre-compute whitespace-normalized text once (handles OCR line breaks)
  chunks_squished <- stringr::str_squish(tolower(chunks$text))

  # Build logical Tier 1 mask via ID matching
  tier1_ids <- paste(tier1_chunks$doc_id, tier1_chunks$chunk_id, sep = "||")
  chunk_ids <- paste(chunks$doc_id, chunks$chunk_id, sep = "||")
  tier1_mask <- chunk_ids %in% tier1_ids

  results <- list()

  for (k in seq_along(act_names)) {
    if (act_names[k] %in% names(COOCCURRENCE_RULES)) {
      # Co-occurrence path: match chunks containing ALL terms in any rule pair
      matched <- rep(FALSE, nrow(chunks))
      for (rule in COOCCURRENCE_RULES[[act_names[k]]]) {
        co_match <- Reduce(`&`, lapply(rule, function(term) {
          stringr::str_detect(chunks_squished, stringr::fixed(term))
        }))
        matched <- matched | co_match
      }
    } else {
      # Subcomponent path: match chunks containing ANY subcomponent term
      subs <- generate_subcomponents(act_names[k])
      subs_squished <- stringr::str_squish(tolower(subs$term))
      matched <- rep(FALSE, nrow(chunks))
      for (s in subs_squished) {
        matched <- matched |
          stringr::str_detect(chunks_squished, stringr::fixed(s))
      }
    }

    # Exclude Tier 1 chunks only (no year or keyword filters)
    matched <- matched & !tier1_mask

    if (any(matched)) {
      match_idx <- which(matched)
      results[[length(results) + 1]] <- tibble::tibble(
        chunk_id = chunks$chunk_id[match_idx],
        doc_id = chunks$doc_id[match_idx],
        year = chunks$year[match_idx],
        act_name = act_names[k],
        tier = 2L
      )
    }
  }

  tier2 <- dplyr::bind_rows(results) |>
    dplyr::distinct(chunk_id, doc_id, act_name, .keep_all = TRUE)

  message(sprintf(
    "Tier 2: %d chunks matched across %d acts",
    nrow(tier2), dplyr::n_distinct(tier2$act_name)
  ))

  tier2
}


#' Identify negative chunks (all non-positive chunks, tagged with key_density)
#'
#' All chunks not in Tier 1 or Tier 2 become negatives. Each negative is tagged
#' with a continuous `key_density` score (n_relevance_keys / n_words) measuring
#' fiscal vocabulary concentration. This replaces the binary relevance-key
#' exclusion with a gradient for S3 error analysis.
#'
#' @param chunks Tibble from make_chunks()
#' @param tier1_chunks Tibble of Tier 1 chunks (must have doc_id, chunk_id)
#' @param tier2_chunks Tibble of Tier 2 chunks (must have doc_id, chunk_id)
#' @param relevance_keys Character vector of relevance keywords
#' @return Tibble with chunk_id, doc_id, year, source_type, text, approx_tokens,
#'   key_density, n_relevance_keys, n_words
#' @export
identify_negative_chunks <- function(chunks, tier1_chunks, tier2_chunks,
                                     relevance_keys) {
  # Build relevance key pattern (word-boundary matching)
  relevance_pattern <- paste0(
    "\\b(", paste(relevance_keys, collapse = "|"), ")\\b"
  )

  # Exclude all Tier 1 and Tier 2 chunks using anti_join
  all_positives <- dplyr::bind_rows(
    tier1_chunks |> dplyr::select(doc_id, chunk_id),
    tier2_chunks |> dplyr::select(doc_id, chunk_id)
  ) |>
    dplyr::distinct()

  negatives <- chunks |>
    dplyr::anti_join(all_positives, by = c("doc_id", "chunk_id"))

  # Compute key_density: n_relevance_keys / n_words
  text_lower <- tolower(negatives$text)
  negatives <- negatives |>
    dplyr::mutate(
      n_relevance_keys = stringr::str_count(
        text_lower,
        stringr::regex(relevance_pattern)
      ),
      n_words = stringr::str_count(text_lower, "\\S+"),
      key_density = dplyr::if_else(
        n_words > 0, n_relevance_keys / n_words, 0
      )
    )

  # Derive source_type from doc_id pattern
  negatives <- negatives |>
    dplyr::mutate(
      source_type = dplyr::case_when(
        stringr::str_detect(doc_id, stringr::regex("erp|economic.report",
                                                    ignore_case = TRUE)) ~ "ERP",
        stringr::str_detect(doc_id, stringr::regex("treasury|annual.report",
                                                    ignore_case = TRUE)) ~ "Treasury",
        stringr::str_detect(doc_id, stringr::regex("budget",
                                                    ignore_case = TRUE)) ~ "Budget",
        TRUE ~ "Other"
      )
    )

  message(sprintf(
    "Negative chunks: %d (ERP: %d, Treasury: %d, Budget: %d, Other: %d)",
    nrow(negatives),
    sum(negatives$source_type == "ERP"),
    sum(negatives$source_type == "Treasury"),
    sum(negatives$source_type == "Budget"),
    sum(negatives$source_type == "Other")
  ))
  message(sprintf(
    "  Key density: median=%.3f, mean=%.3f, max=%.3f",
    stats::median(negatives$key_density),
    mean(negatives$key_density),
    max(negatives$key_density)
  ))

  negatives |>
    dplyr::select(
      chunk_id, doc_id, year, source_type, text, approx_tokens,
      key_density, n_relevance_keys, n_words
    )
}


#' Prepare C1 chunk-based evaluation data
#'
#' Orchestrator that calls tier identification functions and returns
#' structured data for C1 LOOCV and behavioral testing.
#'
#' @param aligned_data Tibble from align_labels_shocks()
#' @param chunks Tibble from make_chunks()
#' @param relevance_keys Character vector of relevance keywords
#' @return List with tier1, tier2, negatives, summary, and
#'   negative_density_summary components
#' @export
prepare_c1_chunk_data <- function(aligned_data, chunks, relevance_keys,
                                  max_doc_year = 2007L) {
  message("=== Preparing C1 chunk-based evaluation data ===")

  # Filter corpus to documents R&R had access to (published 2010,
  # last act 2003, documents through ~2007). Post-2007 docs only
  # add retrospective mentions that inflate recall beyond what the
  # actual R&R task requires.
  if (!is.null(max_doc_year)) {
    n_before <- nrow(chunks)
    chunks <- chunks |> dplyr::filter(is.na(year) | year <= max_doc_year)
    message(sprintf(
      "  Filtered chunks to doc_year <= %d: %d -> %d chunks",
      max_doc_year, n_before, nrow(chunks)
    ))
  }

  # Step 1: Tier 1 — verbatim passage matches
  tier1 <- identify_tier1_chunks(aligned_data, chunks)

  # Step 2: Tier 2 — act name mentions (excluding Tier 1)
  tier2 <- identify_tier2_chunks(aligned_data, chunks, tier1)

  # Step 3: Negatives — all non-positive chunks, tagged with key_density
  negatives <- identify_negative_chunks(
    chunks, tier1, tier2, relevance_keys
  )

  # Coverage validation: warn if any act has zero Tier 1 AND zero Tier 2 matches
  acts_in_tier1 <- unique(tier1$act_name)
  acts_in_tier2 <- unique(tier2$act_name)
  all_covered <- unique(c(acts_in_tier1, acts_in_tier2))
  uncovered <- setdiff(aligned_data$act_name, all_covered)
  if (length(uncovered) > 0) {
    warning(sprintf(
      "%d act(s) have zero Tier 1 AND zero Tier 2 matches: %s",
      length(uncovered),
      paste(uncovered, collapse = "; ")
    ))
  }

  # Summary statistics
  n_total_chunks <- nrow(chunks)
  n_tier1 <- nrow(tier1)
  n_tier2 <- nrow(tier2)
  n_negative <- nrow(negatives)

  summary_tbl <- tibble::tibble(
    category = c("Tier 1", "Tier 2", "Negative", "Total"),
    n_chunks = c(n_tier1, n_tier2, n_negative, n_total_chunks),
    pct = round(n_chunks / n_total_chunks * 100, 1)
  )

  message("\n=== Chunk Tier Summary ===")
  message(sprintf("  Tier 1 (verbatim passage): %d chunks", n_tier1))
  message(sprintf("  Tier 2 (act name mention): %d chunks", n_tier2))
  message(sprintf("  Negative (all remaining):  %d chunks", n_negative))
  message(sprintf("  Total:                     %d chunks", n_total_chunks))

  # Negative key_density distribution (binned summary for diagnostics)
  density_breaks <- c(0, 0.001, 0.01, 0.03, 0.05, Inf)
  density_labels <- c("0", "(0, 1%]", "(1%, 3%]", "(3%, 5%]", ">5%")
  negative_density_summary <- negatives |>
    dplyr::mutate(
      density_bin = cut(
        key_density,
        breaks = density_breaks,
        labels = density_labels,
        include.lowest = TRUE,
        right = TRUE
      )
    ) |>
    dplyr::count(density_bin, .drop = FALSE) |>
    dplyr::mutate(pct = round(n / sum(n) * 100, 1))

  # Join chunk text to tier1 and tier2 for evaluation
  tier1_with_text <- tier1 |>
    dplyr::left_join(
      chunks |> dplyr::select(doc_id, chunk_id, text, approx_tokens),
      by = c("doc_id", "chunk_id")
    )

  tier2_with_text <- tier2 |>
    dplyr::left_join(
      chunks |> dplyr::select(doc_id, chunk_id, text, approx_tokens),
      by = c("doc_id", "chunk_id")
    )

  list(
    tier1 = tier1_with_text,
    tier2 = tier2_with_text,
    negatives = negatives,
    summary = summary_tbl,
    negative_density_summary = negative_density_summary
  )
}


#' Pre-compute diagnostics for verify_chunk_tiers.qmd
#'
#' Runs the heavy string-matching loops (mechanism analysis, timing analysis,
#' contamination check) inside the pipeline so the notebook only loads
#' lightweight, pre-aggregated tibbles. This avoids loading the full `chunks`
#' object (~400-650 MB RAM) during notebook rendering.
#'
#' @param aligned_data Tibble from align_labels_shocks()
#' @param chunks Tibble from make_chunks() with doc_id, year, text
#' @return List with mechanism_tbl, timing_df, corpus_year_counts,
#'   contamination_rate, contamination_examples
#' @export
prepare_chunk_tier_diagnostics <- function(aligned_data, chunks,
                                           c1_chunk_data,
                                           max_doc_year = 2007L) {
  message("=== Pre-computing chunk tier diagnostics ===")

  # Apply same year filter as prepare_c1_chunk_data
  if (!is.null(max_doc_year)) {
    chunks <- chunks |>
      dplyr::filter(is.na(year) | year <= max_doc_year)
    message(sprintf("  Filtered to doc_year <= %d: %d chunks",
                    max_doc_year, nrow(chunks)))
  }

  # Extract lightweight vectors from chunks, then free the heavy tibble.
  # This avoids holding chunks (~400-650 MB) AND chunks_squished (~400-650 MB)
  # simultaneously, which OOMs in the 8 GB container.
  chunk_years <- chunks$year
  chunk_doc_ids <- chunks$doc_id
  chunk_chunk_ids <- chunks$chunk_id
  corpus_year_counts <- dplyr::count(chunks, year)
  chunks_squished <- stringr::str_squish(tolower(chunks$text))
  rm(chunks)
  gc()
  message("  Freed chunks tibble, retained squished text + metadata vectors")

  # --- 1. Mechanism analysis (per-act, per-search-term match counts) ---
  mechanism_rows <- list()

  for (i in seq_len(nrow(aligned_data))) {
    act <- aligned_data$act_name[i]
    yr <- aligned_data$year[i]

    if (act %in% names(COOCCURRENCE_RULES)) {
      for (rule in COOCCURRENCE_RULES[[act]]) {
        co_match <- Reduce(`&`, lapply(rule, function(term) {
          stringr::str_detect(chunks_squished, stringr::fixed(term))
        }))
        mechanism_rows[[length(mechanism_rows) + 1]] <- tibble::tibble(
          act_name = act, year = yr,
          mechanism = "Co-occurrence",
          search_term = paste(rule, collapse = " + "),
          n_matches = sum(co_match)
        )
      }
    } else {
      subs <- generate_subcomponents(act)
      for (j in seq_len(nrow(subs))) {
        sub_sq <- stringr::str_squish(tolower(subs$term[j]))
        n <- sum(stringr::str_detect(chunks_squished, stringr::fixed(sub_sq)))
        mechanism_rows[[length(mechanism_rows) + 1]] <- tibble::tibble(
          act_name = act, year = yr,
          mechanism = subs$mechanism[j],
          search_term = subs$term[j],
          n_matches = n
        )
      }
    }
  }

  mechanism_tbl <- dplyr::bind_rows(mechanism_rows)
  message(sprintf("  Mechanism analysis: %d rows for %d acts",
                  nrow(mechanism_tbl), dplyr::n_distinct(mechanism_tbl$act_name)))

  # --- 2. Timing analysis (per-act chunk-year distributions) ---
  timing_data <- list()

  for (i in seq_len(nrow(aligned_data))) {
    act <- aligned_data$act_name[i]

    if (act %in% names(COOCCURRENCE_RULES)) {
      matched <- logical(length(chunks_squished))
      for (rule in COOCCURRENCE_RULES[[act]]) {
        co_match <- Reduce(`&`, lapply(rule, function(term) {
          stringr::str_detect(chunks_squished, stringr::fixed(term))
        }))
        matched <- matched | co_match
      }
    } else {
      subs <- generate_subcomponents(act)
      subs_squished <- stringr::str_squish(tolower(subs$term))
      matched <- logical(length(chunks_squished))
      for (s in subs_squished) {
        matched <- matched |
          stringr::str_detect(chunks_squished, stringr::fixed(s))
      }
    }

    if (any(matched)) {
      timing_data[[length(timing_data) + 1]] <- tibble::tibble(
        act_name = act,
        chunk_year = chunk_years[matched]
      )
    }
  }

  timing_df <- dplyr::bind_rows(timing_data) |>
    dplyr::left_join(
      aligned_data |> dplyr::select(act_name, signed_year = year),
      by = "act_name"
    ) |>
    dplyr::mutate(
      act_name_year = as.integer(stringr::str_extract(act_name, "\\d{4}")),
      has_name_year = !is.na(act_name_year),
      year_diff = chunk_year - act_name_year,
      label = dplyr::if_else(
        nchar(act_name) > 40,
        paste0(stringr::str_trunc(act_name, 37), " (", signed_year, ")"),
        paste0(act_name, " (", signed_year, ")")
      ),
      label = forcats::fct_reorder(label, signed_year)
    )
  message(sprintf("  Timing analysis: %d rows", nrow(timing_df)))
  message(sprintf("  Corpus year counts: %d years", nrow(corpus_year_counts)))

  # --- 3. Contamination check (act names in negative pool) ---
  # Use chunks_squished (already lowercased) for detection
  act_names_pattern <- paste0(
    "(",
    paste(
      stringr::str_replace_all(
        tolower(aligned_data$act_name),
        "([.()\\[\\]])", "\\\\\\1"
      ),
      collapse = "|"
    ),
    ")"
  )

  has_act_name <- stringr::str_detect(
    chunks_squished,
    stringr::regex(act_names_pattern)
  )

  # Derive negative mask from c1_chunk_data (avoids re-running tier ID)
  all_positive_ids <- paste(
    c(c1_chunk_data$tier1$doc_id, c1_chunk_data$tier2$doc_id),
    c(c1_chunk_data$tier1$chunk_id, c1_chunk_data$tier2$chunk_id),
    sep = "||"
  )
  chunk_ids <- paste(chunk_doc_ids, chunk_chunk_ids, sep = "||")
  is_negative <- !chunk_ids %in% all_positive_ids

  neg_has_act_name <- has_act_name & is_negative
  n_negative <- sum(is_negative)
  n_contaminated <- sum(neg_has_act_name)
  contamination_rate <- if (n_negative > 0) n_contaminated / n_negative else 0

  # Sample contamination examples (up to 5, reproducible)
  contaminated_idx <- which(neg_has_act_name)
  sample_idx <- if (length(contaminated_idx) > 5) {
    set.seed(42)
    sample(contaminated_idx, 5)
  } else {
    contaminated_idx
  }
  contamination_examples <- if (length(sample_idx) > 0) {
    tibble::tibble(
      chunk_id = chunk_chunk_ids[sample_idx],
      doc_id = chunk_doc_ids[sample_idx],
      year = chunk_years[sample_idx],
      text_excerpt = stringr::str_trunc(chunks_squished[sample_idx], 150)
    )
  } else {
    tibble::tibble(
      chunk_id = integer(), doc_id = character(),
      year = integer(), text_excerpt = character()
    )
  }

  message(sprintf("  Contamination: %d / %d negatives (%.2f%%)",
                  n_contaminated, n_negative, contamination_rate * 100))

  rm(chunks_squished, has_act_name, chunk_ids, chunk_doc_ids,
     chunk_chunk_ids, chunk_years)
  gc()

  list(
    mechanism_tbl = mechanism_tbl,
    timing_df = timing_df,
    corpus_year_counts = corpus_year_counts,
    contamination_rate = contamination_rate,
    n_contaminated = n_contaminated,
    n_negatives = n_negative,
    contamination_examples = contamination_examples
  )
}
