# Chunk Tier Identification for C1 Evaluation
#
# Assigns chunks to tiers for chunk-based C1 evaluation:
#   Tier 1: Contains verbatim us_labels passage text (gold standard)
#   Tier 2: Mentions known act name via substring, subcomponent decomposition,
#           or co-occurrence matching (pipeline positive)
#   Negative: All non-positive chunks, tagged with continuous key_density

# --- Module-level constants ---------------------------------------------------

# Co-occurrence rules for acts where single-term matching is insufficient.
# Each entry maps an act name to a list of term-pair vectors. A chunk matches
# if it contains ALL terms in any one rule pair.
COOCCURRENCE_RULES <- list(
  "Expiration of Excess Profits Tax and of Temporary Income Tax Increases" = list(
    c("expiration", "excess profits")
  )
)

# Terms too broad for Tier 2 matching. Generated by generate_subcomponents()
# but excluded before matching to avoid false positives.
BROAD_EXCLUSIONS <- c(
  # Topic-level phrases (match policy areas, not specific acts)
  "investment tax credit", "balanced budget",
  "taxpayer relief", "excess profits tax",
  "depreciation guidelines", "temporary income tax",
  # Fragments from splitting on "and" (too generic)
  "revenue", "economic growth", "jobs",
  "tax reduction", "job creation", "tax equity"
)

#' Generate searchable subcomponents from a (possibly compound) act name
#'
#' Decomposes act names into searchable terms using five strategies:
#' split on "and", parenthetical extraction, embedded formal names,
#' Public Law numbers, and action+subject phrases. Excludes overly
#' broad terms listed in BROAD_EXCLUSIONS.
#'
#' @param act_name Character string of the act name
#' @return Tibble with columns `term` and `mechanism`
#' @export
generate_subcomponents <- function(act_name) {
  rows <- list()

  # Full name always included
  rows[[length(rows) + 1]] <- tibble::tibble(
    term = act_name, mechanism = "Full name"
  )

  # Split on " and "
  if (stringr::str_detect(act_name, " and ")) {
    parts <- stringr::str_trim(stringr::str_split(act_name, " and ")[[1]])
    for (p in parts) {
      rows[[length(rows) + 1]] <- tibble::tibble(
        term = p, mechanism = "Split on 'and'"
      )
    }
  }

  # Parenthetical content + "of the" variant + before-parenthetical
  if (stringr::str_detect(act_name, "\\(")) {
    paren <- stringr::str_extract(act_name, "(?<=\\()([^)]+)(?=\\))")
    if (!is.na(paren)) {
      rows[[length(rows) + 1]] <- tibble::tibble(
        term = paren, mechanism = "Parenthetical"
      )
      if (stringr::str_detect(paren, " of ") &&
            !stringr::str_detect(paren, " of the ")) {
        rows[[length(rows) + 1]] <- tibble::tibble(
          term = stringr::str_replace(paren, " of ", " of the "),
          mechanism = "Parenthetical + article"
        )
      }
    }
    before_paren <- stringr::str_trim(
      stringr::str_remove(act_name, "\\s*\\([^)]+\\)")
    )
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = before_paren, mechanism = "Before parenthetical"
    )
  }

  # Embedded formal act names: "Xyz Act of YYYY"
  formal <- stringr::str_extract_all(
    act_name,
    "[A-Z][\\w-]+(?:\\s+[A-Z][\\w-]+)*\\s+Act\\s+of\\s+\\d{4}"
  )[[1]]
  for (f in formal) {
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = f, mechanism = "Embedded formal name"
    )
  }

  # Public Law numbers
  pl <- stringr::str_extract(act_name, "Public Law \\d+-\\d+")
  if (!is.na(pl)) {
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = pl, mechanism = "Public Law number"
    )
  }

  # Action + subject phrases for event-type names
  event_match <- stringr::str_match(
    act_name,
    "^(Expiration|Suspension|Restoration|Changes)\\s+(?:in\\s+)?(.*?)(?:\\s+and\\s+|$)"
  )
  if (!is.na(event_match[1, 1]) && !is.na(event_match[1, 2]) &&
        !is.na(event_match[1, 3])) {
    action <- tolower(event_match[1, 2])
    subject <- tolower(stringr::str_trim(event_match[1, 3]))
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = paste(action, "of the", subject),
      mechanism = "Action + subject"
    )
    rows[[length(rows) + 1]] <- tibble::tibble(
      term = paste(action, "of", subject),
      mechanism = "Action + subject"
    )
  }

  result <- dplyr::bind_rows(rows)

  # Exclude overly broad terms
  result <- result |>
    dplyr::filter(!tolower(term) %in% BROAD_EXCLUSIONS)

  # Deduplicate by term (keep first mechanism label)
  dplyr::distinct(result, term, .keep_all = TRUE)
}


#' Identify Tier 1 chunks (contain verbatim labeled passages)
#'
#' For each passage in aligned_data, extracts a distinctive substring and
#' searches chunks for an exact match. Narrows search by matching document
#' source and year when possible.
#'
#' @param aligned_data Tibble from align_labels_shocks() with passages_text
#' @param chunks Tibble from make_chunks() with doc_id, year, text
#' @return Tibble with chunk_id, doc_id, year, act_name, tier, passage_idx
#' @export
identify_tier1_chunks <- function(aligned_data, chunks) {
  results <- list()

  for (i in seq_len(nrow(aligned_data))) {
    act <- aligned_data[i, ]
    passages <- stringr::str_split(act$passages_text, "\n\n")[[1]]
    passages <- stringr::str_trim(passages)
    passages <- passages[nchar(passages) > 50]

    act_year <- act$year

    # Narrow chunk search to same year +/- 1 (passages may appear in adjacent years)
    candidate_chunks <- chunks |>
      dplyr::filter(
        is.na(year) | (year >= act_year - 1 & year <= act_year + 1)
      )

    for (j in seq_along(passages)) {
      # Extract first 100 chars as search substring
      passage <- passages[j]
      search_str <- substr(stringr::str_squish(passage), 1, 100)

      # Try exact substring match
      matches <- candidate_chunks |>
        dplyr::filter(
          stringr::str_detect(text, stringr::fixed(search_str))
        )

      if (nrow(matches) == 0) {
        # Fall back to shorter substring (first 60 chars)
        search_str <- substr(stringr::str_squish(passage), 1, 60)
        matches <- candidate_chunks |>
          dplyr::filter(
            stringr::str_detect(text, stringr::fixed(search_str))
          )
      }

      if (nrow(matches) > 0) {
        # Take the first match (if multiple chunks contain the passage due to
        # overlap, any one is a valid Tier 1 representative)
        results[[length(results) + 1]] <- tibble::tibble(
          chunk_id = matches$chunk_id[1],
          doc_id = matches$doc_id[1],
          year = matches$year[1],
          act_name = act$act_name,
          tier = 1L,
          passage_idx = j
        )
      }
    }
  }

  tier1 <- if (length(results) > 0) {
    dplyr::bind_rows(results) |>
      dplyr::distinct(chunk_id, doc_id, act_name, .keep_all = TRUE)
  } else {
    tibble::tibble(
      chunk_id = integer(), doc_id = character(), year = integer(),
      act_name = character(), tier = integer(), passage_idx = integer()
    )
  }

  if (nrow(tier1) == 0) {
    warning("No Tier 1 chunks matched. Check passage text quality and year coverage.")
  }

  n_acts_matched <- dplyr::n_distinct(tier1$act_name)
  n_acts_total <- nrow(aligned_data)
  total_passages <- sum(purrr::map_int(
    stringr::str_split(aligned_data$passages_text, "\n\n"),
    ~ sum(nchar(stringr::str_trim(.x)) > 50)
  ))

  n_passages_found <- length(results)

  message(sprintf(
    "Tier 1: %d chunks matched across %d/%d acts (%d/%d passages found)",
    nrow(tier1), n_acts_matched, n_acts_total,
    n_passages_found, total_passages
  ))

  tier1
}


#' Identify Tier 2 chunks (mention act names, not already Tier 1)
#'
#' Searches remaining chunks for act name mentions using three strategies:
#' (1) whitespace-normalized substring matching, (2) subcomponent decomposition
#' for compound names, and (3) co-occurrence matching for event-type names.
#' Requires at least 2 fiscal keyword co-occurrences to filter incidental matches.
#'
#' @param aligned_data Tibble from align_labels_shocks()
#' @param chunks Tibble from make_chunks()
#' @param tier1_chunks Tibble of Tier 1 chunks (must have doc_id, chunk_id)
#' @return Tibble with chunk_id, doc_id, year, act_name, tier
#' @export
identify_tier2_chunks <- function(aligned_data, chunks, tier1_chunks) {
  act_names <- unique(aligned_data$act_name)

  # Fiscal keywords for co-occurrence filter
  fiscal_keywords <- c(
    "tax", "revenue", "fiscal", "spending", "appropriation",
    "deficit", "excise", "tariff", "deduction", "credit",
    "enacted", "legislation", "provision", "billion", "percent"
  )
  fiscal_pattern <- paste0(
    "\\b(", paste(fiscal_keywords, collapse = "|"), ")\\b"
  )

  # Pre-compute whitespace-normalized text once (handles OCR line breaks)
  chunks_squished <- stringr::str_squish(tolower(chunks$text))

  # Build logical Tier 1 mask via ID matching
  tier1_ids <- paste(tier1_chunks$doc_id, tier1_chunks$chunk_id, sep = "||")
  chunk_ids <- paste(chunks$doc_id, chunks$chunk_id, sep = "||")
  tier1_mask <- chunk_ids %in% tier1_ids

  results <- list()

  for (k in seq_along(act_names)) {
    act_year <- aligned_data$year[
      aligned_data$act_name == act_names[k]
    ][1]

    # Year window mask (±5 years)
    year_mask <- is.na(chunks$year) |
      (chunks$year >= act_year - 5 & chunks$year <= act_year + 5)

    # Combined candidate mask: not Tier 1, within year window
    candidate_mask <- !tier1_mask & year_mask

    if (act_names[k] %in% names(COOCCURRENCE_RULES)) {
      # Co-occurrence path: match chunks containing ALL terms in any rule pair
      matched <- rep(FALSE, nrow(chunks))
      for (rule in COOCCURRENCE_RULES[[act_names[k]]]) {
        co_match <- Reduce(`&`, lapply(rule, function(term) {
          stringr::str_detect(chunks_squished, stringr::fixed(term))
        }))
        matched <- matched | co_match
      }
      matched <- matched & candidate_mask
    } else {
      # Subcomponent path: match chunks containing ANY subcomponent term
      subs <- generate_subcomponents(act_names[k])
      subs_squished <- stringr::str_squish(tolower(subs$term))
      matched <- rep(FALSE, nrow(chunks))
      for (s in subs_squished) {
        matched <- matched |
          stringr::str_detect(chunks_squished, stringr::fixed(s))
      }
      matched <- matched & candidate_mask
    }

    if (any(matched)) {
      # Apply fiscal keyword co-occurrence filter (>=2 keywords)
      fiscal_counts <- stringr::str_count(
        chunks_squished[matched],
        stringr::regex(fiscal_pattern)
      )
      fiscal_pass <- fiscal_counts >= 2
      match_idx <- which(matched)[fiscal_pass]

      if (length(match_idx) > 0) {
        results[[length(results) + 1]] <- tibble::tibble(
          chunk_id = chunks$chunk_id[match_idx],
          doc_id = chunks$doc_id[match_idx],
          year = chunks$year[match_idx],
          act_name = act_names[k],
          tier = 2L
        )
      }
    }
  }

  tier2 <- dplyr::bind_rows(results) |>
    dplyr::distinct(chunk_id, doc_id, act_name, .keep_all = TRUE)

  message(sprintf(
    "Tier 2: %d chunks matched across %d acts",
    nrow(tier2), dplyr::n_distinct(tier2$act_name)
  ))

  tier2
}


#' Identify negative chunks (all non-positive chunks, tagged with key_density)
#'
#' All chunks not in Tier 1 or Tier 2 become negatives. Each negative is tagged
#' with a continuous `key_density` score (n_relevance_keys / n_words) measuring
#' fiscal vocabulary concentration. This replaces the binary relevance-key
#' exclusion with a gradient for S3 error analysis.
#'
#' @param chunks Tibble from make_chunks()
#' @param tier1_chunks Tibble of Tier 1 chunks (must have doc_id, chunk_id)
#' @param tier2_chunks Tibble of Tier 2 chunks (must have doc_id, chunk_id)
#' @param relevance_keys Character vector of relevance keywords
#' @return Tibble with chunk_id, doc_id, year, source_type, text, approx_tokens,
#'   key_density, n_relevance_keys, n_words
#' @export
identify_negative_chunks <- function(chunks, tier1_chunks, tier2_chunks,
                                     relevance_keys) {
  # Build relevance key pattern (word-boundary matching)
  relevance_pattern <- paste0(
    "\\b(", paste(relevance_keys, collapse = "|"), ")\\b"
  )

  # Exclude all Tier 1 and Tier 2 chunks using anti_join
  all_positives <- dplyr::bind_rows(
    tier1_chunks |> dplyr::select(doc_id, chunk_id),
    tier2_chunks |> dplyr::select(doc_id, chunk_id)
  ) |>
    dplyr::distinct()

  negatives <- chunks |>
    dplyr::anti_join(all_positives, by = c("doc_id", "chunk_id"))

  # Compute key_density: n_relevance_keys / n_words
  text_lower <- tolower(negatives$text)
  negatives <- negatives |>
    dplyr::mutate(
      n_relevance_keys = stringr::str_count(
        text_lower,
        stringr::regex(relevance_pattern)
      ),
      n_words = stringr::str_count(text_lower, "\\S+"),
      key_density = dplyr::if_else(
        n_words > 0, n_relevance_keys / n_words, 0
      )
    )

  # Derive source_type from doc_id pattern
  negatives <- negatives |>
    dplyr::mutate(
      source_type = dplyr::case_when(
        stringr::str_detect(doc_id, stringr::regex("erp|economic.report",
                                                    ignore_case = TRUE)) ~ "ERP",
        stringr::str_detect(doc_id, stringr::regex("treasury|annual.report",
                                                    ignore_case = TRUE)) ~ "Treasury",
        stringr::str_detect(doc_id, stringr::regex("budget",
                                                    ignore_case = TRUE)) ~ "Budget",
        TRUE ~ "Other"
      )
    )

  message(sprintf(
    "Negative chunks: %d (ERP: %d, Treasury: %d, Budget: %d, Other: %d)",
    nrow(negatives),
    sum(negatives$source_type == "ERP"),
    sum(negatives$source_type == "Treasury"),
    sum(negatives$source_type == "Budget"),
    sum(negatives$source_type == "Other")
  ))
  message(sprintf(
    "  Key density: median=%.3f, mean=%.3f, max=%.3f",
    stats::median(negatives$key_density),
    mean(negatives$key_density),
    max(negatives$key_density)
  ))

  negatives |>
    dplyr::select(
      chunk_id, doc_id, year, source_type, text, approx_tokens,
      key_density, n_relevance_keys, n_words
    )
}


#' Prepare C1 chunk-based evaluation data
#'
#' Orchestrator that calls tier identification functions and returns
#' structured data for C1 LOOCV and behavioral testing.
#'
#' @param aligned_data Tibble from align_labels_shocks()
#' @param chunks Tibble from make_chunks()
#' @param relevance_keys Character vector of relevance keywords
#' @return List with tier1, tier2, negatives, summary, and
#'   negative_density_summary components
#' @export
prepare_c1_chunk_data <- function(aligned_data, chunks, relevance_keys) {
  message("=== Preparing C1 chunk-based evaluation data ===")

  # Step 1: Tier 1 — verbatim passage matches
  tier1 <- identify_tier1_chunks(aligned_data, chunks)

  # Step 2: Tier 2 — act name mentions (excluding Tier 1)
  tier2 <- identify_tier2_chunks(aligned_data, chunks, tier1)

  # Step 3: Negatives — all non-positive chunks, tagged with key_density
  negatives <- identify_negative_chunks(
    chunks, tier1, tier2, relevance_keys
  )

  # Coverage validation: warn if any act has zero Tier 1 AND zero Tier 2 matches
  acts_in_tier1 <- unique(tier1$act_name)
  acts_in_tier2 <- unique(tier2$act_name)
  all_covered <- unique(c(acts_in_tier1, acts_in_tier2))
  uncovered <- setdiff(aligned_data$act_name, all_covered)
  if (length(uncovered) > 0) {
    warning(sprintf(
      "%d act(s) have zero Tier 1 AND zero Tier 2 matches: %s",
      length(uncovered),
      paste(uncovered, collapse = "; ")
    ))
  }

  # Summary statistics
  n_total_chunks <- nrow(chunks)
  n_tier1 <- nrow(tier1)
  n_tier2 <- nrow(tier2)
  n_negative <- nrow(negatives)

  summary_tbl <- tibble::tibble(
    category = c("Tier 1", "Tier 2", "Negative", "Total"),
    n_chunks = c(n_tier1, n_tier2, n_negative, n_total_chunks),
    pct = round(n_chunks / n_total_chunks * 100, 1)
  )

  message("\n=== Chunk Tier Summary ===")
  message(sprintf("  Tier 1 (verbatim passage): %d chunks", n_tier1))
  message(sprintf("  Tier 2 (act name mention): %d chunks", n_tier2))
  message(sprintf("  Negative (all remaining):  %d chunks", n_negative))
  message(sprintf("  Total:                     %d chunks", n_total_chunks))

  # Negative key_density distribution (binned summary for diagnostics)
  density_breaks <- c(0, 0.001, 0.01, 0.03, 0.05, Inf)
  density_labels <- c("0", "(0, 1%]", "(1%, 3%]", "(3%, 5%]", ">5%")
  negative_density_summary <- negatives |>
    dplyr::mutate(
      density_bin = cut(
        key_density,
        breaks = density_breaks,
        labels = density_labels,
        include.lowest = TRUE,
        right = TRUE
      )
    ) |>
    dplyr::count(density_bin, .drop = FALSE) |>
    dplyr::mutate(pct = round(n / sum(n) * 100, 1))

  # Join chunk text to tier1 and tier2 for evaluation
  tier1_with_text <- tier1 |>
    dplyr::left_join(
      chunks |> dplyr::select(doc_id, chunk_id, text, approx_tokens),
      by = c("doc_id", "chunk_id")
    )

  tier2_with_text <- tier2 |>
    dplyr::left_join(
      chunks |> dplyr::select(doc_id, chunk_id, text, approx_tokens),
      by = c("doc_id", "chunk_id")
    )

  list(
    tier1 = tier1_with_text,
    tier2 = tier2_with_text,
    negatives = negatives,
    summary = summary_tbl,
    negative_density_summary = negative_density_summary
  )
}
