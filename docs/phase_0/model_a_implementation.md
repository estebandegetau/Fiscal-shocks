# Model A Implementation: Act Detection

**Status:** ‚úÖ Implementation Complete
**Date:** 2025-01-20
**Phase:** Phase 0, Days 3-4

## Overview

Model A is a binary classifier that determines whether a text passage describes a specific fiscal policy act. This is the first of three models in the Phase 0 pipeline.

**Success Criterion:** F1 > 0.85 on test set

## Files Created

### R Functions

1. **`R/functions_llm.R`** - Shared LLM utilities for all models
   - `call_claude_api()` - Core API wrapper with retry logic and rate limiting
   - `format_few_shot_prompt()` - Prompt builder for few-shot learning
   - `parse_json_response()` - Robust JSON extraction from API responses
   - `log_api_call()` - Usage and cost tracking
   - `log_api_error()` - Error logging
   - `get_api_cost_summary()` - Cost reporting

2. **`R/model_a_detect_acts.R`** - Model A specific functions
   - `model_a_detect_acts()` - Single text classification
   - `model_a_detect_acts_batch()` - Batch processing with progress bar
   - `evaluate_model_a()` - Compute precision, recall, F1, confusion matrix
   - `plot_model_a_confusion_matrix()` - Visualization

3. **`R/generate_few_shot_examples.R`** - Example generation
   - `generate_model_a_examples()` - Sample from training data
   - `save_few_shot_examples()` - Export to JSON

### Prompts

4. **`prompts/model_a_system.txt`** - System prompt with classification criteria
5. **`prompts/model_a_examples.json`** - Few-shot examples (generated by pipeline)

### Configuration

6. **`.env.example`** - API key template

## Architecture

### Classification Criteria

Model A classifies a passage as containing a fiscal act if it meets ALL criteria:

1. ‚úÖ Names specific legislation (e.g., "Revenue Act of 1948")
2. ‚úÖ Describes actual policy change (not proposals)
3. ‚úÖ Involves federal taxes or spending (not state/local)

### Few-Shot Learning

- **Positive examples:** 10 passages containing fiscal acts
- **Negative examples:** 10 passages with general economic discussion
- **Sampling:** Stratified random sample from training set (seed = 20251206)

### API Configuration

- **Model:** Claude 3.5 Sonnet (claude-3-5-sonnet-20241022)
- **Temperature:** 0.0 (deterministic)
- **Max tokens:** 500 (output)
- **Rate limit:** 50 RPM (1.2s sleep between calls)
- **Retry:** 3 attempts with exponential backoff (2s, 4s, 8s)

## Pipeline Integration

### Targets Added to `_targets.R`

```r
# Generate few-shot examples from training data
tar_target(model_a_examples, ...)

# Save examples to JSON file
tar_target(model_a_examples_file, ...)

# Run predictions on validation set
tar_target(model_a_predictions_val, ...)

# Evaluate on validation set
tar_target(model_a_eval_val, ...)

# Run predictions on test set
tar_target(model_a_predictions_test, ...)

# Evaluate on test set
tar_target(model_a_eval_test, ...)
```

### Dependencies

```
training_data_a
    ‚Üì
model_a_examples
    ‚Üì
model_a_examples_file
    ‚Üì
model_a_predictions_val ‚Üí model_a_eval_val
    ‚Üì
model_a_predictions_test ‚Üí model_a_eval_test
```

## Usage

### Setup

1. **Install required packages:**
   ```r
   install.packages(c("httr2", "jsonlite", "progress"))
   ```

2. **Configure API key:**
   ```bash
   cp .env.example .env
   # Edit .env and add your ANTHROPIC_API_KEY
   ```

3. **Verify .env is gitignored:**
   ```bash
   grep "^\.env$" .gitignore
   ```

### Run Model A Pipeline

```r
# Load targets
library(targets)

# Generate few-shot examples (one-time)
tar_make(model_a_examples_file)

# Run validation set predictions
tar_make(model_a_eval_val)

# Check results
tar_read(model_a_eval_val)
# Expected output:
# $f1_score
# [1] 0.87  # Target: > 0.85
#
# $precision
# [1] 0.85
#
# $recall
# [1] 0.89

# If validation looks good, run test set
tar_make(model_a_eval_test)
```

### Single Text Classification

```r
# Load functions
tar_source()

# Classify a single passage
result <- model_a_detect_acts(
  text = "The Revenue Act of 1964 reduced marginal tax rates..."
)

result$contains_act  # TRUE
result$act_name      # "Revenue Act of 1964"
result$confidence    # 0.95
result$reasoning     # Explanation
```

### Batch Classification

```r
# Classify multiple passages
texts <- c(
  "The Revenue Act of 1964...",
  "Economic growth continued at a moderate pace...",
  "The Tax Reform Act of 1986..."
)

results <- model_a_detect_acts_batch(texts, show_progress = TRUE)
# Returns tibble with contains_act, act_name, confidence, reasoning
```

## Evaluation Metrics

### Primary Metrics

- **Precision:** TP / (TP + FP) - Target > 0.80
- **Recall:** TP / (TP + FN) - Target > 0.90 (don't miss real acts)
- **F1 Score:** 2 √ó (P √ó R) / (P + R) - **Target > 0.85**

### Confusion Matrix

```
              Predicted
              No Act   Act
True  No Act    TN      FP
      Act       FN      TP
```

### Confidence Calibration

Check if confidence scores match actual accuracy:

```r
eval <- tar_read(model_a_eval_val)
print(eval$calibration)

# Expected: Confidence ~= Accuracy
# e.g., passages with 0.9 confidence should be ~90% correct
```

## Cost Estimation

### Validation Set
- **Size:** ~55 passages (10 acts + 45 negatives)
- **Avg tokens:** 1000 input + 500 output per passage
- **Cost:** ~$0.14 (55 √ó 1.5K tokens √ó $0.003/1K input + 0.5K √ó $0.015/1K output)

### Test Set
- **Size:** ~40 passages (6 acts + 34 negatives)
- **Cost:** ~$0.10

### Total Model A Cost
- **Estimated:** ~$0.25
- **Actual:** Check with `get_api_cost_summary()`

## Logs

All API activity is logged for debugging and cost tracking:

- **`logs/api_calls.csv`** - Timestamp, model, tokens, cost per call
- **`logs/api_errors.log`** - Error messages with timestamps

View cost summary:
```r
tar_source()
get_api_cost_summary()
```

## Troubleshooting

### API Key Error

```
Error: ANTHROPIC_API_KEY not found in environment
```

**Solution:** Create `.env` file from `.env.example` and add your API key.

### Rate Limit Error

```
Error: API error 429 - Rate limit exceeded
```

**Solution:** The code already includes 1.2s sleep between calls. If error persists, increase sleep time in `call_claude_api()`.

### JSON Parsing Error

```
Warning: Failed to parse JSON response
```

**Cause:** Claude returned malformed JSON or text instead of JSON.

**Solution:** Check `logs/api_errors.log` for raw response. May need to adjust system prompt.

### Low F1 Score

If validation F1 < 0.85:

1. **Check examples:** Review `prompts/model_a_examples.json` - are they representative?
2. **Add more examples:** Increase `n_positive` and `n_negative` to 15-20 each
3. **Adjust threshold:** Try different confidence thresholds (0.3, 0.5, 0.7)
4. **Review errors:** Which passages were misclassified? Add similar examples.

## Next Steps

After Model A validation succeeds (F1 > 0.85):

1. ‚úÖ Run test set evaluation
2. ‚è≠Ô∏è Proceed to Model B: Motivation Classification (Days 4-6)
3. üìä Generate evaluation report with confusion matrix plots

## References

- **Plan:** `docs/phase_0/plan_phase0.md` (Lines 171-312)
- **Training Data:** `notebooks/review_training_data.qmd`
- **Claude API Docs:** https://docs.anthropic.com/claude/reference/messages_post
